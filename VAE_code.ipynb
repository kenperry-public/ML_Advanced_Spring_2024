{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<!--- Copy from VAE_Generative.ipynb  --->\n",
    "\n",
    "$$\n",
    "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
    "\\def\\qr#1{\\mathcal{q}(#1)}\n",
    "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# VAE: Code\n",
    "\n",
    "We discuss the highlights of the code in this [notebook](vae.ipynb)\n",
    "- derived from the [Keras examples](https://keras.io/examples/generative/vae/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Deriving a new Model via sub-classing\n",
    "\n",
    "Similar to the code for the `Autoencoder`, the `VAE` class\n",
    "- is sub-classed from `Model`\n",
    "- *contains* an Encoder `self.encoder` and a Decoder `self.decoder`\n",
    "    - but these are defined *external* to the class\n",
    "    - rather than within the class code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Custom `train_step`\n",
    "\n",
    "Unlike the code for `Autoencoder`, the model for `VAE` implements the model behavior\n",
    "- **not** by overriding the `call` method\n",
    "- but by *overriding* the training step `train_step` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So\n",
    "- we can't actually \"call\" this model (e.g., `x = m(x)`)\n",
    "- but, during training, we can make it behave in the desired manner\n",
    "    - call encoder\n",
    "    - call decoder\n",
    "    \n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Custom training step vs custom loss\n",
    "\n",
    "The reason for overriding `train_step` rather than `call` may not be obvious at first glance.\n",
    "\n",
    "The VAE has a *complex loss function* of two parts\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\loss  & = & - \\log(\\prs{\\x}{\\Theta}) + \\KL( \\qrs{\\z|\\x}{\\Phi} \\; ||\\; \\qr{\\z | \\x}) \\\\\n",
    "& = & \\loss_R + \\loss_D\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "- Reconstruction Loss $\\loss_R$\n",
    "- KL loss $\\loss_D$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One could (in theory) create a custom loss\n",
    "- an the ordinary training mechanism\n",
    "- would compute the loss\n",
    "- and the gradients of the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By overriding the `train_step`, *our code* becomes responsible for the gradient computation\n",
    "\n",
    "     with tf.GradientTape() as tape:\n",
    "                z_mean, z_log_var, z = self.encoder(data)\n",
    "                reconstruction = self.decoder(z)\n",
    "                \n",
    "                reconstruction_loss = ...\n",
    "                kl_loss = ...\n",
    "                total_loss = reconstruction_loss + kl_loss\n",
    "                \n",
    "            grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The computation of `total_loss` \n",
    "- is performed within the scope of `tf.GradientTape()`\n",
    "- which allows automatic differentiation of the loss\n",
    "\n",
    "We then manually compute the gradients\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "\n",
    "and update the weights through the gradients\n",
    "\n",
    "         self.optimizer.apply_gradients(zip(grads, self.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So why go through this effort (as opposed to a custom loss) ?\n",
    "\n",
    "The model *also* provides *custom metrics*\n",
    "- values gathered during training\n",
    "    - we are used to seeing Loss and Validation Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "These metrics are\n",
    "- total loss\n",
    "- reconstruction loss\n",
    "- KL loss\n",
    "\n",
    "        @property\n",
    "        def metrics(self):\n",
    "            return [\n",
    "                self.total_loss_tracker,\n",
    "                self.reconstruction_loss_tracker,\n",
    "                self.kl_loss_tracker,\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The reason for overriding `train_step` is so that *we can update the custom metrics* during training\n",
    "\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is the complete code for the method\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Encoder architecture\n",
    "\n",
    "We can see the Encoder architecture\n",
    "\n",
    "<img src=\"images/vae_conv_encoder.png\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The input image $(28 \\times 28 \\times 1)$\n",
    "- is processed by 2 Convolutional layers\n",
    "- creating a representation with the *same spatial dimensions* as the input\n",
    "- but with many more (16 versus 1) features\n",
    "\n",
    "It is likely that this representation is richer than the alternative\n",
    "- of initial flattening\n",
    "- processing by `Dense` layers\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can see (in components `z_mean` and `z_log_var`)\n",
    "- that the final representation of the input\n",
    "- is used to derive the moments ($\\mu^\\ip$ and $\\sigma_\\ip$) of the *distribution* for example $i$\n",
    "\n",
    "The final layer samples from this *multivariate* distribution\n",
    "- to produce the latent representation \n",
    "- a *vector* of length `latent_dim`\n",
    "\n",
    "**Note**\n",
    "\n",
    "In our code `latent_dim = 2`\n",
    "- this is a sample from a *bivariate* distribution with mean $\\mu^\\ip$ and standard deviation $\\sigma^\\ip$\n",
    "- don't confuse the length of the sample vector with the pair of moments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decoder Architecture\n",
    "\n",
    "We can see the Encoder architecture\n",
    "\n",
    "<img src=\"images/vae_conv_decoder.png\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can see that the Decoder\n",
    "- takes a latent vector of length `latent_dim` as input\n",
    "- Inverts the Encoder's Convolutional layers (`Conv2DTranspose`)\n",
    "\n",
    "These steps are *almost* exactly the inverse of \n",
    "- reverse of the Encoder's operation sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Kernel size of 1\n",
    "\n",
    "The final `Conv2DTranspose` layer \n",
    "- is an example of a Convolution with kernel size **one** [discussed in Intro course](CNN_Space_and_Time.ipynb#Kernel-size-1)\n",
    "- whose sole purpose is to \"re-size\" the channel dimension\n",
    "    - in this case: to 1 channel, just like the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deriving a new Layer via sub-classing\n",
    "\n",
    "Sampling from the multivariate distribution comes from a *new layer type* `Sampling`\n",
    "- sub-class of the generic `Layer` class\n",
    "\n",
    "Similar to sub-classing a `Model` the work of sub-classing a `Layer`\n",
    "- comes from overriding the `call` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There is one sample drawn  from *each example* in the batch\n",
    "- each with it's own $\\mu^\\ip$ and $\\sigma^\\ip$\n",
    "\n",
    "        class Sampling(layers.Layer):\n",
    "            \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "            def call(self, inputs):\n",
    "                z_mean, z_log_var = inputs\n",
    "                batch = tf.shape(z_mean)[0]\n",
    "                dim = tf.shape(z_mean)[1]\n",
    "                epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "                return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exploring the latent space\n",
    "\n",
    "The notebook runs some experiments\n",
    "- [explore the latent space](vae.ipynb#Display-a-grid-of-sampled-digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conditional VAE\n",
    "\n",
    "The [code for the Conditional VAE](vae.ipynb#Conditional-VAE) is very similar to that of the\n",
    "unconditional VAE.\n",
    "\n",
    "The main difference is that both the Encoder and Decoder inputs are now *pairs* where\n",
    "- the second element of the pair is the desired label\n",
    "- the first element is the same as the unconditional VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The label arguments \n",
    "- are implemented One Hot Encoded vectors\n",
    "- not categorical constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The way that an image with a *specific* label gets generated is not obvious\n",
    "- puzzling in its simplicity\n",
    "\n",
    "The Encoder\n",
    "- ignores the label for most of its processing of the input\n",
    "- uses the label\n",
    "    - to modify the alternate representation of the input\n",
    "    - immediately before creating $\\mu^\\ip$ and $\\sigma^\\ip$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus, $\\mu^\\ip$ and $\\sigma^\\ip$ are *conditioned* on both\n",
    "- the alternate representation that the unconditional VAE would produce\n",
    "- **and** the label\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is the Encoder\n",
    "- notice the two distinct `Input` layers\n",
    "\n",
    "<img src=\"images/cond_vae_conv_encoder.png\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Decoder\n",
    "- concatenates the OHE of the label to the latent created by the Encoder\n",
    "- to create a \"longer\" latent representation\n",
    "    - length is: `latent_dim` + `number of classes`\n",
    "    - versus `latent_dim` for the unconditional VAE\n",
    "\n",
    "The CVAE Decoder is almost exactly the same as the unconditional VAE Decoder\n",
    "- just with a longer latent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is the Decoder\n",
    "- notice the two distinct `Input` layers\n",
    "\n",
    "<img src=\"images/cond_vae_conv_decoder.png\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So how does the Decoder produce an image with the chosen label ?\n",
    "\n",
    "**During training it learns**\n",
    "- the \"meaning\" of the label part of the elongated latent\n",
    "- by observing a larger representation loss\n",
    "    - when it creates an output that doesn't match the label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is, consider training example $i$ with label $C$\n",
    "- the Decoder reconstruction loss is large if its output is very different than the input\n",
    "- So the Decoder learns (i.e., its weights associate) a relationship between\n",
    "    - the OHE of the label\n",
    "    - the desired output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**This is the mantra of Deep Learning**\n",
    "- you don't guide or program the model with specific instructions on **how** to achieve a task\n",
    "- it **learns** the association between input and output\n",
    "    - through Loss minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
