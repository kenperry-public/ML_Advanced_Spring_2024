{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vision Transformer\n",
    "\n",
    "[paper](https://arxiv.org/pdf/2010.11929.pdf)\n",
    "\n",
    "We have been conditioned to make different architectural choices based on the type of data\n",
    "- Image: Convolutional Neural Network (CNN)\n",
    "- Text:  Transformer\n",
    "\n",
    "The *Vision Transformer (ViT)* is a proof of concept that Transformers can replace CNN's for Vision tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have some experience with encoding images as Tokens\n",
    "- VQ-VAE\n",
    "    - \"tokenizes\" an Image\n",
    "    - creating a spatial grid of Token Ids (indices into a codebook)\n",
    "    - flattened into a sequence of Token Ids\n",
    "- CLIP\n",
    "    - flattened feature maps of a deep layer of a CNN trained for a Vision task\n",
    "    - *Single* vector encodes an Image\n",
    "        - **not** a sequence of tokens (i.e., not similar to Text as a sequence of Text Token Ids)\n",
    "    - can project the flattened vector to a common space\n",
    "        - shared with the single vectors representing a sequence of Text Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What's wrong with a CNN ?\n",
    "\n",
    "### Transformers scale better than CNN\n",
    "\n",
    "In our module on the [Transformer](Transformer.ipynb#Complexity:-summary) we compared the computational burden of various architectures\n",
    "- given a sequence $\\x$ of length $T$\n",
    "- each element of sequence of dimension $d$\n",
    "- CNN kernel size $k$\n",
    "\n",
    "\n",
    "| Type | Parameters  | Operations  &nbsp; &nbsp; &nbsp; | Sequential steps | Path length\n",
    "|:------|:---|:---|:---|:---|\n",
    "|  CNN | $\\OrderOf{k * d^2}$   | $\\OrderOf{T * k * d^2}$ | $\\OrderOf{T}$   | $\\OrderOf{T}$ |\n",
    "| RNN  | $\\OrderOf{d^2}$       | $\\OrderOf{T * d^2}$     | $\\OrderOf{T}$    | $\\OrderOf{T}$ |\n",
    "| Self-attention | $\\OrderOf{T *d^2} $ | $\\OrderOf{T^2 *d}$ | $\\OrderOf{1}$ | $\\OrderOf{1}$ |\n",
    "\n",
    "Reference:\n",
    "- [Table 1 of Attention paper](https://arxiv.org/pdf/1706.03762.pdf#page=6)\n",
    "- See [Stack overflow](https://stackoverflow.com/questions/65703260/computational-complexity-of-self-attention-in-the-transformer-model) for correction of the number Operations calculated in paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One clear advantage of the Transformer (Self-attention) over the CNN is time\n",
    "- constant Path Length, versus $T$ for the CNN\n",
    "\n",
    "The sequence length $T$ (typical: 64-128) is usually less than $d$ (typical: 256).\n",
    "- So the Transformer entails fewer Operations than the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inductive Bias\n",
    "\n",
    "There are a number of *inductive biases* associated with the CNN\n",
    "- Translation invariance\n",
    "    - the CNN can recognize a sub-object in the Image, even if its position is shifted\n",
    "    - this is because the same kernel is slid over the entire Image\n",
    "- Locality\n",
    "    - The Convolution operation is local\n",
    "        - Pixels within the scope of the kernel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Experiments by the authors show that these biases\n",
    "- give a performance (Accuracy) benefit to the CNN versus the Transformer\n",
    "- **when training datasets are not very large**\n",
    "\n",
    "However, once the Transformer is trained on a sufficiently large dataset\n",
    "- it can transfer these skills to other tasks (transfer learning)\n",
    "- and surpass CNN performance on these tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When training datasets are very large, the relative advantage shifts to the Transformer.\n",
    "\n",
    "This is not completely surprising\n",
    "- More training data facilitates larger models (more parameters)\n",
    "- Larger models are able to create more complex function approximations\n",
    "    - Recall our lecture on [Neural Network is a Universal Function Approximator](Universal_Function_Approximator.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With more training examples,\n",
    "- the Transformer learns to attend to distant pixels\n",
    "    - global view versus local view of CNN\n",
    "- Transfer Learning more successful\n",
    "    - The ViT seems to learn more general properties of Images when given a very large number\n",
    "    - Allowing transfer to Target tasks different than training\n",
    "        - by creating a Target-specific Classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Details\n",
    "\n",
    "The ViT is the Encoder side of the original Encoder/Decoder Transformer.\n",
    "- Multi-head **non-masked** attention to all inputs\n",
    "    - Can attend to the entire input\n",
    "    - No causal ordering restrictions\n",
    "\n",
    "The Transformer is augmented with a Classification Head (labelled \"MLP Head\") and trained on a Vision Classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So how exactly do we go from a spatially organized grid $(w \\times h \\times c)$ of pixel intensities (RGB) to the sequences\n",
    "that a Transformer can process ?\n",
    "\n",
    "The obvious answer\n",
    "- Flatten the non-feature (spatial) dimensions into a sequence (\"raster order\": left to right, top to bottom)\n",
    "- Sequences would be too long !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Instead: the spatial grid of $(w \\times h)$ pixels is decomposed into\n",
    "- $(16 \\times 16)$ groups (called *patches*) of adjacent pixels\n",
    "- Total number of patches $T = \\frac{w * h}{16*16}$\n",
    "\n",
    "The patches are analogous to tokens of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Just like text tokens, the patches are processed through an Embedding layer\n",
    "- are encoded as vectors using a Linear Projection\n",
    "    - maps a flattened vector of length $(16 * 16)$ into a vector of length $d$\n",
    "    - mapping is a **learned embedding** (just like word embeddings)\n",
    "\n",
    "The result of converting to patches followed by embedding is a sequence of image embeddings\n",
    "- of length $T$ (number of patches)\n",
    "- each element of length $d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An optional token `<CLS>` may be prepended to the sequence\n",
    "- pictured as `*` in the diagram\n",
    "\n",
    "As we have seen for other sequence summary/classification tasks\n",
    "- the `<CLS>` token is taken as a proxy for the summary of the sequence\n",
    "- that is: the latent state associated with processing the `<CLS>` token is interpretted as a summary of the entire sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Alternative to patches\n",
    "\n",
    "Rather than using raw pixel grids as patches, the authors also suggest using feature maps of a CNN.\n",
    "\n",
    "We have seen this approach used before in the [VQ-VAE](VQ_VAE_Generative.ipynb).\n",
    "\n",
    "The CNN preserves the number of non-feature (spatial) dimensions\n",
    "- perhaps reducing the length of each, i.e., *down-sampling*\n",
    "- to retain a spatially oriented grid of *feature* vectors (i.e., the vector formed by the features/channels at each spatial location)\n",
    "- each vector is analogous to a \"patch\" of pixels\n",
    "    - but may have semantics (rather than just syntactic) content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since the Transformer can attend to any element in the sequence at any time\n",
    "- a learned Position Embedding is paired with each element of the sequence\n",
    "- so that the Transformer can infer an ordering of patches\n",
    "    - eventually will \"learn\" spatial relationships between patches ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th><center>Vision Transformer</center></th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td><img src=\"https://raw.githubusercontent.com/google-research/vision_transformer/main/vit_figure.png\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Analysis\n",
    "\n",
    "The authors run a number of experiments to better understand the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Position embeddings\n",
    "\n",
    "The authors experiment with several ways of describing the position of a patch\n",
    "- no position embedding\n",
    "- as a one dimensional index into the linearized order of patches\n",
    "- as a (row, column) pair into the downsized spatial grid of patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The experiment was run against a single task\n",
    "- No position embedding performed the worst\n",
    "- Little difference in performance with the 2D versus 1D position embedding.\n",
    "\n",
    "The authors speculate\n",
    "- Since there are a small number of patches\n",
    "    - the difference between 1D and 2D is less important than it would be \n",
    "    - if we were required to encode position of individual pixels, which are far greater in number\n",
    "    - with large number of examples, the ViT can *learn* the 2D spatial relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention Distance\n",
    "\n",
    "Recall the Attention lookup mechanism\n",
    "- There is a *soft lookup* table\n",
    "    - (key, value) pairs\n",
    "    - where $\\text{key} = \\text{value}$\n",
    "    - and there is one key that is equal to each element of the input sequence\n",
    "- A *query* is run against the table\n",
    "    - returning a weighted sum (across keys) of the values associated with the key\n",
    "    - the weights are the *attention weights*\n",
    "        - measure of how closely the query matches the key (e.g., cosine similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given a single example sequence and a single head in the Transformer at some layer $l$\n",
    "- run the Transformer against the sequence\n",
    "- measure the distance (in spatial grid dimensions ?) from the query pixel to each key\n",
    "- weight the distances by the associated attention weight\n",
    "\n",
    "The *Attention Distance* is the average examples of the weighted distance between the query pixel and the other pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The authors provide a diagram of the Attention Distance\n",
    "- for each layer $1 \\le l \\le L$\n",
    "- each of the 16 attention heads at each layer\n",
    "- for examples of spatial dimensions $(224 \\times 224)$\n",
    "\n",
    "This is analogous to the *Receptive Field* of a CNN.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th><center>ViT Attention Distances</center></th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td><img src=\"images/ViT_Attention_distance.png\" width=100%></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In shallow layers: heads attend to pixels at all distances\n",
    "\n",
    "In deep layers: heads attend to far away ($120 \\approx 50\\%$ of width) pixels\n",
    "- For a CNN with kernel size $3$ and stride $1$ to have a receptive field of 120 pixels\n",
    "    - would need more than $58$ layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
