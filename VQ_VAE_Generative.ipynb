{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\V}{\\mathbf{V}}\n",
    "\\newcommand{\\Vr}{\\mathbb{V}}\n",
    "\\newcommand{\\codebook}{\\mathbf{E}}\n",
    "\\newcommand{\\encoder}{\\mathcal{E}}\n",
    "\\newcommand{\\decoder}{\\mathcal{D}}\n",
    "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
    "\\def\\qr#1{\\mathcal{q}(#1)}\n",
    "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "[VQ-VAE-2 paper](https://arxiv.org/pdf/1906.00446.pdf)\n",
    "- [example code from paper's authors](https://github.com/deepmind/sonnet/blob/v1/sonnet/examples/vqvae_example.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>Text is represented as\n",
    "            <ul>\n",
    "                <li>a sequence of integers: an integer index into the list of tokens in the vocabulary</li>\n",
    "            </ul>\n",
    "        <li>We want to represent other types of data (e.g., images) as \n",
    "            <ul>\n",
    "                <li>a sequence of integers: an integer index into a \"code book\": finite list of vectors</li>\n",
    "            </ul>\n",
    "        <li>By making the \"shape\" (sequence) and \"type\" (integer) compatible between text and other data types</li>\n",
    "            <ul>\n",
    "                <li>We facilitate mixing text and other data types</li>\n",
    "                <li>Will enable an implementation of Text to Image as a simple extension of the Language Modeling objective</li>      \n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We now present an Autoencoder with a twist\n",
    "- the latent representation produced for an input\n",
    "- is limited to be one member of a *finite list* of vectors\n",
    "- enabling us to describe the latent by the *integer index* in the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why is an integer encoding of an input interesting ?\n",
    "- It is analogous to the way we treat words (tokens) in Natural Language Processing\n",
    "    - an index into a finite Vocabulary of words\n",
    "- This opens the possibility of dealing with sequences that are a *mixture* of text and other data types (e.g., images)\n",
    "\n",
    "Rather than pre-specifying the finite list, we will *learn* the list by training a Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In a subsequent module, we will use a similar technique for the task of Text to Image\n",
    "- given the description of an image in words\n",
    "- create an image matching the description\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But there is a significant problem with a Neural Network that learns discrete values\n",
    "- the network may need to make a \"hard\" (as compared to \"soft\") choice\n",
    "    - a true `if` statement (\"hard\") versus a \"soft\" conditional (sigmoid)\n",
    "    - a Python `dict` (\"hard\" lookup) versus a \"soft\" lookup (Context Sensitive Memory)\n",
    "- \"hard\" means derivatives are not continuous\n",
    "- Gradient Descent won't work\n",
    "\n",
    "We will introduce a new Deep Learning operator (*Stop Gradient*) to deal with \"hard\" operators.\n",
    "\n",
    "**References**\n",
    "\n",
    "- [paper: vanilla VQ-VAE](https://arxiv.org/pdf/1711.00937.pdf)\n",
    "- [paper: VQ-VAE-2](https://arxiv.org/pdf/1906.00446.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# From PCA to VQ-VAE\n",
    "\n",
    "The common element in the design of any Autoencoder method is \n",
    "- to create a \n",
    "latent representation $\\z$ of input $\\x$ \n",
    "- such that $\\z$ can be (approximately) inverted\n",
    "to reconstruct $\\x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Principal Components Analysis is a type of Autoencoder that produces a latent representation $\\z$ of $\\x$\n",
    "\n",
    "- $\\x$ is a vector of length $n$: $\\x \\in \\Reals^n$\n",
    "- $\\z$ is a vector of length $n' \\le n$: $\\z \\in \\Reals^{n'}$\n",
    "\n",
    "Usually $n' << n$: achieving *dimensionality reduction*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is accomplished by decomposing $\\x$ into a weighted product of $n$ *Principal Components*\n",
    "- $\\V \\in \\Reals^{n \\times n}$\n",
    "$$\\x = \\z' \\V^T$$\n",
    "    - where $\\z' \\in \\Reals^n$\n",
    "    - rows of $\\V^T$ are the components\n",
    "\n",
    "So $\\x$ can be decomposed into the weighted sum (with $\\z'$ specifying the weights) \n",
    "- of $n$ component vectors\n",
    "- each of length $n$\n",
    "\n",
    "Since $\\z' \\in \\Reals^n$: there is **no** dimensionality reduction just yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One can view $\\V^T$ as a kind of *code book*\n",
    "- any $\\x$ can be represented (as a linear combination) of the *codes* (components) in $V^T$\n",
    "$$\\x = \\z' \\V^T$$\n",
    "\n",
    "$\\z'$ is like a translation of $\\x$, using $\\V$ as the vocabulary.\n",
    "- weights in the codebook\n",
    "- rather than weights in the standard basis space $I \\in \\Reals^{n \\times n} = \\text{diagonal}(n)$\n",
    "$$\n",
    "\\x = \\x I\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dimensionality reduction is achieved by defining $\\z$ as a length $n'$ prefix of $\\z$\n",
    "- $\\z = \\z'_{1:n'}$\n",
    "- $\\z \\in \\Reals^{n'}$\n",
    "\n",
    "Similarly, we needed only $n'$ components from $\\V$\n",
    "- $\\Vr^T = \\V^T_{1:n'}$\n",
    "- $\\Vr^T \\in \\Reals^{n' \\times n}$\n",
    "\n",
    "We can construct an *approximation* $\\hat\\x$ of $\\x$ using *reduced dimension* $\\z'$ and $\\Vr$\n",
    "$$\n",
    "\\hat\\x = \\z \\Vr^T\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Autoencoder (and variants such as VAE) produces $\\z^\\ip$, the latent representation of $\\x^\\ip$\n",
    "- directly\n",
    "- independent of any other training example $\\x^{(i')}$ for $i \\ne i'$\n",
    "\n",
    "One of our goals in using AE's is in generating synthetic data\n",
    "- the dimensionality reduction achieved thus far was a necessity, not a goal\n",
    "\n",
    "Our goal in introducing the Vector Quantized Autoencoder is not synthesizing data\n",
    "- it is to create a representation of complex data types that are similar to sequences (e.g., image, audio) \n",
    "- so that they can be mixed with other sequence data types (e.g., text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vector Quantized Autoencoder\n",
    "\n",
    "\n",
    "A *Vector Quantized VAE* is a VAE with similarities to PCA.  It creates $\\z$\n",
    "- which is an **integer**\n",
    "- that is the index of a row\n",
    "- in a codebook with $K$ rows\n",
    "\n",
    "That is: the input is represented by one of $K$ possible vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The goal is **not necessarily** dimensionality reduction.\n",
    "\n",
    "Rather, there are some advantages to a **discrete** representation of a continuously-valued vector.\n",
    "- Each vector\n",
    "- Drawn from the infinite space of continuously-valued vectors of length $n$\n",
    "- Can be approximated by one of $K$ possible vectors of length $n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus, a sequence of $T$ continuously valued vectors\n",
    "- can be represented as a sequence of $T$ integers\n",
    "- over a \"vocabulary\" defined by the code book\n",
    "\n",
    "This is analogous to text\n",
    "- sequence of words\n",
    "- represented as a sequence of integer indices in a vocabulary of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Once we put complex objects\n",
    "- like images\n",
    "- timeseries\n",
    "- speech\n",
    "\n",
    "into a representation similar to text\n",
    "- we can have *mixed type* sequences\n",
    "    - e.g., words, images\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In a subsequent module we will take advantage of mixed type sequences\n",
    "- to produce an image\n",
    "- from a text *description* of the image\n",
    "- using the \"predict the next\" element of a sequence technique of Large Language Models\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>DALL-E: Text to Image</center></th>\n",
    "    </tr>  \n",
    "    <tr>\n",
    "        <td>\n",
    "            Text input: \"An illustration of a baby daikon radish in a tutu walking a dog\"\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <center>Image output:</center>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"https://cdn.openai.com/dall-e/v2/samples/anthropomorphism/091432009673a3a126fdec860933cdce_26.png\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Details\n",
    "\n",
    "A VQ-VAE can produce a sequence of integers that encodes many different types of data\n",
    "- including data with \"shape\": *non-feature* dimensions\n",
    "    - image\n",
    "    - audio\n",
    "\n",
    "Each \"location\" in the space of non-feature dimensions\n",
    "- has a vector consisting only of features\n",
    "\n",
    "So an image, for example,\n",
    "- has a 2D space of non-feature dimensions: height by width\n",
    "- each location in the 2D grid is a vector of 3 features: intensities for Red, Green, Blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We illustrate the VQ-VAE using Image examples.\n",
    "\n",
    "Here is diagram of a VQ-VAE \n",
    "- that creates a latent representation of a 3-dimensional image $(w \\times h \\times 3)$\n",
    "- as a 2-dimensional matrix of integers\n",
    "\n",
    "There is a bit of notation: referring to the diagram should facilitate understanding the notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>VQ-VAE</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"https://i.imgur.com/R9VMWD6.png\" width = 200%></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In general, we assume the input has $\\#S$ non-feature (*spatial* in the case of images) dimensions\n",
    "- where each location in the spatial dimension is a vector of features with length $n$\n",
    "- input shape $(n_1 \\times n_2 \\ldots \\times n_{\\#S} \\times n)$\n",
    "\n",
    "We will explain this diagram in steps.\n",
    "\n",
    "First, we summarize the notation in a single spot for easy subsequent reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Notation summary**\n",
    "\n",
    "term &nbsp; &nbsp; &nbsp; &nbsp; | shape &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | meaning \n",
    ":---|:---|:---\n",
    "$S$ | $(n_1 \\times n_2 \\ldots \\times n_{\\#S})$ | Spatial dimensions of $\\#S$-dimensional input\n",
    "$\\x$ | $\\Reals^{S \\times n}$ | Input\n",
    "$D$ | | length of latent vectors (Encoder output, Quantized Encoder output, Codebook entry)\n",
    "$\\encoder$ | | Encoder function\n",
    "$\\mathbb{z}_e(\\x)$ | $\\mathbb{R}^{S \\times D}$ |  Encoder output over each location of spatial dimension\n",
    "                         | | $\\z_e(\\x) = \\encoder(\\x)$\n",
    "$\\z_e(\\x)$ | $\\mathbb{R}^D$ |  Encoder output at a **single** representative spatial location\n",
    "                         | | $\\z_e(\\x) = \\encoder(\\x)$        \n",
    "$K$ | | number of codes\n",
    "$\\codebook$ | $\\mathbb{R}^{K \\times D}$ | Codebook/Embedding\n",
    "  | | $K$ codes, each of length $D$\n",
    "$e \\in \\codebook$ | $\\mathbb{R}^D$  | code/embedding\n",
    "$\\mathbb{z}$ | $\\{1, \\ldots,  K\\}^{S \\times D}$ | latent representation over all spatial dimensions\n",
    "$\\z$ | $\\{1, \\ldots,  K\\}$ | Latent representation at a **single** representative spatial location\n",
    "| | one integer per spatial location\n",
    "$\\qr{ \\z | \\x }$ | $\\text{integer} \\in [1 \\ldots K] $| Index $k$ of $e_k \\in \\codebook$ that is closest to $\\z_e(\\x)$ \n",
    "| | $k = \\argmin{j \\in [1,K]} \\| \\z_e(\\x) - \\e_j \\|_2$\n",
    "         | | actually: encoded as a OHE vector of length $K$\n",
    "$\\z_q(\\x) $     | $\\mathbb{R}^D$ | Quantized $\\z_e(\\x)$\n",
    "|| $\\z_q(\\x) = e_k$ where  $k = \\qr{\\z | \\x }$\n",
    "| | i.e, the element of codebook that is closest to $\\z_e(\\x)$\n",
    "  | | $\\z_q(\\x) \\approx \\z_e(\\x)$\n",
    "$\\tilde\\x$ | $n$ | Output: reconstructed $\\x$\n",
    "       | | $\\pr{ \\x | \\z_q(\\x) }$ \n",
    "$\\decoder$  | $\\mathbb{R}^{n'} \\rightarrow \\mathbb{R}^n$ | Decoder\n",
    "| | input: element of codebook $\\codebook$\n",
    "| | $\\tilde\\x = \\decoder( \\z_q(\\x) )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quanitization\n",
    "\n",
    "Let $S$ denote the spatial dimensions, e.g. $S = (n_1 \\times n_2)$ for 2D\n",
    "\n",
    "So input $\\x \\in \\Reals^{S \\times n}$\n",
    "- $n$ features over $S$ spatial locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The input $\\x$ is transformed in a sequence of steps\n",
    "- Encoder output (continuous value)\n",
    "- Latent representation (discrete value)\n",
    "    - Quantized (continuous value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the first step, the *Encoder* maps input $\\x$ \n",
    "- to Encoder output $\\mathbb{z}_e(\\x)$\n",
    "- an alternate representation of $D$ features over $S'$ spatial locations\n",
    "\n",
    "(For simplicity, we will assume $S' = S$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Notational simplification**\n",
    "\n",
    "In the sequel, we will apply the same transformation **to each element** of the spatial dimension\n",
    "\n",
    "Rather than explicitly iterating over each location\n",
    "we write\n",
    "\n",
    "$$\\z_e(\\x) \\in \\Reals^D$$\n",
    "\n",
    "to denote a representative element of $\\mathbb{z}_e(\\x)$ at a single location $s = (i_1, \\ldots, i_{\\#S})$\n",
    "\n",
    "$$\n",
    "\\z_e(\\x) = \\mathbb{z}_e(\\x)_{s}\n",
    "$$\n",
    "\n",
    "We will continue the transformation at the single representative location\n",
    "- and implicitly iterate over all locations $s \\in S$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The continuous (length $D$) Encoder output vector $\\z_e(\\x)$\n",
    "- is mapped to a *latent representation* $\\qr{ \\z | \\x }$\n",
    "- which is a **discrete** value (integer)\n",
    "\n",
    "$$k = \\qr{ \\z | \\x } \\in \\{1, \\ldots, K\\}$$\n",
    "\n",
    "where $k$ is the *index* of a row $\\e_k$ in codebook $\\codebook$\n",
    "$$\\e_k = \\codebook_k \\in \\Reals^D $$\n",
    "\n",
    "The codebook is also called an *Embedding* table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$k$ is chosen such that $\\e_k$ is the row in $\\codebook$ **closest to** $\\z_e(\\x)$\n",
    "\n",
    " $$\\begin{array}\\\\\n",
    "        k & = & \\qr{ \\z | \\x } \\\\\n",
    "          & = &\\argmin{j \\in \\{ 1, \\ldots, K \\} } \\| \\z_e(\\x) - \\e_j \\|_2 \\\\\n",
    "          \\end{array}\n",
    "        $$\n",
    "\n",
    "We denote the codebook vector \n",
    "- closest to representative encoder output $\\z_e(\\x)$\n",
    "- as $\\z_q( \\x )$\n",
    "$$\n",
    "\\z_q( \\x ) = \\e_k \\text{ where } k = q(\\z|\\x)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Decoder tries to invert the codebook entry $\\e_k = \\z_q(\\x) $\n",
    "so that\n",
    "$$\n",
    "\\begin{array}\\\\\n",
    "\\tilde\\x & = & \\decoder( \\z_q(\\x) ) \\\\\n",
    "& \\approx & \\x \\\\\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discussion\n",
    "\n",
    "## Why do we need the CNN Encoder ?\n",
    "\n",
    "The input $\\x$ is first transformed into an *alternate representation*\n",
    "- the **number** and length of the non-feature dimensions (i.e., $h \\times w$)  are preserved\n",
    "    - conventient but not necessary\n",
    "- but the number of features is transformed from $n$ raw features to $D \\ge n$ synthetic features\n",
    "    - typical behavior for, e.g., an image classifier\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The part of the VQ-VAE after the initial CNN\n",
    "- reduces the size of the **feature dimension** from $D$ to 1\n",
    "- this is the primary source of dimensionality reduction\n",
    "    - the raw $n$ of image input is usually only $n=3$ channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Although our illustration preserved the non-feature dimensions\n",
    "- it may be useful for the CNN to *down-sample* spatial dimension $S$ to a smaller $S'$\n",
    "- resulting in shorter sequences when we eliminate the non-feature dimensions by \"flattening\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example\n",
    "- 3 layers of stride 2 CNN layers\n",
    "- will reduce a 2D image of spatial dimension $(n_1 \\times n_2)$\n",
    "- to spatial dimension $(\\frac{n_1}{8} \\times \\frac{n_2}{8})$\n",
    "\n",
    "This replaces each $(8 \\times 8 \\times n)$ *patch* of raw input\n",
    "- into a single vector of length $D$\n",
    "- that summarizes the $(8 \\times 8)$ the patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One possible role (not strictly necessary) for the CNN Encoder\n",
    "- is to replace a large spatial dimensions\n",
    "- by smaller \"summaries\" of local neighborhoods (patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why quantize ?\n",
    "\n",
    "Quantization \n",
    "- converts the continuous $\\z_e(\\x)$\n",
    "- into discrete $\\qr{ \\z | \\x }$\n",
    "- representing the approximation $\\z_q(\\x) \\approx \\z_e(\\x) $\n",
    "\n",
    "The Decoder inverts the approximation.\n",
    "\n",
    "Why bother when the Quantization/De-Quantization is Lossy ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One motivation comes from observing what happens if we *quantize and flatten* \n",
    "- the $\\#S'$-dimensional spatial locations \n",
    "- to a sequence of integers\n",
    "\n",
    "Quantizing replaces each patch with a single integer index.\n",
    "- the integer is the index of an *image token* within a list of $K$ possible tokens\n",
    "\n",
    "By flattening the quantized higher dimensional matrix of patches, we convert the input\n",
    "- into a sequence of image tokens\n",
    "- over a \"vocabulary\" defined by the codebook $\\codebook$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This yields an image representation\n",
    "- similar to the representation of text\n",
    "\n",
    "Thus, we open the possibility of processing sequences\n",
    "of mixed text and image tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quantized image embeddings mixed with Text: preview of DALL-E\n",
    "\n",
    "The Large Language Model operates on a sequence of text tokens\n",
    "- where the text tokens are fragments of words\n",
    "- when run autoregressively\n",
    "    - concatenating each output to the initial input sequence\n",
    "    - the LLM shows an ability to produce a \"sensible\" continuation of an initial \"thought\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose we train a LLM on input sequences\n",
    "- that start with a sequence of *text* tokens describing an image\n",
    "- followed by a separator `[SEP]` token\n",
    "- followed by a sequence of of quantized image tokens\n",
    "\n",
    "        <text token> <text token> ... <text token> [SEP] <image token> <image token> ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What continuation will our trained LLM produce given prompt\n",
    "\n",
    "        <text token> <text token> ... <text token> [SEP]\n",
    "        \n",
    "Hopefully:\n",
    "- a sequence of *image tokens*\n",
    "- that can be reconstructed\n",
    "- into an image matching the description given by the text tokens !\n",
    "\n",
    "That is the key idea behind a Text to Image model called DALL-E that we will discuss in a later module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There remains an important technical detail\n",
    "- the embedding space of text and image are distinct\n",
    "- they need to be merged into a common embedding space\n",
    "\n",
    "We will visit these issues in the module on CLIP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loss function\n",
    "\n",
    "The Loss function for the VQ-VAE entails several parts\n",
    "- Reconstruction loss\n",
    "    - enforcing constraint that reconstructed image is similar to input\n",
    "    $$\\tilde{\\x} \\approx \\x\n",
    "    $$\n",
    "- Vector Quantization (VQ) Loss:\n",
    "    - enforcing similarity of quantized encoder output and actual encoder output\n",
    "    $$\n",
    "    \\z_q(\\x) \\approx \\z_e(\\x)\n",
    "    $$\n",
    "- Commitment Loss\n",
    "    - a constraint that prevents the Quantization of $\\z_e(\\x)$ from alternating rapidly between code book entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stop Gradient operator\n",
    "\n",
    "Some of the Loss terms will involve an operator that we have not yet seen:\n",
    "- The `sg` operator is the *Stop Gradient* operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The need for this operator stems from the Gradient Descent update process\n",
    "- the partial derivative assumes \"everything else\" other than the denominator (variable being updated) remains constant\n",
    "- if \"everything else\" *also* changes: the gradient update step may not reduce Loss\n",
    "\n",
    "But\n",
    "- changing the Encoder parameters affects Encoder output $\\z_e(\\x)$\n",
    "    - which may affect the Embeddings\n",
    "\n",
    "Similarly\n",
    "- changing the Embeddings \"code book\" will affect the Encoder output (the key used for lookup in the code book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In addition\n",
    "- Quantization (selecting a discrete code from the code book) **is not** differentiable\n",
    "- \"hard\" rather than \"soft\" choice\n",
    "\n",
    "We need to be able to pass gradients backward through the non-differentiable operator.\n",
    "\n",
    "The Stop Gradient operator will facilitate all these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On the Forward Pass, it acts as an Identity operator\n",
    "$$\n",
    "\\text{sg}(\\x)  =  \\x\n",
    "$$\n",
    "\n",
    "But on the Backward Pass of Backpropagation: *it stops the gradient* from flowing backwards\n",
    "$$\n",
    "\\frac{\\partial \\, \\text{sg}(\\x)}{\\partial \\y} =  0 \\text{  for all } \\y\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reconstruction Loss\n",
    "\n",
    "The Reconstruction Loss term is our familiar: Maximize Likelihood \n",
    "- written to minimize the negative of the log likelihood, as usual\n",
    "$$\n",
    "\\pr{ \\x | \\z_q(\\x) } \n",
    "$$\n",
    "\n",
    "It will serve to update parameters for the Encoder and Decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we will see (section: \"Quantization is not differentiable\")\n",
    "- during the backward pass \n",
    "- the Loss gradient from the (quantized) Decoder input $\\z_q(\\x)$ \n",
    "- flows directly to the (continuous) Encoder output $\\z_e(\\x)$\n",
    "\n",
    "Effectively, for the purpose of gradient/weight update due to Reconstruction Loss:\n",
    "$$\\z_q(\\x) = \\z_e(\\x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So the Reconstruction Loss will **not** cause the code book embeddings to be updated\n",
    "- update to the Encoder parameters thus satisfies \"all else being constant\" in that the Code book does not change\n",
    "\n",
    "The Code Book updates will be the job of the two other Loss terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vector Quantization Loss\n",
    "\n",
    "The Vector Quantization Loss and Commitment Loss are similar.\n",
    "- differ only in the placement of the Stop Gradient\n",
    "\n",
    "Vector Quantization Loss:\n",
    "\n",
    "$$\n",
    "\\| \\text{sg} ( \\z_e(\\x) ) - \\z_q({\\x}) \\| \n",
    "$$\n",
    "\n",
    "where `sg` is the *Stop Gradient* Operator (details to follow).\n",
    "\n",
    "The purpose of the Vector Quantization Loss is to update the Embedding (codebook) $\\codebook$\n",
    "- by moving \"code\" $\\z_q({\\x}) = e_k$ closer to Encoder output $\\z_e(\\x)$\n",
    "- assuming Encoder output is held constant\n",
    "    - the stop-gradient on the Encoder output $\\z_e(\\x)$ prevents the Encoder output from being changed by the VQ Loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Commitment Loss\n",
    "\n",
    "Commitment Loss:\n",
    "$$\n",
    "\\| \\z_e(\\x) - \\text{sg} ( \\z_q(\\x) )  \\|\n",
    "$$\n",
    "\n",
    "It is similar to the Vector Quantization loss except for the placement of the Stop Gradient operator.\n",
    "\n",
    "The Stop Gradient in the Commitment Loss prevents a change in the Embeddings from affecting the Encoder weights (and thus, $z_e(\\x)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The reason for the different placement of the Stop Gradient operators in the VQ and Commitment Loss terms\n",
    "- there is a circular dependency\n",
    "    - encoder output $\\z_e(\\x)$ is affected by the Code Book quantization $\\z_q(\\x)$\n",
    "    - the Code Book quantization $\\z_q(\\x)$ is affected by the encoder output $\\z_e(\\x)$\n",
    "\n",
    "By holding one constant while updating the other\n",
    "- we ensure that the embeddings $\\E$ converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Total Loss\n",
    "\n",
    "Loss function\n",
    "\n",
    "$$\\begin{array} \\\\\n",
    "\\loss(\\x, \\decoder(\\e)) & = & || \\x − \\decoder(\\e)||_2^2 & \\text{Reconstruction Loss} \\\\\n",
    "& & + ||\\text{sg}[\\encoder(\\x)] − \\e||_2^2  & \\text{VQ loss, codebook loss: train codebook } \\e \\\\\n",
    "& & + β||\\text{sg}[\\e] − \\encoder(\\x)||_2^2 & \\text{Commitment Loss: force } E(\\x) \\text{  to be close to codebook entries} \\\\\n",
    "& &\\text{where } \n",
    "\\e = \\z_q(\\x)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Need the stop gradient operator $\\text{sg}$ to control the mutual dependence\n",
    "- of the Encoder output $\\encoder(\\x)$ and the chosen code $\\e$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Quantization is not differentiable\n",
    "\n",
    "There is a subtle but important problem.\n",
    "\n",
    "The Quantization operation\n",
    "\n",
    "$$\\begin{array}\\\\\n",
    "        k & = & \\qr{ \\z | \\x } \\\\\n",
    "          & = &\\argmin{j \\in \\{ 1, \\ldots, K \\} } \\| \\z_e(\\x) - \\e_j \\|_2 \\\\\n",
    "          \\end{array}\n",
    "        $$\n",
    "\n",
    "is **not differentiable** because of `argmin`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`argmin` is a problematic operation because\n",
    "- it contains a \"hard choice\" so is not differentiable\n",
    "    - output may change dis-continuously from index $k$ to index $k' \\ne k$\n",
    "    - for small changes in the input\n",
    "    - not continuous as the point of change\n",
    "- it may also be non-deterministic\n",
    "    - when minimum value occurs at more than one index\n",
    "    - when $\\e_k = \\e_{k'}$ for $k \\ne k'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There is a work-around\n",
    "- implement a `VectorQuantizer` layer\n",
    "- using a [Straight Through Estimator](Straight_Through_Estimator.ipynb)\n",
    "    - see that module for details of the technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We see this in\n",
    "the [Colab](https://keras.io/examples/generative/vq_vae/) \n",
    "implementation of Vector Quantization (the\n",
    "`VectorQuantizer` layer)\n",
    "```\n",
    "class VectorQuantizer(layers.Layer):\n",
    "...\n",
    "    def call(self, x):\n",
    "...\n",
    "        # Straight-through estimator.\n",
    "        quantized = x + tf.stop_gradient(quantized - x)\n",
    "```\n",
    "Code similar to the [`VectorQuantizer` of the paper's authors](https://github.com/deepmind/sonnet/blob/v1/sonnet/python/modules/nets/vqvae.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Code\n",
    "\n",
    "[Here](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/vq_vae.ipynb#scrollTo=LWYJf1MYvzap) is a Colab notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Creating synthetic examples\n",
    "\n",
    "Consider an example that is an Image.\n",
    "\n",
    "It is a structured arrangement of feature vectors\n",
    "- a 2D grid\n",
    "- each element of the grid is a vector of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In general: our examples may have an arbitrary number of dimensions.\n",
    "\n",
    "By convention, we will refer\n",
    "- to the last dimensions as the \"feature\" dimension\n",
    "- all the preceding dimensions as *non-feature* dimensions\n",
    "    - for an Image: *spatial* dimensions\n",
    "\n",
    "Denoting `#S` as the number of non-feature dimensions and $n_{(0)}$$ as the number of features, examples have shape\n",
    "\n",
    "$$(n_1 \\times  \\ldots \\times n_{\\#S} \\times n_{(0)})$$\n",
    "where `#S` denotes the number of non-feature dimensions (`#S` = 2 for an Image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Encoder creates an output $\\z_e(\\x)$  of shape\n",
    "$$(n_1 \\times  \\ldots \\times n_{\\#S} \\times n_e)$$\n",
    "which is quantized into $\\z_q(\\x)$  of shape\n",
    "$$(n_1 \\times  \\ldots \\times n_{\\#S} \\times 1)$$\n",
    "\n",
    "That is: the feature dimension of $\\z_q(\\x)$  is a single integer index into a learned codebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order for us to generate synthetic examples\n",
    "- we must create a Tensor $\\z_q(\\x)$  of $(n_1 \\times  \\ldots \\times n_{\\#S} \\times 1)$ integers (*latents*)\n",
    "- feed this to the Decoder\n",
    "- get a synthetic example as output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning the distribution of latents\n",
    "\n",
    "But we can't create $\\z_q(\\x)$ *completely at random*.\n",
    "- The elements at adjacent locations in the non-feature dimensions may not be **independent**\n",
    "\n",
    "For example: consider an Image\n",
    "- the pixels in an image are related to one another\n",
    "\n",
    "We must learn a distribution of $\\z_q(\\x)$ that respects the dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One solution\n",
    "- flatten  $\\z_q(\\x)$\n",
    "- from shape $(n_1 \\times  \\ldots \\times n_{\\#S} \\times 1)$\n",
    "- into a sequence\n",
    "$$\n",
    "\\z_{(1)}, \\z_{(2)}, \\ldots, \\z_{(n_1 * n_2 \\ldots * n_{\\#S}) }\n",
    "$$\n",
    "of integer indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can then learn the distribution of the latents\n",
    "- through auto-regressive modeling of the sequence sequence $\\z$\n",
    "$$\n",
    "\\pr{\\z_{(k+1)} | \\z_{(1)}, \\ldots, \\z_{(k)} }\n",
    "$$\n",
    "\n",
    "This is just like the Language Model objection for NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Aside**\n",
    "\n",
    "*Learning* a distribution is less restrictive than *assuming* a distribution\n",
    "\n",
    "For the case of \"structured\" examples: we have no choice.\n",
    "- adjacent elements are not independent\n",
    "\n",
    "But recall that, for the VAE, we *assumed* that the latents came from a Normal distribution.\n",
    "\n",
    "Use Auto-regressive modeling is a nice \"trick\" to learn distributions rather than having to assume\n",
    "a \"convenient\" functional form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
