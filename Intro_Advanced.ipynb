{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "A Deep Learning model\n",
    "- is a combination of components\n",
    "- each component parameterized\n",
    "- is fit (finding optimal values for parameters) based on a training dataset\n",
    "- by optimizing a Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the Intro course\n",
    "- things were \"small\": datasets, number of parameters\n",
    "- components were assembled in a simple but rigid order: sequence of Layers (the \"Sequential\" architecture)\n",
    "- Loss functions were few and pre-specified: Mean Squared Error, Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the Advanced course we depart from the simplifications of the Intro course\n",
    "- How to use Big Data in Small Memory\n",
    "- Deal with  models with parameters numbering in the billions\n",
    "- Introduce an unrestricted (technically: Directed Acyclic Graph) architecture for organizing components (the \"Functional\" model)\n",
    "- Re-using advanced models that are too big to construct ourselves\n",
    "- Loss functions that express the semantics of the task to be solved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Loss Functions we will see used in cutting-edge models are complex\n",
    "- less \"pure math\" (e.g., Mean Squared Error)\n",
    "- More: a mathematical formulation that captures the \"semantics\" of the task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here are some non-trivial tasks characterized by these more complex Loss functions\n",
    "\n",
    "<center><strong>Tasks with interesting Loss Functions</strong></center>\n",
    "<br>\n",
    "<center><strong>Synthetic Data: Create a timeseries of a cross section of returns of US equities</strong></center>\n",
    "<br>\n",
    "<center><strong>Neural Style Transfer: Transform a photo into a painting in the style of Van Gogh</strong></center>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><center><img src=images/chicago.jpg width=50%></center></td>\n",
    "        <td><strong>+</strong></td>\n",
    "        <td><center><img src=images/starry_night_crop.jpg width=30%></center></td>\n",
    "        <td><strong>=</strong></td>\n",
    "        <td><center><img src=images/chicago_starry_night.jpg width=70%></center></td>\n",
    "    </tr>\n",
    "</table>\n",
    "<br>\n",
    "<center><strong>Text to image: create cartoon of NYU professor teaching Machine Learning</strong></center>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><center><img src=\"images/Craiyon_NYU_professor_teaching_ML.png\" width=30%></center></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The objectives of the course are two-fold\n",
    "- Expose you to the cutting-edge **ideas** that we believe will form the basis for the future of Deep Learning\n",
    "- To equip you with the **technical skills** to participate in this future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Major Themes\n",
    "\n",
    "- Advanced Models\n",
    "    - Technical background\n",
    "    - Novel architectures\n",
    "- Modern Transfer Learning\n",
    "    - Unsupervised Pre-training with Supervised Fine-Tuning\n",
    "- Generative AI\n",
    "    - Synthetic Data\n",
    "    - Large Language Models (e.g., ChatGPT and relatives)\n",
    "        - novel uses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theme - Advanced Models\n",
    "\n",
    "We focus mostly on *concepts* but their utility is not maximized without *technical proficiency*.\n",
    "- being able to read code\n",
    "    - eliminates ambiguity\n",
    "- enables you to\n",
    "    - modify and adapt existing models\n",
    "    - create new models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Technical\n",
    "\n",
    "This is (to me) the least interesting, but necessary, part of the course\n",
    "- technical prerequisite for *understanding* and *implementing* state of the art models\n",
    "- necessary to understand the code behind the model\n",
    "\n",
    "Technical: so that **you** can code and use these models \n",
    "- How to build Functional models\n",
    "- How to use Big Data: the Dataset API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Novel architectures\n",
    "\n",
    "Conceptual: State of the art models.\n",
    "\n",
    "We will understand, analyze, and use the \"AI\" that is breathlessly reported in the popular press.\n",
    "- ChatGPT\n",
    "- DALL-E, Stable Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some highlights\n",
    "- Transformers\n",
    "    - architecture that forms the basis of many of the most advanced models\n",
    "- Autoencoders, Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Theme - Modern Transfer Learning\n",
    "\n",
    "This theme will be motivated by recent advances in Natural Language Processing: *Large Language Models (LLM)*\n",
    "- but is applicable for other tasks as well\n",
    "\n",
    "The problem:\n",
    "- Models are getting so big\n",
    "- That it is *impractical* for individuals/small organizations to compete with better-endowed players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "GPT is a family of Large Language Models.\n",
    "\n",
    "These models have recently captured the popular imagination (e.g., ChatGPT).\n",
    "\n",
    "GPT-3 is a newer member of the family\n",
    "- 175 billion parameters\n",
    "- trained on vast quantities of data\n",
    "\n",
    "For the most part: the techniques have been published and are well-known.\n",
    "\n",
    "Can you train your own GPT-3 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Cost of Training GPT-3 on your own**\n",
    "- Amazon Cloud G5 instance\n",
    "- NVidia A10G Tensor Core GPUs @ 250 Tflops/GPU\n",
    "- 8 GPU instance (2 Pflops) @\\\\$10/hour (with yearly contract; \\\\$16/hour on-demand)\n",
    "    - \\\\$240 per 2Pflops-day\n",
    "\n",
    "Training GPT-3 takes $\\approx$ 3000 Pflop-days\n",
    "- $ 3000/2 = 1500$ days G5 instances to get 3000 Pflops-days\n",
    "- Cost = 1500 * \\\\$240/day = \\\\$360K\n",
    "\n",
    "How much does a typo in your code cost !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Fortunately: pre-trained versions of these large models are often published\n",
    "- Model hubs/Model Zoos\n",
    "\n",
    "You can \"fine-tune\" these costly models (developed for broad tasks) to your *narrow* tasks\n",
    "- Unsupervised Pre-training with Supervised Fine-Tuning\n",
    "    - Fine-tune on small number of narrow task-specific examples\n",
    "    \n",
    "We will focus on the \"Model Hub\" from [Hugging Face](https://huggingface.co)\n",
    "- located just down the road !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There is another way of \"re-using\" costly pre-trained models\n",
    "\n",
    "*Zero Shot Learning*\n",
    "- is a method allowing a model trained for one task\n",
    "- to solve other tasks\n",
    "- *without* being trained on the other tasks\n",
    "- *often* with **no coding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, GPT (and its relatives) can form the basis of \"no programming\" new apps\n",
    "- exploiting Zero Shot Learning\n",
    "- *Prompt engineering*: specially engineered \"prompts\"\n",
    "    - often pre-pended to the feature vector for the new task to be solved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some new tasks that can be derived by using GPT and Zero Shot Learning\n",
    "- Summarize an article\n",
    "- Write an article !\n",
    "- ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Theme - Generative AI\n",
    "\n",
    "In the Intro course, many of our tasks were *discriminative*\n",
    "- learning a relationship between features and targets\n",
    "    - Classification: discriminating among a finite number of possible target labels\n",
    "    - Regression: continuous target\n",
    "\n",
    "Many of the tasks we will address in this course (including many listed above) are *generative*\n",
    "- Learning a distribution of feature vectors\n",
    "    - Answer is a sample from the learned distribution\n",
    "- Output is often structured, rather than label or numeric\n",
    "    - A block of text\n",
    "    - An image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Many Generative Models we will study are based on Large Language Models\n",
    "- text input to text output\n",
    "- text input to image output\n",
    "    - DALL-E\n",
    "    - Stable Diffusion\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><center><img src=\"images/Craiyon_NYU_professor_teaching_ML.png\" width=30%></center></td>\n",
    "    </tr>\n",
    "</table>    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A different (but interesting for Finance) use of Generative AI\n",
    "- Creating synthetic training examples\n",
    "\n",
    "Large models (many parameters) need lots of training data\n",
    "- Finance data (particularly at daily frequency) just not big enough\n",
    "\n",
    "Being able to create synthetic examples\n",
    "- facilitates large models for Finance\n",
    "- is a way of dealing with *class imbalance*\n",
    "    - lots of the interesting Finance phenomena are rare\n",
    "        - Risk Management\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Organization\n",
    "\n",
    "The themes are *not orthogonal*: many are linked\n",
    "- a Novel Architecture (the Transformer) is the basis of many models used for Transfer Learning\n",
    "- we need the Technical tools to understand the code (and to build) Advanced Models\n",
    "\n",
    "We also want to front-load the Technical tools presentation so that you may  be started on the Project.\n",
    "\n",
    "So we will probably wind up weaving back and forth between topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The goal\n",
    "- to give you the knowledge and tools\n",
    "- and intellectual background and curiousity\n",
    "\n",
    "to participate (and maybe lead) advances in ML and Finance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You will need both a solid conceptual basis and technical coding skills\n",
    "\n",
    "**Concepts** come first.  For example\n",
    "- Background motivating the Transformer\n",
    "- The Transformer in concept\n",
    "\n",
    "**Code** comes second.  For example\n",
    "- we will examine the code behind a Transformer implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Course organization\n",
    "\n",
    "We will be dealing with a lot of new concepts\n",
    "- many introduced only in the past 1-3 years\n",
    "\n",
    "The lectures will cover the *most important* points\n",
    "- function of limited time\n",
    "- *real* understanding and proficiency will come from reading the papers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Academic papers are dense and require effort to understand\n",
    "\n",
    "Academic papers have a particular style\n",
    "- more science and math\n",
    "- as opposed to engineering\n",
    "- assume the reader has substantial background\n",
    "\n",
    "This makes them a bit dense and requires effort to grasp.\n",
    "- lots of discussion and analysis\n",
    "- almost no code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, we take a look at one of the [early and important papers](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "  on Large Language Models.\n",
    "\n",
    "- The architecture is [described](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf#page=3)  a *multi-layer Transformer Decoder*\n",
    "    - reference to existing concept\n",
    "    - which is *clarified* via a set of recursive equations\n",
    "    \n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "h_0 & =  U W_e + W_p \\\\\n",
    "h_i & =  \\text{transformer_block} ( h_{i-1} ) &  \\\\\n",
    "&& \\text{for }  1 \\le i \\le n & \\\\\n",
    "\\pr{U} & =  \\text{softmax}( h_n  W_e^T ) &  \\\\\n",
    "\\end{array}\n",
    "$$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the Intro course, I would likely explain this via a diagram\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Stacked Transformer Layers (Encoder/Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Transformer_Encoder_Decoder_multi.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And I would likely add explanation to the equations\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "h_0 & =  U W_e + W_p  & \\text{concatenate Input Embedding and Positional Encoding}\\\\\n",
    "h_i & =  \\text{transformer_block} ( h_{i-1} ) & \\text{connect output of layer } (i-1) \\text{ to input of layer } i \\\\\n",
    "&& \\text{for }  1 \\le i \\le n & \\\\\n",
    "\\pr{U} & =  \\text{softmax}( h_n  W_e^T ) & \\text{Final output is probability distribution over vocabulary}\\\\\n",
    "& & h_n \\text{ is output of top transformer block} \\\\\n",
    "& & h_n  W_e^T \\text{reverses the embedding to obtain tokens in }U\n",
    "\\end{array}\n",
    "$$\n",
    "where\n",
    "$$ \n",
    "\\begin{array}[lll] \\\\\n",
    "U   & \\text{context of size } k: [ u_{-k}, \\ldots, u_{-1} ] \\\\\n",
    "W_e & \\text{token embedding matrix} \\\\\n",
    "W_p & \\text{position encoding matrix} \\\\\n",
    "h_i & \\text{Output of transformer block } i\\\\ \n",
    "n   & \\text{number of transformer blocks/layers} \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The model \"details\" are given in a singe [dense paragraph](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf#page=5)\n",
    "- full of references to other concepts\n",
    "\n",
    "And the optimization objective is described as\n",
    "maximizing \n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\mathcal{L}_2 ( \\mathcal{C} ) = \\sum_{(\\x,\\y)} { \\log \\pr{ \\y | \\x_1, \\ldots, \\x_m } }  \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "- where  $\\mathcal{C}$  is the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The experienced reader (the intended audience for the author) will recognize\n",
    "- the sum of logs\n",
    "- as log-likehood\n",
    "\n",
    "and immediately understand that the optimization is maximization of log-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the Introductory course\n",
    "- I would have described the objective as *Cross Entropy*\n",
    "    - which helps to relate it to the code needed for training\n",
    "- and would have helpfully pointed out\n",
    "$$\n",
    "\\y = \\text{softmax}(h_l^m W_y )\n",
    "$$\n",
    "\n",
    "to connect the mathematical concept (target $\\y$) to the *output of the final layer*\n",
    "of the multi-layer Transformer Decoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Be patient: the effort will be rewarded\n",
    "- faster to read a dense paper\n",
    "- deeper understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Academic papers focus on results and analysis rather than code\n",
    "\n",
    "In our limited time, we focus on concepts and code.\n",
    "\n",
    "But academic papers spend the bulk of their text on *analysis and discussion*.\n",
    "- lots of experimental results\n",
    "- discussion of results\n",
    "\n",
    "These are really important to those of you interested in deeper understanding of the field\n",
    "- but less important to those of you with an engineering focus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One common concept seen often is the *abltation study*\n",
    "- The present paper has introduced one or more novel concepts or features\n",
    "- Remove them one at a time (*ablation*)\n",
    "    - to try to understand the relative importance of each novel concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The HuggingFace course\n",
    "\n",
    "There will be a Final Project which involves coding.\n",
    "\n",
    "You will need to understand\n",
    "- Transfer Learning\n",
    "- Transformers\n",
    "- Natural Language Processing concepts\n",
    "- Keras code to implement all of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In our lectures\n",
    "- you will learn all the necessary concepts\n",
    "- be introduced to the coding techniques\n",
    "\n",
    "We will be using [HuggingFace](https://https://huggingface.co/) as our \"model hub\"\n",
    "- the source of pre-trained models\n",
    "- that we will adapt (fine-tune) to solve a particular task\n",
    "    - e.g., the Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "I *strongly suggest* that you follow the excellent [HuggingFace course](https://huggingface.co/course)\n",
    "- reinforces the concepts\n",
    "- but *emphasizes the coding* that you will need for the Final Project\n",
    "    - worth being familiar with their Hub by the middle of this course\n",
    "    \n",
    "There are no assignments (other than the Final Project) in this course\n",
    "- you can consider the time on the HuggingFace course as being time spent in lieu of homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The HuggingFace course contains code in *either* Keras or Pytorch\n",
    "- we will use Keras in this course\n",
    "- there is a toggle (upper right) on most HuggingFace pages to switch between Keras and Pytorch\n",
    "- some advanced features (e.g., the Trainer) are not (yet) available in Keras\n",
    "    - but are not necessary for our course (convenience rather than necessity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
