{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb\n",
    "%run ml_advanced_profile.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**References**\n",
    "\n",
    "- [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/pdf/2202.12837.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What makes In-context Learning work ?\n",
    "\n",
    "- [blog](http://ai.stanford.edu/blog/understanding-incontext/)\n",
    "- [paper](https://arxiv.org/pdf/2202.12837.pdf)\n",
    "    - more empirical\n",
    "    - various models\n",
    "        - MetaICL: trained with InContextLearning objective\n",
    "        - 2 methods: Direct vs Channel ???\n",
    "    - gold-label vs random (uniform sampling) label: ground-truth not necessary\n",
    "        - gold improves over zero shot\n",
    "        - random: small decrease vs gold\n",
    "            - **very** small for MetaICL\n",
    "        - sampling for true label distribution: smaller decrease\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How does In Context Learning work ?\n",
    "\n",
    "*In-context Learning* describes a means of using a fixed LLM to solve a task\n",
    "- by supplying some number $k$ of *exemplars* (or *demonstrations*) of the new task\n",
    "- as a *pre-prompt*\n",
    "- and the presenting a prompt $\\x$ to the model\n",
    "- expecting the model to produce a $\\y$\n",
    "- that is the correct \"response\" to the task on input $\\x$\n",
    "\n",
    "<img src=\"images/icl_example.png\">\n",
    "\n",
    "Atttribution: https://arxiv.org/pdf/2202.12837.pdf#page=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In-Context Learning appears to be a way\n",
    "- of extending a LM\n",
    "- *without* further training\n",
    "    - as opposed to Fine-Tuning\n",
    "- since\n",
    "    - the exemplars are given at *test* time\n",
    "    - no parameter  updates to the LLM occur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In-Context Learning uses a pre-trained LLM and the trick of using the Universal Text API\n",
    "- to turn the new task\n",
    "- into a text-continuation (\"predict the next\") task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is:\n",
    "- given some number $k$ of exemplars: $\\langle \\x^{(1)}, \\y^{(1)} \\rangle, \\ldots, \\langle \\x^{(k)}, \\y^{(k)} \\rangle $\n",
    "- the prompt string $\\x$\n",
    "\n",
    "we create a sequence $\\dot \\x$ encoding the exemplars and prompt\n",
    "- and ask the LLM model to predict\n",
    "\n",
    "$$\n",
    "\\pr{\\y | \\dot \\x }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A common way to create the sequence $\\dot \\x$\n",
    "by concatenating the exemplars and prompt, using separator characters to as delimiters.\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\dot \\x = \\text{concat} (  & \\x^{(1)}, \\langle \\text{SEP}_1 \\rangle, \\y^{(1)}, \\langle \\text{SEP}_2 \\rangle,  \\\\\n",
    "              &   \\vdots \\\\\n",
    "              &   \\x^{(k)}, \\langle \\text{SEP}_1 \\rangle, \\y^{(k)}, \\langle \\text{SEP}_2 \\rangle, \\\\\n",
    "              &   \\x \\\\\n",
    "              & ) \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The LLM then computes\n",
    "$$\n",
    "\\pr{ \\y | \\dot \\x }\n",
    "$$\n",
    "\n",
    "For notational convenience, we will omit writing the concatenation\n",
    "- and just write this as the conditional probability\n",
    "$$\n",
    "\\pr{\\y | \\x,  \\x^{(1)}, \\y^{(1)}, \\ldots, \\x^{(k)}, \\y^{(k)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But why should this work ?\n",
    "\n",
    "More interestingly\n",
    "- what is a good theory\n",
    "- and how can we test it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will present a [paper](https://arxiv.org/pdf/2202.12837.pdf)\n",
    "that attempts to present some insights into the process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Testing some theories\n",
    "\n",
    "In order to test a theory\n",
    "- various aspects of the exemplars are proposed as variables\n",
    "- one variable at a time is perturbed\n",
    "- the effect of the perturbations is measured across a range of benchmarks\n",
    "- and compare to measurements before the perturbation\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The results are summarized in the following diagram\n",
    "- that we will subsequently refer to for each experiment\n",
    "\n",
    "<img src=\"images/incontext_gold_vs_random.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This chart shows the result of perturbations\n",
    "- run across a variety of models\n",
    "- of sizes ranging from 774M to 175B parameters\n",
    "- each experiment is averaged across multiple benchmarks\n",
    "\n",
    "The number of demonstrations, when present, is $k = 16$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Zero shot verus $k \\ge 1$ shot\n",
    "\n",
    "The first experiment measures the effect of the presence/absence of exemplars.\n",
    "\n",
    "In the diagram, compare\n",
    "- \"No demos\": the blue bar\n",
    "- \"Gold labels\": the gold bar\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "$k \\ge 1$ exemplars *improves performance* relative to zero-shot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parts of the Context\n",
    "\n",
    "The next set of experiments varies parts of the context (exemplars and prompt).\n",
    "\n",
    "Given exemplars  $\\langle \\x^{(1)}, \\y^{(1)} \\rangle, \\ldots, \\langle \\x^{(k)}, \\y^{(k)} \\rangle $\n",
    "the authors posit some salient characteristics\n",
    "- the *input distribution* $I$ from which the exemplar *features*  are drawn $\\x^{(1)}, \\ldots, \\x^{(k)}$\n",
    "- the distribution $L$ of the exemplar *labels* $\\y^{(1)}, \\ldots, \\y^{(k)}$\n",
    "- the feature/label mapping relationship $M$\n",
    "    - i.e., the pair of $\\x^\\ip$ and $\\y^\\ip$, for $1 \\le i \\le k$\n",
    "- formatting\n",
    "    - the encoding of the exemplars and prompt into $\\dot \\x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feature/label mapping relationship\n",
    "\n",
    "Let $\\mathcal{C}$ denote the set from which exemplar labels are drawn.\n",
    "\n",
    "In this experiment, replace\n",
    "- correct label $\\y^\\ip$ for exemplar $i$\n",
    "- with label $\\tilde \\y^\\ip$ drawn at random (uniformly) from $\\mathcal{C}$.\n",
    "\n",
    "That is, we preserve $I$ and $L$, but break $M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the diagram, compare\n",
    "- \"Gold labels\": the gold bar (true labels)\n",
    "- \"Random labels\": the reddish bar\n",
    "    \n",
    "**Conclusions**\n",
    "- Correct (\"gold\") labels improve performance over random labels\n",
    "    - but not as much as expected, perhaps\n",
    "- Random labels *improves performance* over *no exemplars*\n",
    "    - \"Ground truth\" matters surprisingly little !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The fact that an *incorrect* $M$ improves performance relative to no exemplars is surprising.\n",
    "\n",
    "This suggests \n",
    "- that the exemplars are used to infer the *task to be performed*\n",
    "\n",
    "\n",
    "- once the task has been identified\n",
    "    - the exemplar mis-labeling is ignored\n",
    "- the model is able to perform the task as it was *trained* during training\n",
    "\n",
    "See the [Signifier theory in the module](hPrompt_Engineering_Suggestions.ipynb#Signifier:-direct-specification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Input distribution\n",
    "\n",
    "In this experiment\n",
    "- each exemplar input $\\x^\\ip$ is replaced by \n",
    "- a random $\\x_\\text{rand}^\\ip$ drawn from a text corpus *other than the one used for Training*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We note that this experiment *also* breaks the feature/label relationship $M$\n",
    "- we preserve the original labels $\\y^\\ip$ for exemplar $i$\n",
    "- which is not necessarily related to $\\x_\\text{rand}^\\ip$ \n",
    "\n",
    "We can contrast the results of this experiment the effect of breaking $M$ alone.\n",
    "\n",
    "<img src=\"images/incontext_input_distr.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the above diagram, compare\n",
    "- the lavender (third bar from left): perturbed $I$ and $M$\n",
    "- the red bar (second bar from left): perturbed $M$ alone\n",
    "\n",
    "**Conclusions**\n",
    "\n",
    "The $M$ relationship is broken in both cases. But\n",
    "- preserving the original distribution $I$ of exemplar features\n",
    "- improves performance relative to changing the distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why might this be ?\n",
    "\n",
    "The suggestion is that the model was trained with the LLM objective (\"predict the next\")\n",
    "- from a training distribution\n",
    "- and $\\x_\\text{rand}$ is from a *different* distribution\n",
    "- so the model struggles on non-training input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Output distribution\n",
    "\n",
    "In this experiment\n",
    "- each exemplar label $\\y^\\ip$ (from $\\mathcal{C}$) is replaced by \n",
    "- $\\tilde\\y^\\ip$ a *random* English word from $\\mathcal{C}_\\text{rand}$\n",
    "\n",
    "Note that we *also break* $M$\n",
    "- e.g., $\\y^\\ip = \\y^{(i')}$ does not imply $\\tilde\\y^\\ip = \\tilde\\y^{(i)}$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Gold labels</center></th><th><center>Random paired labels</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/incontext_output_space1.png\"></td>\n",
    "        <td><img src=\"images/incontext_output_space2.png\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Attribution: http://ai.stanford.edu/blog/understanding-incontext/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/incontext_output_distr.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the above diagram, compare\n",
    "- the red bar: random  labels\n",
    "- the turquoise bar (third from left): random words\n",
    "\n",
    "The difference: Although we break $M$ in both cases\n",
    "- in the \"random labels\" case: the labels are chosen from the correct output distribution $\\mathcal{C}$\n",
    "- in the \"random words\" case: the labels come from a distribution other than $\\mathcal{C}$\n",
    "\n",
    "\n",
    "**Conclusions**\n",
    "\n",
    "The $M$ relationship is broken in both cases. But\n",
    "- preserving the original distribution $L$ of exemplar labels\n",
    "- improves performance relative to changing the distribution of labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Formatting\n",
    "\n",
    "In this experiment\n",
    "- *format* is defined as the *pairing* of a feature and label within an exemplar\n",
    "- not necessarily a *correct pairing*: mapping $M$ not necessarily correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One experiment is run\n",
    "- with *only* exemplar features (and no exemplar labels): $\\x^{(1)}, \\ldots, \\x^{(k)}$\n",
    "- natural comparison is with experiment of correct format\n",
    "    - $\\x^\\ip$\n",
    "    - paired with random English words (from $\\mathcal{C}_\\text{rand}$) as labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A second experiment is run\n",
    "- with *only* exemplar labels (and no exemplar features): $\\y^{(1)}, \\ldots, \\y^{(k)}$\n",
    "- natural comparison is with experiment of correct format\n",
    "    - a random $\\x^\\ip_\\text{rand}$ drawn from a text corpus \n",
    "    - paired with $\\y^\\ip$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Both comparison experiments\n",
    "- preserve the format: feature/exemplar pairs\n",
    "- without preserving $M$\n",
    "- or the distribution $I$ in the first case, and $L$ in the second case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/incontext_format.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Conclusions**\n",
    "\n",
    "- Not keeping the format has performance *on par* with **no demonstrations** at all\n",
    "- Keeping the format retains most of the benefit achievable with either\n",
    "    - correct $I$ (but incorrect $M$)\n",
    "    - or correct $L$ (but incorrect $M$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The suggestion is that correct format an important feature\n",
    "- to enable the LLM to recognize the task from exemplars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exemplars that differ from the LLM\n",
    "\n",
    "The on-line articles makes another interesting observation.\n",
    "\n",
    "Observe \n",
    "the encoding of \n",
    "- the exemplars  \n",
    "$\\langle \\x^{(1)}, \\y^{(1)} \\rangle, \\ldots, \\langle \\x^{(k)}, \\y^{(k)} \\rangle $\n",
    "- the prompt string $\\x$\n",
    "\n",
    "into the string $\\dot \\x$ that is the input to the LLM\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\dot \\x = \\text{concat} (  & \\x^{(1)}, \\langle \\text{SEP}_1 \\rangle, \\y^{(1)}, \\langle \\text{SEP}_2 \\rangle,  \\\\\n",
    "              &   \\vdots \\\\\n",
    "              &   \\x^{(k)}, \\langle \\text{SEP}_1 \\rangle, \\y^{(k)}, \\langle \\text{SEP}_2 \\rangle, \\\\\n",
    "              &   \\x \\\\\n",
    "              & ) \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The distribution from which encoded $\\dot \\x$ is drawn\n",
    "- is probably *much different* than \n",
    "- the distribution (Internet text documents) on which the LLM was trained\n",
    "\n",
    "in several ways\n",
    "- syntax\n",
    "    - sentences (e.g., exemplars)\n",
    "    - are not naturally separated by an inter-example separator $\\langle \\text{SEP} \\rangle$ (whatever is chosen) in the training distribution\n",
    "- coherence\n",
    "    - exemplars $i$ and $i+1$ \n",
    "    - many not naturally follow one another in the training distribution\n",
    "        - may be different topics\n",
    "        - but demonstrate the same concept (that is why they were chosen as exemplars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The article posits that \n",
    "- these encoding anomalies\n",
    "- are low-frequency noise\n",
    "- that the LLM is able to ignore\n",
    "- providing there is more \"signal\" in the exemplars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A theory of In-Context Learning\n",
    "\n",
    "A more [theoretical paper](https://arxiv.org/pdf/2111.02080.pdf)\n",
    "and accompanying [online article](http://ai.stanford.edu/blog/understanding-incontext/)\n",
    "- combine these experimental insights\n",
    "- into a theory\n",
    "- and mathematical model of the theory\n",
    "- that is consistent with the experimental results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The authors posit\n",
    "- during training, the LLM learns \"concepts\", for example\n",
    "    - abstract ideas\n",
    "        - question answering\n",
    "        - sentiment\n",
    "    - plans\n",
    "        - how to solve a multi-step task: travel directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <center><strong>Concepts</strong></center>\n",
    "    <img src=\"images/incontext_concepts.png\">\n",
    "</table>\n",
    "\n",
    "Attribution: https://arxiv.org/pdf/2111.02080.pdf#page=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The model's LLM \"predict the next\" training objective did not specify to goal of learning concepts.\n",
    "\n",
    "But \n",
    "- summarizing a large number of similar training documents (e.g., collection of biographies)\n",
    "- in a parameters-constrained model\n",
    "- logically suggests that concepts emerge as a way of reducing parameter usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The authors suggest that the LLM's probability of outputting $\\y$ given prompt $\\x$\n",
    "is formed by\n",
    "$$\n",
    "\\pr{\\y | \\x } = \\int_{c \\in \\text{Concepts}} \\pr{\\y | \\x, c } \\pr{ c } \\; d(c)\n",
    "$$\n",
    "\n",
    "That is, the output\n",
    "- is the sum over all concepts\n",
    "- of the probability of outputting $\\y$ given prompt $\\x$ and concept $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Furthermore: the context (i.e., exemplars) of in-context learning\n",
    "- helps the LLM identify the concept $c$\n",
    "- to which the prompt $\\x$ implicitly refers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The experimental results seem to suggest that the exemplars\n",
    "- don't need to be fully accurate\n",
    "    - the model tolerates inaccurate mappings $M$ between feature input space $I$ and label space $L$\n",
    "- that correctly identifying $I$ and $L$ through the exemplars is\n",
    "    - advantageous\n",
    "    - but not completely necessary\n",
    "- that the *format* of the exemplar\n",
    "    - paired features and labels\n",
    "    - is important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Under this theory\n",
    "- the exemplars **are not teaching** new concepts\n",
    "    - hence $M$ can be inaccurate\n",
    "- but serving to help the LLM **identify**  a concept learned in training\n",
    "\n",
    "That is, the encoded exemplars in $\\dot \\x$\n",
    "- are related to $\\pr{c}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Once the concept $c$ is identified, the output $\\y$ depends\n",
    "-  on the distributions $I$ and $L$\n",
    "- on the mapping $M$\n",
    "\n",
    "that were learned during training.\n",
    "\n",
    "\n",
    "The actual output $\\y$\n",
    "$$\n",
    "\\pr{ \\y | \\x, \\c }\n",
    "$$\n",
    "depends on training examples\n",
    "- the exemplars \n",
    "$$\\langle \\x^{(1)}, \\y^{(1)} \\rangle, \\ldots, \\langle \\x^{(k)}, \\y^{(k)} \\rangle $$\n",
    "do not appear in the equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
