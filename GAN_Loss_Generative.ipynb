{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb\n",
    "%run beautify_plots.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# How does the GAN make $\\pdata \\approx \\pmodel$ ?\n",
    "\n",
    "The Generator Loss function we constructed is a proxy to achieve the goal\n",
    "\n",
    "$$\\pmodel \\approx \\pdata$$\n",
    "\n",
    "That is: the distribution of samples produced by the Generator is (approximately) the same as the \"true\" distribution\n",
    "- we note that we don't know the \"true\" $\\pdata$\n",
    "    - we only have available a sample and those the training set defines an *empirical* distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are several ways to quantify\n",
    "\n",
    "$$\\pmodel \\approx \\pdata$$\n",
    "\n",
    "One choice would be the minimization of KL Divergence\n",
    "- $\\KL( \\pdata || \\pmodel)$\n",
    "\n",
    "As a reminder: we now show that this is equivalent to Maximum Likelihood estimation\n",
    "\n",
    "Choose $\\pmodel$ to Minimize\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\KL( \\pdata || \\pmodel ) & = & \\int_\\x  { \\pdata(\\x) \\, \\left( \\log\\frac{\\pdata(\\x)}{\\pmodel\\x)} \\right) }{d\\x} & \\text{Definition of KL Divergence} \\\\\n",
    "& = & \\E_{\\x \\in \\pdata} \\log(\\pdata(\\x) - \\log(\\pmodel(\\x)) & \\text{Definiton of log of ratio as difference in logs} \\\\\n",
    "\\text{minimizing KL} \\\\\n",
    "& \\approx & \\E_{\\x \\in \\pdata} - \\log(\\pmodel(\\x))  & \\text{Since } $\\log(\\pdata(\\x))$ \\text{ is only a constant in the term being minimized} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "So minimizing $\\KL$ is equivalent to minimizing the Negative Log Likelihood.\n",
    "mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice that the expectation is over the \"true\" distribution $\\pdata$.\n",
    "\n",
    "The expectation is certainly reasonable for training put perhaps not best for the purposes of generating synthetic data\n",
    "- Measures fidelity to training data\n",
    "- NOT how \"realistic\" the synthetic data is\n",
    "- the penalty for $\\pmodel$ placing large probability mass around a particular $\\hat{\\x}'$\n",
    "is small when $\\pmodel{\\hat{\\x}'} \\approx 0$\n",
    "    - so Generator may create large quantity of synthetic data that is improbable given the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we knew the true $\\pdata$, a better objective to minimize for the purpose of generating synthetic data would\n",
    "be the similar\n",
    "$$\n",
    "\\KL( \\pmodel || \\pdata)\n",
    "$$\n",
    "\n",
    "which is equivalent to maximizing\n",
    "$$\n",
    "\\E_{\\x \\in \\pmodel} - \\log(\\pdata(\\x))\n",
    "$$\n",
    "\n",
    "The expectation is over the synthetic data, not the true data\n",
    "- $\\log(\\pdata(\\x))$ is defined as log of Perplexity\n",
    "    - an element of \"surprise\" in seeing $\\x$\n",
    "- So the expectation asks: for each synthetic datum generated, how likely is it to occur in the true distribution ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is merely a theoretical argument\n",
    "- In practical terms: we only have empirical $\\pdata$\n",
    "- So can't evaluate  log Perplexity $\\pdata(\\hat{\\x})$ for $\\hat{\\x} \\in \\pmodel$ unless synthetic $\\hat{\\x}$ replicates a sample in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Jensen-Shannon Divergence\n",
    "\n",
    "We have observed that the KL divergence is *not* symmetric\n",
    "$$\n",
    "\\KL( P || Q ) \\ne \\KL( Q || P )\n",
    "$$\n",
    "because the expectations are taken over different distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An alternative measure of similarity of two distributions is the Jensen-Shannon Divergence (JSD)\n",
    "\n",
    "  $$\n",
    "    \\begin{array} \\\\\n",
    "    \\text{JSD}( P || Q ) & = & \\text{JSD}( Q || P )\\\\\n",
    "    & = & \\frac{1}{2} \\; \\text{KL} \\left( P \\, ||\\, \\frac{P+Q}{2} \\right) + \\\\\n",
    "    && \\frac{1}{2} \\; \\text{KL} \\left( Q \\, || \\, \\frac{P+Q}{2} \\right)\n",
    "    \\end{array}\n",
    "    $$\n",
    "    \n",
    "This measure is\n",
    "- symmetric\n",
    "- is a kind of mixture of $\\KL(P || Q)$ and $\\KL(Q || P)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Huszar](https://arxiv.org/pdf/1511.05101.pdf) has a Generalized JSD which interpolates between the two terms\n",
    "$$\n",
    "    \\begin{array} \\\\\n",
    "    \\text{JSD}_\\pi( P || Q ) & = & \\text{JSD}( Q || P )\\\\\n",
    "    & = & \\pi \\; \\text{KL} \\left( P \\, ||\\, \\pi P + (1-\\pi) Q \\right) + \\\\\n",
    "    && (1-\\pi) \\; \\text{KL} \\left( Q \\, || \\, \\pi P + (1-\\pi) Q \\right)\n",
    "    \\end{array}\n",
    "    $$\n",
    "    \n",
    "The Generalized JSD\n",
    "- **Not** symmetric although\n",
    "$$\n",
    "\\text{JSD}_\\pi( P || Q ) = \\text{JSD}_{1-\\pi}( Q || P )\n",
    "$$\n",
    "- Is similar to Maximum Likelihood when $\\pi \\approx \\epsilon$\n",
    "- Is similar to $\\KL(Q || P )$ when $\\pi \\approx (1-\\epsilon)$\n",
    "\n",
    "$$\n",
    "\\frac{\\text{JSD}_{1-\\epsilon}( P || Q )}{1-\\epsilon} \\approx \\text{KL}( Q || P )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In implementing Generalized JSD\n",
    "- The Discriminator is trained (as usual) on a mix of real an fake examples\n",
    "    - But *not* in equal numbers\n",
    "    - $\\pi$ is fraction of  samples  from $Q$\n",
    "    - $(1-\\pi)$ is fraction of samples from $P$\n",
    "    - $\\pi \\lt \\frac{1}{2}$: real samples over represented\n",
    "    - $\\pi \\gt \\frac{1}{2}$: biased toward $Q$\n",
    "- Explains why we often see training with Generator updated twice for each update of Discriminator ?\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adversarial Training and the Jensen-Shannon Divergence\n",
    "\n",
    "The Discriminator Loss $\\loss_D$, summed over all examples (ignoring the $\\frac{1}{2}$ from the previous presentation where we assumed equal number of Real and Fake)\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\ \n",
    "\\loss_D \n",
    "& = &  - \\left(  \\E_{\\x^\\ip \\in \\pdata } { \\log D(\\x^\\ip) }  + \\E_{\\x^\\ip  \\in \\pmodel} { \\log \\left( 1 - D(\\x^\\ip)  \\right) } \\right) & D(G(\\z)) = \\x^\\ip \\text{ for fake examples}\\\\\n",
    "\\end{array}\n",
    "$$ \n",
    "\n",
    "We also showed that the optimal Discriminator results in \n",
    "$$\n",
    "D^*(\\x) =  \\frac{\\pdata (\\x)}{ \\pmodel(\\x) +\\pdata(\\x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Plugging $D^*(\\x)$ into $\\loss_D$ (Goodfellow Equation ):\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\ \n",
    "\\loss_D \n",
    "& = &  - \\left(  \\E_{\\x^\\ip \\in \\pdata } { \\log  \\frac{\\pdata (\\x)}{ \\pmodel(\\x) +\\pdata(\\x)} }  + \\E_{\\x^\\ip  \\in \\pmodel} { \\log  \\frac{\\pmodel (\\x)}{ \\pmodel(\\x) +\\pdata(\\x)} } \\right) \\\\\n",
    "& = & \n",
    "- \\left(  \\KL( \\pdata || \\pmodel(\\x) +\\pdata(\\x)) + \\KL(\\pmodel ||  \\pmodel(\\x) +\\pdata(\\x) \\right) & \\text{Def. of } \\KL \\\\\n",
    "& = & - \\left( \\log 4 + \\KL( \\pdata || \\frac{\\pmodel(\\x) +\\pdata(\\x)}{2}) + \\KL(\\pmodel || \\frac{ \\pmodel(\\x) +\\pdata(\\x)}{2} \\right) & \\text{dividing second arg. of each KL term by 2}  \\\\\n",
    "& & & \\text{translates into } - \\log 2 \\text{ in expansion of each KL term} \\\\\n",
    "& & & \\text{into log form.} \\\\\n",
    "& & & \\text{The } \\log 4 \\text{ offsets this}  \\\\\n",
    "& = & - \\left( \\log 4  + 2 * \\text{JSD} (  \\pdata || \\pmodel ) \\right) & \\text{Def. of JSD}\\\\\n",
    " & & & \\text{this is Equation 6 of Goodfellow}\\\\\n",
    "\\end{array}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus Goodfellow proves that solving the minimax optimally minimizes\n",
    "the JSD divergence between $\\pdata$ and $\\pmodel$.\n",
    "\n",
    "To summarize\n",
    "- $\\loss_D$ is implemented by KL Divergence\n",
    "- *Under the assumption* that the Discriminator can train to be the **optimal** adversary\n",
    "    - $\\loss_D$ becomes equivalent to the Jensen-Shannon Distance"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
