{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<html>\n",
    "<p style=\"font-size:32px\"><strong>Preliminaries</strong></p>\n",
    "</html>\n",
    "\n",
    "<html>\n",
    "<p style=\"font-size:26px\"><strong>Week 0</strong></p>\n",
    "</html>\n",
    " \n",
    "\n",
    "**Plan**\n",
    "- Setting up your learning and programming environment\n",
    "\n",
    "\n",
    "**Getting started**\n",
    "- [Setting up your ML environment](Setup_NYU.ipynb)\n",
    "    - [Choosing an ML environment](Choosing_an_ML_Environment_NYU.ipynb)\n",
    "- [Quick intro to the tools](Getting_Started.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 1\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We give a brief introduction to the course.\n",
    "\n",
    "We then present the key concepts that form the basis for this course\n",
    "- For some: this will be review\n",
    "\n",
    "\n",
    "## Intro to Advanced Course\n",
    "\n",
    "- [Introduction to Advanced Course](Intro_Advanced.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Review/Preview of concepts from Intro Course\n",
    "\n",
    "**Notation, concepts**\n",
    "\n",
    "Here is a *quick reference* of key concepts/notations from the Intro course\n",
    "- For some: it will be a review, for others: it will be a preview.  \n",
    "- We will devote a sub-module of this lecture to elaborate on each topic in slightly more depth.\n",
    "    - For a more detailed explanation: please refer to the material from the Intro course ([repo](https://github.com/kenperry-public/ML_Spring_2023))\n",
    "    \n",
    "- [Review and Preview](Review_Advanced.ipynb)\n",
    "\n",
    "**Colab**\n",
    "\n",
    "You may want to run your code on Google Colab in order to take advantage of powerful GPU's.\n",
    "\n",
    "Here are some useful tips:\n",
    "\n",
    "[Google Colab tricks](Colab_practical.ipynb)\n",
    "\n",
    "**Review: in-depth**\n",
    "\n",
    "### [Transfer Learning: Review ](Review_TransferLearning.ipynb)\n",
    "  \n",
    "### [Transformers: Review](Review_Transformer.ipynb)\n",
    "\n",
    "**Suggested reading**\n",
    "- Attention\n",
    "    - [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)  \n",
    "- Transfer Learning    \n",
    "    - [Sebastian Ruder: Transfer Learning](https://ruder.io/transfer-learning/)\n",
    "    \n",
    "**Further reading**\n",
    "- Attention\n",
    "    - [Neural Machine Translation by Jointly Learning To Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "    - Geron Chapter 16\n",
    "    - [An Analysis of BERT's Attention](https://arxiv.org/pdf/1906.04341.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 2: Review/Preview (continued); Technical\n",
    "\n",
    "**Preview**\n",
    "\n",
    "We continue with our \"review\" of Attention and follow up by showing how to implement it.\n",
    "\n",
    "This will enable us to \"review\" the Transformer\n",
    "- a fundamental architecture for processing sequences\n",
    "- built very heavily upon Attention\n",
    "\n",
    "The Transformer is important because it is the tool upon which Large Language Models are built.\n",
    "\n",
    "\n",
    "We will then take a detour and study the Functional model architecture of Keras.  Unlike the Sequential model, which is an ordered sequence of Layers, the organization of blocks in a Functional model is more general.  The Advanced architectures (e.g., the Transformer) are built using the Functional model.\n",
    "\n",
    "Once we understand the technical prerequisites, we will examine the code for the Transformer.\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We continue the review/preview of key concepts that we started last week.\n",
    "\n",
    "Our ultimate goal is to introduce the Transformer (which uses Attention heavily) in theory, and demonstrate its use in Large Language Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review/preview continued\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Encoder/Decoder without Attention<br>Bottleneck</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_Encoder_Decoder_1.png\" width=80%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Encoder_Decoder_no_attention.png\" width=70%</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Encoder/Decoder with Attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_Encoder_Decoder_Attention_1.png\" width=80%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Encoder_Decoder_no_attention.png\" width=70%</td>\n",
    "</table>\n",
    "    \n",
    "### Transformers: Review continued\n",
    "- [Attention review: continued](Intro_to_Attention.ipynb#Visualizing-Attention)\n",
    "    - visualizing Attention; flavors\n",
    "- [Implementing Attention](Attention_Lookup.ipynb) \n",
    "- [Transformer](Transformer.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Functional Models\n",
    "\n",
    "**Plan**\n",
    "\n",
    "Enough theory (for the moment) !\n",
    "\n",
    "The Transformer (whose theory we have presented) is built from plain Keras.\n",
    "\n",
    "Our goal is to dig into the **code** for the Transformer so that you too will learn how to build advanced models.\n",
    "\n",
    "Before we can do this, we must\n",
    "- go beyond the Sequential model of Keras: introduction to the Functional model\n",
    "- understand more \"advanced\" features of Keras: customomizing layers,  training loops, loss functions\n",
    "- The Datasets API\n",
    "\n",
    "**Basics**\n",
    "\n",
    "We start with the basics of Functional models, and will give a coding example of such a model in Finance.\n",
    "\n",
    "- [Functional API](Functional_Models.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 3\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We continue our study of Functional Models, introducing Advanced capabilities of Keras.\n",
    "\n",
    "## Functional Models (continued)\n",
    "\n",
    "- [Functional API (continued)](Functional_Models.ipynb#Example:-Nested-Models)\n",
    "\n",
    "### Functional Model Code:  A Functional model in Finance: \"Factor model\"\n",
    "\n",
    "We illustrate the basic features of Functional models with an example\n",
    "- does not use the additional techniques of the next section (Advanced Keras)\n",
    "\n",
    "[Autoencoders for Conditional Risk Factors](Autoencoder_for_conditional_risk_factors.ipynb)\n",
    "- [code](https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/20_autoencoders_for_conditional_risk_factors/06_conditional_autoencoder_for_asset_pricing_model.ipynb)\n",
    "\n",
    "### Threading\n",
    "\n",
    "In our [Autoencoders for Conditional Risk Factors](Autoencoder_for_conditional_risk_factors.ipynb), we skipped over an important detail\n",
    "- the \"Beta\" side of the notebook takes as input a matrix rather than a vector\n",
    "- how does a `Dense` layer work on higher dimensional data ?\n",
    "\n",
    "The answer is similar to how Python works:  [Threading](Keras_Advanced.ipynb#Factor-Models-and-Autoencoders:-Threading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advanced Keras\n",
    "\n",
    "- As part of our introduction to the [Functional API](Functional_Models.ipynb) we briefly covered\n",
    "some advanced topics\n",
    "    - [Custom layer type](Keras_Advanced.ipynb#Layer-specialization)\n",
    "\n",
    "- We continue with some other useful techniques, emphasizing interesting \"Loss\" functions\n",
    "    - [Custom loss, Custom training loop](Keras_Advanced.ipynb#Model-specialization)\n",
    "    - [Non-trivial Custom Loss: Neural Style Transfer](Neural_Style_Transfer.ipynb)\n",
    "    - [Gradient Ascent: Maximize utility for visualization](Keras_Advanced.ipynb#Visualizing-what-CNN's-learn:-Gradient-Ascent-and-the-Gradient-Tape)\n",
    "\n",
    "**Deeper dives**\n",
    "\n",
    "We have a [notebook](Keras_Advanced.ipynb)that is a **reference** to all the advanced techniques.\n",
    "- most will be introduced as part of our study of different interesting models\n",
    "- but this notebook is one convenient place to see them all\n",
    "\n",
    "If you *really* want to dig into the micro-details of TensorFlow, here are some important subtleties\n",
    "- [Computation Graphs](Computation_Graphs.ipynb)\n",
    "- [Eager vs Graph Execution](TF_Graph.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Putting it all together: Code: the Transformer\n",
    "\n",
    "**Plan**\n",
    "\n",
    "With the base of advanced Keras under our belts, it's time to understand the Transformer, in code\n",
    "\n",
    "We will examine the code in the excellent [TensorFlow tutorial on the Transformer](https://www.tensorflow.org/text/tutorials/transformer)\n",
    "\n",
    "- [The Transformer: Understanding the Pieces](Transformer_Understanding_the_Pieces.ipynb)\n",
    "- [The Transformer: Code](Transformer_code.ipynb)\n",
    "- [Choosing a Transformer architecture](Transformer_Choosing_a_PreTrained_Model.ipynb)\n",
    "\n",
    "**Suggested reading**\n",
    "\n",
    "There is an excellent tutorial on Attention and the Transformer which I recommend:\n",
    "- [Tensorflow tutorial: Neural machine translation with a Transformer and Keras](https://www.tensorflow.org/text/tutorials/transformer)\n",
    "\n",
    "\n",
    "**Deeper dive**\n",
    "- [Implementing Attention](Attention_Lookup.ipynb)\n",
    "- [Residual connections](RNN_Residual_Networks.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review/Preview: Language Models (continue)\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We will spend a lot of time dealing with inputs/outputs that are sequences of tokens (\"words\").\n",
    "\n",
    "We start with a quick review of the basics of Natural Language Processing.\n",
    "\n",
    "We then do a quick review of Large Language Models\n",
    "- a specific kind of NLP model, that solves a very simple task\n",
    "- is the basis for the Assistants (e.g., ChatGPT) that have become increasingly prevalent\n",
    "\n",
    "### [Natural Language Processing: Review](Review_NLP.ipynb)\n",
    "\n",
    "### LLM review\n",
    "\n",
    "Large Language Models (LLMs) are the basis of the amazing advances we are witnessing in NLP (e.g., GPT).\n",
    "\n",
    "It will also be an essential part of our Final Project.\n",
    "\n",
    "Let's introduce the topic.\n",
    "\n",
    "\n",
    "[Language Models, the future (present ?) of NLP: Review](Review_LLM.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond Transfer Learning: Fine-tuning a pre-trained model\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We begin the \"technical\" part of the course: the programming tools that will enable the Course Project.\n",
    "\n",
    "We introduce \"Modern Transfer Learning\": using model hubs.\n",
    "\n",
    "The hub we will use for the final project: HuggingFace\n",
    "- illustrate how to fine-tune a pre-trained model\n",
    "- quick Intro to HF\n",
    "    - best way to learn: through the course !\n",
    "    - uses Datasets\n",
    "        - will introduce later\n",
    "    - PyTorch version (uses Trainer); we will focus on Tensorflow/Keras version\n",
    "\n",
    "**HuggingFace Transformers course**\n",
    "\n",
    "The best way to understand and use modern Transfer Learning is via\n",
    "the [HuggingFace course](https://huggingface.co/course).\n",
    "\n",
    "You will learn\n",
    "- about the Transformer\n",
    "- how to use HuggingFace's tools for NLP (e.g., Tokenizers)\n",
    "- how to perform common NLP tasks\n",
    "    - especially with Transformers\n",
    "- how to fine-tune a pre-trained model\n",
    "- how to use the HuggingFace dataset API\n",
    "\n",
    "All of this will be invaluable for the Course Project.\n",
    "- does not have to be done using HuggingFace\n",
    "- but using at least parts of it will make your task easier\n",
    "\n",
    "\n",
    "- [HuggingFace intro](Transfer_Learning_HF.ipynb)\n",
    "    - [linked notebook: Using a pretrained Sequence Classifier](HF_quick_intro_to_models.ipynb) \n",
    "\n",
    "**Suggested reading**\n",
    "\n",
    "[HuggingFace course](https://huggingface.co/course)\n",
    "- you are well-advised to follow this material over the next 3 weeks in preparation for the Course Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 4\n",
    "\n",
    "## Datasets: Big data in small memory\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We continue our exploration of the Functional API in Keras.\n",
    "\n",
    "We will spend some time examining the code for the Transformer.\n",
    "\n",
    "We will also introduce the TensorFlow Dataset (TFDS) API, a way to consume large datasets using a limited amount of memory.\n",
    "\n",
    "\n",
    "**Plan**\n",
    "\n",
    "Last piece of technical info to enable the project\n",
    "\n",
    "- [TensorFlow Dataset](TF_Data_API.ipynb)\n",
    "\n",
    "**Background**\n",
    "- [Python generators](Generators.ipynb)\n",
    "\n",
    "**Notebooks**\n",
    "- [Dataset API: play around](TFDatasets_play_v1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond the LLM\n",
    "\n",
    "**Plan**\n",
    "\n",
    "The LLM is trained to \"predict the next\" token.\n",
    "\n",
    "Although this sounds boring, it is the basis of a Universal API for problem solving.\n",
    "\n",
    "We also show the remarkable ability to adapt on Pre-Trained LLM to a new task *without* fine-tuning\n",
    "- and without changing any parameters !\n",
    "\n",
    "[Beyond the LLM](Review_LLM.ipynb#Beyond-the-LLM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformers: Scaling\n",
    "\n",
    "We now have the capabilities to build models with extremely large number of weights.\n",
    "Is it possible to have too many weights ? \n",
    "\n",
    "Yes: weights, number of training examples and compute capacity\n",
    "combine to determine the performance of a model.\n",
    "\n",
    "There is an empirical result that suggests that in order to take advantage of GPT-3's use of 175 billion weights\n",
    "- 1000 times more compute is required than what was used\n",
    "- 10 times more training examples is required compared to what was used\n",
    "\n",
    "[How large should my Transformer be ?](Transformers_Scaling.ipynb)\n",
    "\n",
    "**Suggested reading**\n",
    "- [Scaling laws](https://arxiv.org/pdf/2001.08361.pdf)\n",
    "\n",
    "**Further reading**\n",
    "- Inference budget\n",
    "    - [Transformer Inference Arithmetic](https://kipp.ly/transformer-inference-arithmetic/)\n",
    "    - [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf)\n",
    "    - [Large language models aren't trained enough](https://finbarr.ca/llms-not-trained-enough/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Re-visiting the Transformer code: deeper details\n",
    "\n",
    "- [Implementing Attention](Attention_Lookup.ipynb)\n",
    "- [Transformer code: parts that we skipped over](Transformer_Understanding_the_Pieces.ipynb#General)\n",
    "    - Residual connections, Embeddings, Positional Encoding\n",
    "- [Residual connections](RNN_Residual_Networks.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignments\n",
    "\n",
    "Your assignments should follow the [Assignment Guidelines](assignments/Assignment_Guidelines.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Additional Deep Learning resources\n",
    "\n",
    "Here are some resources that I have found very useful.\n",
    "\n",
    "Some of them are very nitty-gritty, deep-in-the-weeds (even the \"introductory\" courses)\n",
    "- For example: let's make believe PyTorch (or Keras/TensorFlow) didn't exists; let's invent Deep Learning without it !\n",
    "    - You will gain a deeper appreciation and understanding by re-inventing that which you take for granted\n",
    "    \n",
    "\n",
    "## [Andrej Karpathy course: Neural Networks, Zero to Hero](https://karpathy.ai/zero-to-hero.html)\n",
    "- PyTorch\n",
    "- Introductory, but at a very deep level of understanding\n",
    "    - you will get very deep into the weeds (hand-coding gradients !) but develop a deeper appreciation\n",
    "    \n",
    "## fast.ai\n",
    "\n",
    "`fast.ai` is a web-site with free courses from Jeremy Howard.\n",
    "- PyTorch\n",
    "- Introductory and courses \"for coders\"\n",
    "- Same courses offered every few years, but sufficiently different so as to make it worthwhile to repeat the course !\n",
    "    - [Practical Deep Learning](https://course.fast.ai/)\n",
    "    - [Stable diffusion](https://course.fast.ai/Lessons/part2.html)\n",
    "        - Very detailed, nitty-gritty details (like Karpathy) that will give you a deeper appreciation\n",
    "        \n",
    "## [Stefan Jansen: Machine Learning for Trading](https://github.com/stefan-jansen/machine-learning-for-trading)\n",
    "\n",
    "An excellent github repo with notebooks\n",
    "- using Deep Learning for trading\n",
    "- Keras\n",
    "- many notebooks are cleaner implementations of published models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
