{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text and Images together\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>Text is represented as\n",
    "            <ul>\n",
    "                <li>a sequence of integers: an integer index into the list of tokens in the vocabulary</li>\n",
    "            </ul>\n",
    "        <li>Using the ideas in the VQ-VAE, an image can be represented as\n",
    "            <ul>\n",
    "                <li>a sequence of integers: an integer index into a \"code book\": finite list of vectors</li>\n",
    "            </ul>\n",
    "        <li>Both text and image sequences can be reduced to a fixed-length vector: an embedding\n",
    "        <li>Goal:\n",
    "        <ul>\n",
    "            <li>Create a shared embedding space for text and images</li>\n",
    "            <li>Learn projection matrices\n",
    "                <ul>\n",
    "                    <li>One to project the text embedding into the shared space</li>\n",
    "                    <li>One to project the image embedding into the shared space</li>\n",
    "                </ul>\n",
    "            <li>Such that the projections\n",
    "                <ul>\n",
    "                    <li>Of an image</li>\n",
    "                    <li>And a text string that describes the image</li>\n",
    "                    <li>Are close to one another in the shared space\n",
    "                </ul>\n",
    "            <li>and the projections</li>\n",
    "                <ul>\n",
    "                    <li>Of an image</li>\n",
    "                    <li>And a text string that <b>does not</b> describes the image</li>\n",
    "                    <li>Are <b>not close</b> to one another in the shared space\n",
    "                </ul>\n",
    "        </ul>\n",
    "        <li>By creating text and image embeddings with this \"closeness\" property</li>\n",
    "            <ul>\n",
    "                <li>We can perform zero-shot Image Classification</li>\n",
    "                <li>Classify images by matching them against sentences describing a possible class: \"Photo of a cat\"</li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**References**\n",
    "- [CLIP paper](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf)\n",
    "- [LiT paper](https://arxiv.org/pdf/2111.07991.pdf)\n",
    "\n",
    "\n",
    "The standard \"Computer Vision\" task is to learn an association between images and a predetermined set of labels.\n",
    "\n",
    "Highly successful image classifiers have thus been obtained but they do not \n",
    "generalize to successfully classify datasets distinct from the training set.\n",
    "- Transfer Learning would be necessary to train a new Classifier Head (to accommodate the target set's labels)\n",
    "- Disjoint roles for the two types of data\n",
    "    - Images are Features\n",
    "    - Text are Labels\n",
    "    - Can't learn to associate (parts of) Images with semantically related Text\n",
    "- Training datasets are \"small-ish\": depends on human-intensive hand-labeling of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transfer Learning: pre-trained model</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center><img src=\"images/Transfer_Learning_1.jpg\" width=800></center></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transfer Learning: replace the head, deep layers of the pre-trained model</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center><img src=\"images/Transfer_Learning_3.jpg\"></center></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*CLIP* (Contrastive Image Language-Image Pre-training) is a model\n",
    "- That creates representations of Image and Text *in a joint space*\n",
    "- Leverages recent advances in Natural Language Processing to enable better Computer Vision models\n",
    "- Facilitates **zero shot** learning of new Image Classification datasets\n",
    "- Uses naturally occurring (and abundant) source of Training data\n",
    "\n",
    "A key objective is use Natural Language supervision to learn about images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Image embedding space\n",
    "\n",
    "The prefix of an Image Classification model creates representations of Images that make it possible for a Classifier Head to predict the correct label.\n",
    "- A chosen Deep layer of an Image Classification model creates *image embeddings*\n",
    "- Transfer Learning: graft a new Classifier Head to learn the class labels of a new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But Transfer Learning typically treats the labels as OHE vectors\n",
    "- mutually orthogonal\n",
    "- no relationship between semantically related class labels\n",
    "\n",
    "Thus, the representation of Images potentially encodes \"common\" vision concepts\n",
    "- But no such encoding of the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text embedding space\n",
    "\n",
    "Similarly: \"Language Models\" (e.g., GPT) have demonstrated a great ability for creating representations of text sequences\n",
    "- Seem to capture semantics\n",
    "- The fixed length \"summary\" of a text sequence is a *text embedding*\n",
    "- Facilitate zero shot learning\n",
    "    - Universal \"text to text\" API for all language tasks\n",
    "    - Single model can \"learn\" to solve a new task without adjustment of weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center><strong>Latent state $\\h_\\tp$ is a fixed length \"summary\" of $\\x_{(1)}, \\ldots \\x_\\tp$</strong></center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center><img src=\"images/RNN_many_to_one_to_classifier.jpg\"></td></center>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating a common embedding space\n",
    "\n",
    "By contrast: CLIP creates a *joint embedding space* for Image and Text.\n",
    "- The embedding space of Images\n",
    "- The embedding space of Text\n",
    "- are combined into a *common* embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To be precise:\n",
    "\n",
    "- Image $I$ is encoded by an Image Encoder into $I_f$ in \"image embedding space\"\n",
    "$$I_f = \\text{image_encoder}(I)$$\n",
    "\n",
    "    - $I_f$ is projected into $I_e$ in  \"common embedding space\" via matrix $W_i$\n",
    "    $$I_e = I_f \\cdot W_i$$\n",
    "    \n",
    "- Text $T$ is encoded by a Text Encoder into $T_f$ in \"text embedding space\"\n",
    "$$T_f = \\text{text_encoder}(T)$$\n",
    "\n",
    "    - $T_f$ is projected into $T_e$ in \"common embedding space\" via matrix $W_t$\n",
    "    $$T_e = T_f \\cdot W_t$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A primary use of the common embedding space\n",
    "- Generative models that mix text and images\n",
    "\n",
    "But we remark in passing that the paper had a different objective\n",
    "- better Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since $I_e$ and $T_e$ are both in \"common embedding space\"\n",
    "- their similarity may be measured via the dot product\n",
    "$$\n",
    "I_e \\cdot T_e\n",
    "$$\n",
    "\n",
    "This creates the potential for a different (and perhaps better) kind of Image Classifier\n",
    "- match an Image\n",
    "- to textual descriptions of the label\n",
    "\n",
    "   ` An image of a {label}`\n",
    "- rather than a numeric encoding of the label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An important part of the Model is the *Contrastive Training* objective\n",
    "- Minimize the distance between *correct* Image/Text Pairs\n",
    "- Maximize the distance between *incorrect* Image/Text pairs\n",
    "\n",
    "This type of objective is used in many places in Deep Learning\n",
    "- so is worthy to  introduce as an independent concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Details\n",
    "\n",
    "**Notation summary**\n",
    "\n",
    "term | dimension &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | meaning \n",
    ":---|:---|:---\n",
    "$n$ | | number of examples in a training batch\n",
    "$N$ | | number of exaples in the training dataset\n",
    "$I$ | $(n \\times h \\times w \\times c )$ | Image batch\n",
    "| | image dimension $(h \\times w \\times c )$\n",
    "$T$ | $(n \\times l )$ | Text labels for batch\n",
    "$d_i$ | | dimension of Image embedding\n",
    "$d_t$ | | dimension of Text embedding\n",
    "$d_e$ | | dimension of common embedding\n",
    "$W_i$ | $(d_i \\times d_e)$ | learned projection of image embedding to common embedding\n",
    "$W_t$ | $(d_t \\times d_e)$ | learned projection of image embedding to common embedding\n",
    "$t$   | | learned temperature parameter (used in softmax)\n",
    "$I_f$ | $(n \\times d_i)$ | embedding of Image batch\n",
    "$T_f$ | $(n \\times d_t)$ | embedding of Text (label) batch\n",
    "      | $(n \\times d_e)$ | size of common embedding (for each of the $n$ Image and Labels)\n",
    "$\\text{logits}$ | $(n \\times n)$ | $\\text{logits}_{i,j}$ is similarity of image $i$ to label $j$      \n",
    "$\\loss_i$ | $1$ | Loss across images: reduce $(\\text{logits} \\cdot \\text{labels})$ across text dimension\n",
    "           |    | For each image: compare image's logits to labels\n",
    "$\\loss_t$ | $1$ | Loss across labels: reduce $(\\text{logits} \\cdot \\text{labels})$ across image dimension\n",
    "           |    | For each label: compare labels's logits to images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Contrastive pre-training\n",
    "\n",
    "CLIP is trained to solve an Image Classification task\n",
    "- Associate the best text sequence description\n",
    "- To an input Image\n",
    "\n",
    "The \"standard\" approach is to jointly train\n",
    "- an image feature extractor (e.g., a CNN)\n",
    "- with a Classifier \"head\"\n",
    "    - the labels are sparse OHE vectors representing \"word\" labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In contrast, CLIP jointly trains\n",
    "- an Image encoder\n",
    "- a Text sequence encoder\n",
    "- with the objective of\n",
    "    - matching a Training image with its associated Text\n",
    "    - the text is a semantically meaningful sequence of \"words\" rather than a OHE vector\n",
    "\n",
    "The key is for the Image Encoder and Text Encoder to produce embeddings in a common space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To be precise, CLIP is trained (enforced by the Loss function) to create a similarity metric such that\n",
    "- the similarity between\n",
    "    - the embedding of a Training image (in the joint embedding space)\n",
    "    - and the embedding of the Text label of the training image (in the joint embedding space)\n",
    "    - is *maximal* across the embeddings of all text labels in the Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The method of *Contrastive Learning* \n",
    "- is to learn features (Image/Text embeddings in the common space)\n",
    "- that make related concepts (an Image and correct Text) \"similar\"\n",
    "    - high similarity, low distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hopefully, the picture (labeled \"(1)\") describing the *training* will help:\n",
    "- A training example is an (Image, Text) pair: $(I_i, T_i)$\n",
    "- The Image $I_i$ is encoded into the joint embedding space by the Image Encoder\n",
    "- It's Text $T_i$ is encoded into the joint embedding space by the Text Encoder\n",
    "- The matrix is a \"similarity\" (inverse of distance) between all Images and all Text labels\n",
    "    - the similarity is defined by the dot product of the Image and Text embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center><strong>CLIP architecture</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"https://raw.githubusercontent.com/openai/CLIP/main/CLIP.png\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pre-training details\n",
    "\n",
    "\n",
    "## Choosing Encoders for Image and Text\n",
    "\n",
    "There are many existing \"encoders\" for both Image and Text.\n",
    "- Image\n",
    "    - A deep layer of any Image Classifier\n",
    "- Text\n",
    "    - Simplest: Continuous Bag of Words (CBOW)\n",
    "    - Final latent state of an RNN (Encoder)\n",
    "    - Transformer\n",
    "        - use latent representation of the special `<CLS>` token typically prepended to the sequence\n",
    "\n",
    "There is no need to re-invent one just for CLIP.\n",
    "\n",
    "The authors report experiments with different Encoder choices.\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating a common embedding space\n",
    "\n",
    "However: there are some issues\n",
    "- Technical\n",
    "    - length of Image embedding is $d_i$\n",
    "    - length of Text embedding is $d_t$\n",
    "    - Not necessarily the case that $d_i = d_t$\n",
    "- Semantic\n",
    "    - The two embedding spaces were trained independently\n",
    "    - Can't assume embedding of an Image in its own space is related to embedding of Text in its own space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The solution to both these problems is to project (via the dot product) each embedding into\n",
    "a shared space where embeddings are of length $d_e$.\n",
    "\n",
    "- Matrix $W_i$ of dimension $(d_i \\times d_e)$ is a *learned projection* of image embedding to common embedding\n",
    "- Matrix $W_t$ of dimension $(d_t \\times d_e)$ is a *learned projection* of text embedding to common embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Once projected into the common embedding space\n",
    "- The similarity matrix (each element a dot product of a single Image embedding and single Text embedding) can be computed\n",
    "\n",
    "The key: projection matrices $W_i, W_t$ are *learned* as part of training\n",
    "- not pre-specified\n",
    "- they adapt to the objective (as defined by minimization of Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pre-training Loss function\n",
    "\n",
    "Suppose we have a single (Image, Text) pair $(I_i, T_i)$ from the set of $N$ training examples.\n",
    "\n",
    "Our objective is to ensure that the similarity  (dot product of vectors within common embedding space) is such that\n",
    "- $I_i \\cdot T_i$ is as large as possible\n",
    "- $I_i \\cdot T_j$ is as small as possible for $i \\ne j$\n",
    "\n",
    "That is: the similarity (a scalar value) is maximized for the correct Text of the Image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can state this mathematically.\n",
    "\n",
    "First compute the $(N \\times N)$ similarity matrix $S$\n",
    "- Normalize each embedding vector so the elements are positive and sum to 1\n",
    "    - L2 normalization\n",
    "- Compute  the similarity of Image $I_i$ with each Text $T_j$\n",
    "    - as the dot product $I_i \\cdot T_j$ of the normalized vectors\n",
    "    - resulting in a vector of length $N$ for each Image $I_i$\n",
    "- Convert the similarity vector (row of length $N$) for Image $I_i$ into a probability distribution via a Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The last 2 steps implement a multinomial Logistic Regression Classifier\n",
    "-  where $T_j$ are the weights (\"pattern matched against and Image) for logit $j$ of the Classifier output\n",
    "\n",
    "The import of having Image and Text embeddings in a joint space is that we can perform this pattern matching.\n",
    "\n",
    "In essence: the Image and Text are equivalent representations of the same concept.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We create an *Image Loss* for example $i: \\loss_i^\\ip$\n",
    "- subscript $i$ denotes *image* (not an index)\n",
    "\n",
    "$S^\\ip$, row $i$ of the similarity matrix\n",
    "- Can be interpreted as the *probability distribution over the $N$ possible* **labels** for Image $I_i$\n",
    "- column $j = i$ is the correct label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Use Binary Cross Entropy for the Image loss\n",
    "- want the probability of the correct label (column $i$) to be 1: \n",
    "$$S^\\ip_i = 1$$\n",
    "- want the probability of any other label $j \\ne i$ to be 0: \n",
    "$$S^\\ip_j = 0, \\, j \\ne i$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We also create a *Text Loss* for example $i: \\loss_t^\\ip$\n",
    "- subscript $t$ denotes text\n",
    "\n",
    "Rather than using the *rows* of similarity matrix $S$, we use the *columns*\n",
    "\n",
    "$S_j$, column $j$ of the similarity matrix\n",
    "- Can be interpreted as the *probability distribution over the $N$ possible* **images** for Text $T_j$\n",
    "- Row $i = j$ is the correct image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Use Binary Cross Entropy for the Text Loss \n",
    "- want the probability of the correct image (row $i$) to be 1:\n",
    "$$\n",
    "S^{(j)}_j = 1\n",
    "$$\n",
    "- want the probability of any other image $i \\ne j$ to be 0\n",
    "$$\n",
    "S^\\ip_j = 0, \\, i \\ne j\n",
    "$$\n",
    "\n",
    "n.b., above we write the loss for example $j$, per our custom of using $j$ to index columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is called a *Contrastive* objective\n",
    "- maximizing the *contrast* between correct (value 1) and incorrect (value 0)\n",
    "\n",
    "Cross Entropy (loss used in the multinomial Logistic Regression Classifier) *is* a contrastive loss.\n",
    "- It is minimized when\n",
    "    - the probability assigned to the correct logit is highest (i.e, 1)\n",
    "    - the probabilities assigned to incorrect logits is lowest (i.e., 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But how do we create the *many possible* negative examples to contrast with the positive one ?\n",
    "\n",
    "Rather than explicitly creating many negative examples\n",
    "- we assume that all examples in *a single batch*\n",
    "- other than the one with the correct label\n",
    "- are negatives: completely incorrect\n",
    "\n",
    "This is called the *in-batch negatives* trick\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The in-batch negatives trick is simple but crude.\n",
    "- a \"second best\" label\n",
    "- is considered equally bad\n",
    "- as a \"completely incorrect\" label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The per example loss is the sum of the per example Image and Text losses\n",
    "$$\n",
    "\\loss^\\ip = \\loss_i^\\ip + \\loss_t^\\ip\n",
    "$$\n",
    "\n",
    "and the Total Loss is the sum over the $N$ per example losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Technical notes**\n",
    "\n",
    "In Mini-batch Gradient Descent, examples are processed in batches\n",
    "- batch size denoted as $n$ in this paper\n",
    "- rather than over all $N$ training examples\n",
    "- to be precise: in the explanation above, replace $N$ with $n$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The batches are arranged so that there is a single example for each of the $n$ labels\n",
    "- arranged so that example $i$ of the batch has the $i^{th}$ label\n",
    "- doing so allows the code to specify the target vector for the mini-batch as $[0, n-1]$\n",
    "\n",
    "The in-batch negatives trick is implemented by using matrix multiplication\n",
    "- which achieves the dot product of each image encoding with each text encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pseudo-code for Pre-Training \n",
    "\n",
    "Pseudo code for training batch:\n",
    "```\n",
    "I_f = image_encoder(I)\n",
    "T_f = text_encoder(T)\n",
    "\n",
    "# joint multimodal embedding [n, d_e]\n",
    "I_e = l2_normalize(np.dot(I_f, W_i), axis=1)\n",
    "T_e = l2_normalize(np.dot(T_f, W_t), axis=1)\n",
    "\n",
    "# scaled pairwise cosine similarities [n, n]\n",
    "logits = np.dot(I_e, T_e.T) * np.exp(t)\n",
    "\n",
    "# symmetric loss function\n",
    "labels = np.arange(n)\n",
    "loss_i = cross_entropy_loss(logits, labels, axis=0)\n",
    "loss_t = cross_entropy_loss(logits, labels, axis=1)\n",
    "\n",
    "loss = (loss_i + loss_t)/2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pre-training dataset\n",
    "\n",
    "Image Classification models need (Image, Text) pairs as training examples.\n",
    "\n",
    "Traditionally, these were created manually (sometimes crowd-sourced) at considerable effort\n",
    "- hence, datasets were limited in size\n",
    "\n",
    "This is because labels were\n",
    "- pre-defined\n",
    "- from a small set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This problem is similar to what researchers in NLP encountered.\n",
    "\n",
    "The recent rapid advances in NLP were achieved, in part, by cleverly using the entire Web as a source of text\n",
    "- Raw text is unlabeled\n",
    "- But can turn a sequence into a \"predict the next element of sequence\" Semi-Supervised example\n",
    "- Hence: lots of training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The authors adopt a similar approach to obtaining training data for Image Classification\n",
    "- Search the Web for Images that have captions\n",
    "- The caption become the labels for the training image\n",
    "- **Unbounded** number of distinct labels\n",
    "    - many Texts to describe the same (or similar image)\n",
    "        - \"Picture of a cat\"\n",
    "        - \"Picture of my cat named 'Kitty'\"\n",
    "        - unbounded length of label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is one reason Text labels need to be embedded\n",
    "- Need a fixed length representation an unbounded number of labels, each of unbounded length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inference: Zero shot\n",
    "\n",
    "The fact that CLIP can deal with arbitrary labels (in the force of Text sequences) creates the possibility of classifying Images\n",
    "- from an *unseen* Target dataset\n",
    "- with no further training (other than the initial pre-training on the Source dataset)\n",
    "\n",
    "Being able to solve a Target task without specifically being trained with examples of the task\n",
    "- is called **Zero shot** learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The pictures (labeled \"(2)\" and \"(3)\") in the CLIP diagram above describe the process\n",
    "for predicting the label of a single image\n",
    "- Given the finite set of labels from the Target Task\n",
    "- Convert the short labels (nouns or phrases) into longer sentences\n",
    "    - i.e., \"Cat\" becomes \"photo of a cat\"\n",
    "- Embed the sentences into joint embedding space,  resulting in $T_1, \\ldots T_N$\n",
    "- Embed the Target Image into joint embedding space, resulting in $I_1$\n",
    "- Create the similarity matrix of dimension $(1 \\times N)$\n",
    "    - computing $I_1 \\cdot T_j$, for each $1 \\le j \\le N$\n",
    "- Predict $j^* = \\argmax{j}{I_1 \\cdot T_j}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discussion\n",
    "\n",
    "## Prompt Engineering\n",
    "\n",
    "In the description of zero shot inference, short labels of the Target task were expanded into longer sequences of words.\n",
    "\n",
    "The authors call this *prompt engineering* (i.e., creating new \"prompts\" for input.\n",
    "\n",
    "They suggest that careful prompt engineering for each Target task can improve Zero shot classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example\n",
    "- converting a label (denoted by placeholder `{label}`) for a pet  to\n",
    " > A photo of a {label}, a type of pet\n",
    "- can improve classification\n",
    "    - helps with polysemy (two words with identical spelling but different meaning)\n",
    "        - \"crane\": a bird; a piece of construction equipment\n",
    "    - the extra words \"photo\" and \"type of pet\" become attributes of the image\n",
    "        - that can be related (by textual similarity) to other images/labels through the Text embeddings\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Theory**\n",
    "\n",
    "Representation of CLIP is much more detailed than other Image Classification models\n",
    "- Other models only need to find representation that separates examples\n",
    "    - may be hyper-specific and not generalize well\n",
    "- CLIP needs to understand other details of the image **through the text**\n",
    "    - difference between image formats: \"photo\", \"illustration\", \"drawing\"\n",
    "    - sub-images that are mixed with the target sub-image\n",
    "        - a \"cat\" in a group of \"cats\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Zero shot classification with Prompt Engineering\n",
    "\n",
    "Let's explore Prompt Engineering using this [Colab notebook](https://github.com/openai/CLIP/blob/main/notebooks/Prompt_Engineering_for_ImageNet.ipynb)\n",
    "\n",
    "Goal is zero-shot classification of CIFAR image dataset\n",
    "- 1000 classes\n",
    "- most class labels are single word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Feature engineering transformed each CIFAR class label into *multiple* prompts\n",
    "using \"templates\"\n",
    "```\n",
    "imagenet_templates = [\n",
    "    'a bad photo of a {}.',\n",
    "    'a photo of many {}.',\n",
    "    'a sculpture of a {}.',\n",
    "    'a photo of the hard to see {}.',\n",
    "    'a low resolution photo of the {}.',\n",
    "    'a rendering of a {}.',\n",
    "    'graffiti of a {}.',\n",
    "    'a bad photo of the {}.',\n",
    "    'a cropped photo of the {}.',\n",
    "    'a tattoo of a {}.',\n",
    "    'the embroidered {}.',\n",
    "    'a photo of a hard to see {}.',\n",
    "    'a bright photo of a {}.',\n",
    "    'a photo of a clean {}.',\n",
    "    'a photo of a dirty {}.',\n",
    "    'a dark photo of the {}.',\n",
    "    'a drawing of a {}.',\n",
    "    'a photo of my {}.',\n",
    "    'the plastic {}.',\n",
    "```\n",
    "$$\\vdots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using forward selection, the authors selected the 7 \"best templates\"\n",
    "- Created 7 prompts (e.g., texts) from each CIFAR class label\n",
    "- Tokenized each prompt\n",
    "- text-encoded each to the common embedding\n",
    "- normalized each embedding\n",
    "- took the average (across the 7 prompts ?) embedding\n",
    "    - and used it as the embedding for the CIFAR class\n",
    "\n",
    "```\n",
    "texts = [template.format(classname) for template in templates] #format with class\n",
    "            texts = clip.tokenize(texts).cuda() #tokenize\n",
    "            class_embeddings = model.encode_text(texts) #embed with text encoder\n",
    "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "            class_embedding = class_embeddings.mean(dim=0)\n",
    "            class_embedding /= class_embedding.norm()\n",
    "            zeroshot_weights.append(class_embedding)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Eventually: the image embedding has its cosine similarity compared with the average embedding for each label\n",
    "\n",
    "```\n",
    "# predict\n",
    "image_features = model.encode_image(images)\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "logits = 100. * image_features @ zeroshot_weights\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Observation from notebook on why templates help\n",
    "> Speculating, we think it's interesting to see different scales (large and small), a difficult view (a bad photo), and \"abstract\" versions (origami, video game, art), were all selected for, but we haven't studied this in any detail. This subset performs a bit better than the full 80 ensemble reported in the paper, especially for the smaller models.\n",
    "\n",
    "Adding textual hints (via prompt engineering: \"a bad photo\") seemed to help\n",
    "- perhaps implicitly creating an attribute of an image\n",
    "- that creates a relationship with other Images having a similar attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Experiments\n",
    "\n",
    "The authors report many experiments using CLIP, in an effort to discover\n",
    "- its strengths, weaknesses\n",
    "- how it works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One experiment compares\n",
    "- Zero shot learning of a Target task, using CLIP\n",
    "- Transfer learning\n",
    "    - creating a Target task specific head on top of an existing Source task Image Classifier (e.g., ResNet)\n",
    "    \n",
    "They call the Transfer Learning method *linear probing*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Zero shot CLIP outperforms Linear Probing with ResNet on many classification tasks\n",
    "- does better on Target tasks in which Target task training set has few examples per class\n",
    "    - i.e., too few examples per class to adequately train the new Classification head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias\n",
    "\n",
    "Section 7 of the paper is an effort to probe implicit biases that we\n",
    "have learned are present\n",
    "in seemingly unbiased text (e.g., Wikipedia)\n",
    "\n",
    "For example, there are datasets used to probe for biases about race\n",
    "- add \"egregious\" categories: animals (\"ape\", \"orangutan\"), criminal (\"thief\") to true Text label\n",
    "- Blacks misclassified as animals more often than Whites\n",
    "- Young people misclassified more often as thief\n",
    "    - but adding a \"child\" class reduces this misclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Understanding that a model learns unintended bias \n",
    "- through biased natural language\n",
    "- through photos in very limited contexts\n",
    "\n",
    "is something that is becoming more prevalent in describing and evaluating models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Similar approach from Keras site\n",
    "\n",
    "There is a similar [example](https://keras.io/examples/nlp/nl_image_search/) on the Keras website.\n",
    "\n",
    "It's instructive to compare the two approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Joint Embedding**\n",
    "\n",
    "CLIP: via a shared learned embedding matrices $\\W_i$ (image) and $\\W_t$ (text).\n",
    "\n",
    "Keras: via two separate multi-layer \"embedding\" networks of blocks consisting of `Dense` layers to transform the dimension.\n",
    "\n",
    "```\n",
    "def project_embeddings(\n",
    "    embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "):\n",
    "    projected_embeddings = layers.Dense(units=projection_dims)(embeddings)\n",
    "    for _ in range(num_projection_layers):\n",
    "        x = tf.nn.gelu(projected_embeddings)\n",
    "        x = layers.Dense(projection_dims)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Add()([projected_embeddings, x])\n",
    "        projected_embeddings = layers.LayerNormalization()(x)\n",
    "    return projected_embeddings\n",
    "```\n",
    "\n",
    "Note that the statement\n",
    "\n",
    "    x = layers.Add()([projected_embeddings, x])\n",
    "\n",
    "is implementing a skip connection.\n",
    "- `projected_embeddings` that is input to the body of the loop\n",
    "- skips over the body\n",
    "- and is added to the updated (by the body) `projected_embeddings`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Loss function**\n",
    "\n",
    "The \"logits\" are the dot-product of the Text and Image embeddings.\n",
    "\n",
    "CLIP: Compare the logits to the text target labels and (separately) to the image target labels.\n",
    "- recall: the target \"labels\" are just indices since the diagonal element is the correct \"target\"\n",
    "    \n",
    "    loss_i = cross_entropy_loss(logits, labels, axis=0)\n",
    "    loss_t = cross_entropy_loss(logits, labels, axis=1)\n",
    "\n",
    "    loss = (loss_i + loss_t)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Keras\n",
    "\n",
    "This approach is a little more sophisticated.\n",
    "\n",
    "It compares the similarity (using dot product) of\n",
    "- pairs of labels\n",
    "- pairs of images\n",
    "\n",
    "It uses the average similarity as the \"correct\" label\n",
    "- the best target (highest average) is the correct one: $I_i, T_i$\n",
    "- but mis-classifying a text/image pair is mitigated if the incorrect class\n",
    "    - have similar texts\n",
    "    - have similar images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CLiP demo (HuggingFace)\n",
    "\n",
    "You can use the Inference API to play with [CLiP on HuggingFace](https://huggingface.co/openai/clip-vit-large-patch14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
