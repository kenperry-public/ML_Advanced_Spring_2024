{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zw4nryteX38f",
    "outputId": "32af3749-1622-4262-ca6c-0bb0705bd25e"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False\n",
    "\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ypuy3uSgX5Hm",
    "outputId": "fcc2a590-e1fa-474b-afbc-8ec5290b65ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TensorFlow version  2.0.0\n",
      "Version 2, minor 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Running TensorFlow version \",tf.__version__)\n",
    "\n",
    "# Parse tensorflow version\n",
    "import re\n",
    "\n",
    "version_match = re.match(\"([0-9]+)\\.([0-9]+)\", tf.__version__)\n",
    "tf_major, tf_minor = int(version_match.group(1)) , int(version_match.group(2))\n",
    "print(\"Version {v:d}, minor {m:d}\".format(v=tf_major, m=tf_minor) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zrPIH-NNX8y4",
    "outputId": "0d456abe-90d8-40e6-8e04-3e8e98b6c701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "else:\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YS8mj2c5bKKk",
    "outputId": "993a6fb1-52ab-430f-a201-e8f056e6d43a"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  !pip install datasets evaluate transformers[sentencepiece]\n",
    "  !pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oftFp7iFbJRe"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-43eca54f7d45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgradio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gradio'"
     ]
    }
   ],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29bnSo8t9EmN"
   },
   "source": [
    "# Use an Inference end-point\n",
    "- Advange:\n",
    "  - free\n",
    "  - does not use *local* RAM so can run big models\n",
    "- [paid hosting](https://huggingface.co/pricing#endpoints)\n",
    "  - don't get charged for a *paused* end-point\n",
    "- [guide](https://huggingface.co/docs/inference-endpoints/index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TTlN7j78659Q",
    "outputId": "60a58e26-6935-403d-b647-7ba0bf4ca8f6"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "def get_API_token(token_file=\"/content/hf.token\"):\n",
    "    # Check for file containing API token to HuggingFace\n",
    "    p = Path(token_file).expanduser()\n",
    "    if not p.exists():\n",
    "      print(f\"Token file {p} not found.\")\n",
    "      return\n",
    "\n",
    "    with open(token_file, 'r') as fp:\n",
    "        token = fp.read()\n",
    "\n",
    "    # Remove trailing newline\n",
    "    token = token.rstrip()\n",
    "\n",
    "    return token\n",
    "\n",
    "API_TOKEN=get_API_token();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtKwuir07YDZ"
   },
   "outputs": [],
   "source": [
    "gen_text_key = \"generated_text\"\n",
    "input_key = \"inputs\"\n",
    "error_key = \"error\"\n",
    "\n",
    "models = { \"small\": \"EleutherAI/gpt-neo-1.3B\",\n",
    "           \"big\":   \"EleutherAI/gpt-neox-20b\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQERqi6o659R",
    "outputId": "4f81a089-b2d4-4d36-e3a4-51764d499bcb"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "def query(payload, model_string):\n",
    "  API_URL = f\"https://api-inference.huggingface.co/models/{model_string}\"\n",
    "\n",
    "  response = requests.post(API_URL, headers=headers, json=payload)\n",
    "  return response.json()\n",
    "\n",
    "def execute_query(q, model_string):\n",
    "  output = query({  input_key: q }, model_string)\n",
    "\n",
    "  # Successful output is a list; error output is a dict\n",
    "  if type(output) is dict:\n",
    "    out = f\"Error: {output[error_key]}\"\n",
    "  else:\n",
    "    out = output[0][gen_text_key]\n",
    "\n",
    "  return out\n",
    "\n",
    "\n",
    "\n",
    "exemplars = [ \"this movie was great: positive\",\n",
    "             \"one of the best films of the year: positive\",\n",
    "             \"just plain awful: negative\",\n",
    "             \"I would not see this one again: negative\",\n",
    "             \"this movie was great: positive\",\n",
    "             \"one of the best films of the year: positive\",\n",
    "             \"just plain awful: negative\",\n",
    "             \"I would not see this one again: negative\",\n",
    "             \"I love this film: positive\"\n",
    "]\n",
    "\n",
    "sep = \" \\n \"\n",
    "exemplar_string = sep.join(exemplars)\n",
    "few_shot_string =  exemplar_string + sep + \"I've heard not so great things about this one:\"\n",
    "\n",
    "q = few_shot_string\n",
    "\n",
    "# q = \"Can you please let us know more details about your \"\n",
    "\n",
    "# Can run a very large model since execution is on remote end-point\n",
    "model_string = models[\"big\"]\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "res = execute_query(q, model_string)\n",
    "\n",
    "time_end = time.time()\n",
    "\n",
    "print(f\"[{time_end-time_start:3.2f} seconds, using {model_string}]\\n {res}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W36n_ylXJ2Of"
   },
   "source": [
    "# Use a pipeline\n",
    "- runs locally\n",
    "- so can only use model small enough to fit in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4JuoPKewcOni"
   },
   "outputs": [],
   "source": [
    "model_string = models[\"small\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gszFoZ4u659R"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "num_return = 3\n",
    "len_return = 30\n",
    "\n",
    "\n",
    "generator = pipeline('text-generation', model = model_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68lsHGxO8esU",
    "outputId": "47bc34a1-c9e8-4c6e-e51a-562c2439cbe6"
   },
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "q = few_shot_string\n",
    "\n",
    "# q = \"Hello, I'm a language model\"\n",
    "resp = generator(q, max_length = 30, num_return_sequences=num_return)\n",
    "\n",
    "time_end = time.time()\n",
    "\n",
    "print(f\"[{time_end-time_start:3.2f} seconds, using {model_string}]\")\n",
    "\n",
    "for i, gen in enumerate(resp):\n",
    "  print(gen[gen_text_key])\n",
    "  print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMTBW-nn_zsr"
   },
   "source": [
    "# Interactive using Gradio\n",
    "- [components doc](https://gradio.app/docs/)\n",
    "\n",
    "**Warning**\n",
    "\n",
    "When using a few-shot prompt: smaller models seem to be particularly sensitive to extra blanks at the end of the final line (the one *without* the answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ac_Ws-9_1sQ"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Use generator from pipeline above\n",
    "\n",
    "def generate(prompt, num_return, len_return):\n",
    "    resp = generator(prompt, max_length = len_return, num_return_sequences=num_return)\n",
    "\n",
    "    # Create output string\n",
    "    out = \"\\n\\n\".join( [ r[gen_text_key] for r in  resp ])\n",
    "\n",
    "    return out\n",
    "\n",
    "iface = gr.Interface(\n",
    "          generate,\n",
    "          inputs=[\n",
    "              gr.Textbox(type=\"text\",\n",
    "                         value=few_shot_string,\n",
    "                         label=\"Type your input here:\", show_label=True\n",
    "                         ),\n",
    "              gr.Number(value=num_return,\n",
    "                        precision=0,\n",
    "                        label=\"# of outputs to generate\", show_label=True\n",
    "                        ),\n",
    "              gr.Number(value=len_return,\n",
    "                        precision=0,\n",
    "                        label=\"length of output\", show_label=True\n",
    "                        ),\n",
    "          ],\n",
    "          outputs=[\n",
    "              gr.Textbox(type=\"text\", label=\"Output: \", show_label=True)\n",
    "          ],\n",
    "          title=f\"Text completion using {model_string}\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "id": "AhONjs9UEPif",
    "outputId": "2e3e6be2-1285-4931-b938-8cf7b639a3f2"
   },
   "outputs": [],
   "source": [
    "iface.launch(share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "HuggingFace",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
