{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Attention: motivation\n",
    "\n",
    "Let us consider a task familiar to all of us who have taken standardized exams: Question Answering.\n",
    "\n",
    "Input consists of two pieces of text\n",
    "- a paragraph (called the *Context*)\n",
    "- a Question whose answer can be found in the paragraph\n",
    "\n",
    "Output is a piece of text that answers the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\n",
    "\\x = \\;\n",
    "\\begin{Bmatrix}\\\\\n",
    "\\text{Context:} & \\text{The FRE Dept offers many Spring classes.  The students are great. ...} \\\\\n",
    "& \\vdots \\\\\n",
    "& \\text{Professor Perry taught them Machine Learning. The students ...}, \\\\\n",
    "& \\vdots \\\\\n",
    "& \\text{Professor Blecherman led a class in ...} \\\\\n",
    "& \\vdots \\\\\n",
    "\\text{Question:} & \\text{What did Professor Perry do ?} \\\\\n",
    "\\end{Bmatrix}\n",
    "$\n",
    "<br><br><br>\n",
    "$\n",
    "\\y = \\;\n",
    "\\begin{array} \\\\\n",
    "\\text{Answer:} & \\text{He taught them Machine Learning}\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us hypothesize how a model might learn to solve this task\n",
    "- it is only an hypothesis: we don't really know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The model might have generalized\n",
    "- from seeing many training examples of disparate questions and their answers\n",
    "- that there is a parameterized *template* for both the Question and the Answer\n",
    "\n",
    "Question Template\n",
    "\n",
    "> What did Professor `<PROPER NOUN>` teach in the Spring ?\n",
    "\n",
    "Answer Template:\n",
    "```\n",
    "<PRONOUN> <VERB> <INIDRECT OBJECT> <OBJECT>\n",
    "```\n",
    "\n",
    "where `<PROPER NOUNE>, <PRONOUN>, <VERB>`, etc. are *pattern place-holders* parameters.\n",
    "\n",
    "By using a parameterized template, the model captures\n",
    "- commonality\n",
    "- in many different types of questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to produce the answer the model needs to\n",
    "- Generate the tokens of the Answer Template in order\n",
    "- Substituting in concrete values for the place-holders\n",
    "    - by performing a Lookup in the Context in order to obtain these values\n",
    "\n",
    "We will examine how the Lookup might be performed\n",
    "- first: by using mechanisms that we have already studied\n",
    "- subsequently: via a new mechanism called *Attention*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using an RNN without Attention\n",
    "\n",
    "## Encoder-Decoder architecture: review\n",
    "\n",
    "For the Question Answering task\n",
    "- both the Input and Output are sequences\n",
    "- thus, the task is a Sequence to Sequence task\n",
    "    - just like: Language Translation\n",
    "\n",
    "We learned that Recurrent architectures are best-suited for processing sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "These architectures\n",
    "- operate in a \"loop\"\n",
    "    - processing one Input or Output token at a time\n",
    "- utilize **memory** (latent state)\n",
    "    - necessary because Input/Output sequence lengths are unbounded\n",
    "    - after processing the token at position $\\tt$\n",
    "        - the latent state is finite representation of the prefix of the sequence of length $\\tt$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The use of latent state/memory evolved over the models we studied\n",
    "- RNN\n",
    "    - latent state encodes\n",
    "        - input representation\n",
    "        - \"control\" state\n",
    "            - guiding how the model processes the data: state transitions\n",
    "- LSTM\n",
    "    - latent state partitioned into\n",
    "        - Short Term memory: control state\n",
    "        - Long Term memory\n",
    "        \n",
    "Both these models processed the input sequence **once**\n",
    "- so input-specific representation needs to be part of memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A common architecture for Sequence to Sequence tasks is the Encoder-Decoder:\n",
    "- The Encoder is an RNN\n",
    "    - Acts on input sequence $[\\x_{(1)} \\dots \\x_{(\\bar{T})}]$\n",
    "    - Producing a sequence of latent states $[ \\bar{\\h}_{(1)}, \\dots, \\bar{\\h}_{(\\bar{T})} ]$\n",
    "        - latent state $\\bar{\\h}_\\tt$ is a summary of $[\\x_{(1)} \\dots \\x_\\tp ]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Decoder\n",
    "- Acts on the *final* Encoder latent state $\\bar{\\h}_{(\\bar{T})}$\n",
    "    - which summarizes the entire input sequence $\\x$\n",
    "    - Producing a sequence of latent states $[ \\h_{(1)}, \\dots, \\h_{(T)} ]$\n",
    "        - latent state $\\h_\\tp$ is response for generating output token $\\hat{\\y}_\\tp$\n",
    "    - Thus outputting a sequence  $[ \\hat{\\y}_{(1)}, \\dots, \\hat{\\y}_{(T)} ]$\n",
    "- Often feeding step $\\tt$ output $\\hat{\\y}_\\tp$ as Encoder input at step $(\\tt+1)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Encoder/Decoder</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_Encoder_Decoder.png\"</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Decoder output $\\hat\\y_\\tp$\n",
    "\n",
    "The simplest RNN (corresponding to our diagrams) use the latent state $\\h_\\tp$ as the output $\\hat\\y_\\tp$\n",
    "$$\n",
    "\\hat\\y_\\tp = \\h_\\tp\n",
    "$$\n",
    "\n",
    "It is easy to add another NN to transform $\\h_\\tp$ into a $\\hat\\y_\\tp$ that is different.\n",
    "\n",
    "- We can add a NN to the Decoder RNN that implements a function $D$ that transforms the latent state into an output.\n",
    "\n",
    "$$\\hat\\y_\\tp = D(\\h_\\tp)$$\n",
    "\n",
    "For clarity: we will omit this additional NN from our diagrams until it becomes necessary\n",
    "\n",
    "Here is what the additional NN looks like:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Decoder output transformation: No attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Decoder_no_attention.png\" width=70%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How does the Decoder perform Lookup (without Attention) ?\n",
    "\n",
    "Suppose the Decoder has already output \n",
    "$$\\hat\\y_{([1:3])} = \\text{He taught them}$$\n",
    "\n",
    "It must subsequently output\n",
    "$$\n",
    "\\hat\\y_{([4:5])} = \\text{Machine Learning}$$\n",
    "\n",
    "In order to do this\n",
    "- it must Lookup \"Machine Learning\" in the Context, resulting in\n",
    "$$\\begin{array} \\\\\n",
    "D( \\h_{(4)}) & = & \\text{Machine} \\\\\n",
    "D( \\h_{(5)}) & = & \\text{Learning} \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But  $D$ is conditioned on the single input $\\h_\\tp$.\n",
    "\n",
    "Thus, in order for $D( \\h_{(4)} )$ to be equal to \"Machine\"\n",
    "- this information must somehow be encoded in $\\h_{(4)}$\n",
    "\n",
    "\n",
    "How did it get there ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "All \"knowledge\" from the Context must be transfered from Encoder to Decoder\n",
    "- through final Encoder state $\\bar \\h_{(\\bar T)}$\n",
    "- which in turn was encoded in all Encoder states $\\bar \\h_{({\\bar \\tt}')} \\text{ for } \\tt' \\ge \\bar p$\n",
    "    - where $\\bar p$ is the position withing sequence $\\x$ of the word \"Machine\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can hypothesize that the final Encoder latent state $\\bar\\h_{\\bar T}$\n",
    "- encodes a Dictionary (key/value pairs)\n",
    "- mapping Place-holder names to Concrete values\n",
    "- the dictionary is built incrementally by prior latent states of the Encoder\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Answering questions using Attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Attention_example_1.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This dictionary is passed to the Decoder via the single connection from Encoder to Decoder\n",
    "- and must be carried forward by the Decoder\n",
    "- through Decoder states $[ \\h_{(1)}, \\dots, \\h_{(4)} ]$\n",
    "- in order to make the dictionary available to subsequent latent states of the Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We further hypothesize that the Decoder\n",
    "- performs Lookups \n",
    "- by using the Decoder latent state $\\h_\\tp$\n",
    "    - as a *query* that matches against the keys of the Dictionary\n",
    "    - in order to obtain the Concrete value required to produced output token at position $\\tt$\n",
    "    $$\\hat\\y_\\tp = D(\\h_\\tp)$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Query performing a Lookup in the Dictionary</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Attention_example_2.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a picture describing this hypothetical functioning.\n",
    "<br>\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Encoder/Decoder without Attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_Encoder_Decoder.png\" width=80%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Encoder_Decoder_no_attention.png\" width=70%</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Connecting the Encoder and Decoder through the \"bottleneck\" of $\\bar \\h_{(\\bar T)}$ thus burdens the\n",
    "- Encoder: passing knowledge forward to the bottleneck\n",
    "- Decoder: passing knowledge from the bottleneck\n",
    "\n",
    "This results in an inefficient use of the model's latent state variable\n",
    "- In addition to\n",
    "    - the Encoder and Decoder allocating some of the model's latent state for \"control\"\n",
    "    - guiding the loop that processes the Input, or generates the output positions in the template\n",
    "- It must **also** allocate some of the model's latent state for \"knowledge storage\"\n",
    "    - in order to Lookup the concrete value corresponding to a place-holder in the Output template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Attention\n",
    "\n",
    "**Reference**\n",
    "\n",
    "[Neural Machine Translation by Jointly Learning To Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "[paper that introduced Attention](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "\n",
    "<br><br>\n",
    "The flaw in the Encoder-Decoder without Attention is \n",
    "- the input $\\x$ is processed *only once*\n",
    "- by the Encoder\n",
    "- which has to summarize it in $\\bar{\\h}_{(\\bar T)}$\n",
    "\n",
    "We will introduce a mechanism called *Attention*\n",
    "- that allows the input sequence to be *re-visited* at each time step of Output generation\n",
    "\n",
    "This will result in a cleaner separation between control memory and input memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Attention allows the Decoder\n",
    "- to directly access all of the Encoder latent states $\\bar\\h_{(1)} \\dots \\h_{(\\bar{T})}$\n",
    "- at each time step of the Decoder\n",
    "\n",
    "Thus, there is no need\n",
    "- for an Encoder to create a full dictionary as the final Encoder latent state $\\bar\\h_{(\\bar T)}$\n",
    "- for the Decoder to keep the dictionary in all it's latent states $\\h_{(1)} \\dots \\h_{(T)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a picture of an Encoder/Decoder augmented with Attention\n",
    "- we have add an additional box to the diagram for the NN that implements the function $D$\n",
    "    - that maps $\\h_\\tp$ to $\\y_\\tp$\n",
    "<br>\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Encoder/Decoder with Attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_API_Encoder_Decoder_Attention.png\" width=80%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Encoder_Decoder_no_attention.png\" width=70%</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice that the final Encoder latent state $\\bar\\h_{(\\bar T)}$ is **no longer**  connected to the Decoder.\n",
    "\n",
    "What is going on inside the \"box\" implementing function $D$ that we added at each time step ?\n",
    "\n",
    "The box's input at step $\\tt$\n",
    "- the Decoder latent state $\\h_\\tp$\n",
    "- the collection of Encoder latent states $\\bar\\h_{(1)} \\dots \\h_{(\\bar{T})}$\n",
    "    - the red box in the above diagram\n",
    "\n",
    "That is, it is computing a $\\hat\\y_\\tp$ that is a function of both $\\h_\\tp$ and $\\bar\\h_{(1)} \\dots \\h_{(\\bar{T})}$\n",
    "\n",
    "$$\n",
    "\\hat\\y_\\tp = D( \\h_\\tp,  [ \\bar\\h_{(1)} \\dots \\h_{(\\bar{T})} ])\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here are the inner workings of the NN for $D$:\n",
    "    \n",
    "<br>\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Decoder output transformation with attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Decoder_attention.png\" width=60%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Inside the box:\n",
    "- the Decoder latent state $\\h_\\tp$ is used as a *query*\n",
    "- which is matched against each of the Encoder latent states\n",
    "- resulting in one Encoder latent state being chosen as $\\mathbf{c}_\\tp$\n",
    "\n",
    "The chosen Encoder latent state $\\mathbf{c}_\\tp$ and Decoder latent state $\\h_\\tp$\n",
    "- are input to another Neural Network\n",
    "- which produces output $\\hat\\y_\\tp$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Essentially we change  $D$ so that it is conditioned on\n",
    "- $\\h_\\tp$: the query (Decoder state)\n",
    "- **and** summary of the Context $\\bar \\h_{([1:\\bar T])}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "The \"Choose\" box implements an *Attention* mechanism, which allows the Decoder\n",
    "- to **attend to** the part of Input $\\x$ (represented via some Encoder latent state $\\bar\\h_{(\\bar\\tt)}$)\n",
    "- that is *relevant* for producing $\\hat \\y_\\tp$\n",
    "- exactly when it is needed\n",
    "\n",
    "This seems very natural to a human\n",
    "- rather than memorizing details (e.g., the big dictionary $\\bar\\h_{(\\bar T)}$ in the architecture without Attention)\n",
    "- we refer back to the context\n",
    "- focusing of only the part that is immediately needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A big advantage of this approach:\n",
    "- the latent state of the Decoder is solely for \"control\" (e.g., creating the output according to the Answer template)\n",
    "- and not for storing \"knowledge\" (dictionary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The discussion of the **implementation** of Attention will be deferred to a\n",
    "later module [Attention lookup](Attention_Lookup.ipynb).\n",
    "\n",
    "For now, think of the \"Choose\" box as a Context Sensitive Memory (as described in the module on [Neural Programming](Neural_Programming.ipynb#Soft-Lookup))\n",
    "- Like a Python `dict`\n",
    "    - Collection of key/value pairs: $\\langle \\bar\\h_{(\\bar \\tt)}, \\bar\\h_{(\\bar \\tt)} \\rangle$\n",
    "    - Key is equal to value; they are latent states of the Encoder\n",
    "- But with *soft* lookup\n",
    "    - The current Decoder state $\\h_\\tp$ is presented to the CSM \n",
    "        - Called the *query*\n",
    "        - Is matched across each key of the dict (i.e., a latent state $\\bar \\h_{(\\bar \\tt)}$)\n",
    "    - The CSM returns an approximate match of the query to a *key* of the `dict`\n",
    "        - The distance between the query and each key in the CSM is computed\n",
    "        - The Soft Lookup returns a *weighted* (by inverse distance) sum of the *values* in the CSM `dict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Have we seen this before ?\n",
    "\n",
    "If you recall the architecture of the LSTM\n",
    "- *short term* (control) memory \n",
    "- was separated from *long term* memory\n",
    "- elements of long term memory are moved to short term memory *as needed*\n",
    "\n",
    "This is partly similar to the advantages of Attention.\n",
    "\n",
    "But\n",
    "- all factual information from input $\\x$ has to flow through the bottleneck $\\bar \\h_{(\\bar T)}$ of the Encoder output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Visualizing Attention\n",
    "\n",
    "We can illustrate the behavior of Neural Networks that have been augmented with Attention through diagrams.\n",
    "- at a particular output position $\\tt$\n",
    "- we can display the amount of \"attention\"\n",
    "- that each position in the input receives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Attention can be used to create a Context Sensitive Encoding of words\n",
    "- The meaning of a word may change depending on the rest of the sentence\n",
    "\n",
    "We can illustrate this with an example: how the meaning of the word \"it\" changes\n",
    "- The thickness of the blue line indicates the attention weight that is given in processing the word \"it\".\n",
    "\n",
    "<img src=https://1.bp.blogspot.com/-AVGK0ApREtk/WaiAuzddKVI/AAAAAAAAB_A/WPV5ropBU-cxrcMpqJBFHg73K9NX4vywwCLcBGAs/s1600/image2.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Much of the recent advances in NLP may be attributed to these improved, context sensitive embeddings.\n",
    "\n",
    "We note that simple Word Embeddings\n",
    "- also capture \"meaning\"\n",
    "- but are *not* sensitive to context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "**Entailment: Does the \"hypothesis\" logically follow from the \"premise\"**\n",
    "\n",
    "<br>\n",
    "<center><strong>Attention: Entailment</strong></center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Attention_visualization_Entailment.png\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center>Does the Premise logically entail the Hypothesis.</center></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Attribution: https://arxiv.org/pdf/1509.06664.pdf#page=6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Date normalization example**\n",
    "- Source: Dates in free-form: \"Saturday 09 May 2018\"\n",
    "- Target: Dates in normalized form: \"2018-05-09\"\n",
    "\n",
    "[link](https://github.com/datalogue/keras-attention#example-visualizations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Image captioning example**\n",
    "- Source: Image\n",
    "- Target: Caption: \"A woman is throwing a **frisbee** in a park.\"\n",
    "- Attending over *pixels* **not** sequence\n",
    "\n",
    "<center><strong>Visual attention</strong></center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images/shat_-002-027.jpg\"></td>\n",
    "        <td><img src=\"images/shat_-002-028.jpg\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=2><center>A woman is throwing a <strong>frisbee</strong> in a park.</center></td>\n",
    "    </tr>\n",
    "</table>\n",
    "Attribution: https://arxiv.org/pdf/1502.03044.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Self-attention\n",
    "\n",
    "The are several \"flavors\" of Attention.\n",
    "\n",
    "We have motivated the Attention from a Decoder to the output of an Encoder.\n",
    "\n",
    "The Attention between two different Neural Networks (e.g., Decoder, Encoder) is called *Cross-Attention*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Cross Attention allows the Decoder to not have to expend part of its latent state encoding \"knowledge\"\n",
    "- it can obtain the knowledge \"just in time\" via an Attention query to the output of the Encoder\n",
    "\n",
    "But the Decoder's latent state, when generating output $\\hat\\y_\\tp$, must still \"remember\" the partial output $\\hat\\y_{(1:\\tt-1)}$ previously generated.\n",
    "\n",
    "For example, suppose the Decoder is generating a long sentence\n",
    "- in many languages, there needs to be agreement between the gender/plurality of a subject and verbs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can use a different flavor of Attention to enable the Decoder to attend to its own inputs.\n",
    "\n",
    "This is called *Self-Attention*\n",
    "- when generating $\\hat\\y_\\tp$: attend to the most relevant parts of $\\hat\\y_{(1:\\tt-1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Encoder can also benefit from Self-Attention\n",
    "- when creating $\\bar\\h_\\tp$, attend to the relevant parts of $\\x$, not just $\\x_\\tp$\n",
    "    -  what is the meaning of the word \"it\" in the two sentences:\n",
    "    \n",
    "             The animal didn't cross the street because **it** was too tired\n",
    "        \n",
    "        - attend to \"animal\"\n",
    "        \n",
    "              The animal didn't cross the street because **it** was too wide\n",
    "         \n",
    "     - attend to \"street\"\n",
    "\n",
    "We will see an example of Self-Attention in our module on the Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Masked attention\n",
    "\n",
    "As presented, the Attention mechanism can refer to an entire sequence\n",
    "- e.g., the sequence of Encoder latent states\n",
    "\n",
    "This is very powerful but not appropriate in all cases.\n",
    "- It is allowed to access elements of the input sequence \"out of order\" for some tasks\n",
    "    - for example: the task of creating a context-sensitive representation of each word in a sentence\n",
    "        - typical use case for an Encoder\n",
    "- it is *not* allowed to access a future element for some tasks\n",
    "    - for example: a trading strategies decisions at time $\\tt$ must not be influenced by inputs occurring after time $\\tt$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Restricting attention to the past is called *Causal Attention*.\n",
    "- the next output depends only on things that could have caused it (the past), not the future\n",
    "\n",
    "There is a mechanism to restrict what may be attended to in a general way\n",
    "- create a \"mask\"\n",
    "- a bit vector for each position of the sequence being attended to\n",
    "- such that attention is limited to positions where the mask element if True.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is called *Masked Attention*.\n",
    "\n",
    "It is frequently used to enable a Decoder, when predicting output $\\hat \\y_\\tp$\n",
    "- to attend to **previously** generate outputs $\\hat \\y_{[1:\\tt-1])}$\n",
    "- but not **future** outputs $\\hat\\y_{(\\tt')}$ for $\\tt' \\ge \\tt$\n",
    "\n",
    "When used in this manner, we refer to the behavior as *Masked Self Attention*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Aside**\n",
    "\n",
    "You may wonder how it is even practically possible for a Decoder to refer to the future.\n",
    "\n",
    "When using *Teacher Forcing* for **training**\n",
    "- the Decoder does not use the *predicted* target sequence $\\hat \\y_{(1:T)}$\n",
    "- the Decoder uses the *actual* target sequence $\\y_{(1:T)}$\n",
    "    - hence, \"future\" positions $\\tt' \\ge \\tt$ are available\n",
    "- this prevents a single mis-prediction at position $\\tt$ from cascading and ruining all future output\n",
    "    - facilitates training\n",
    "- at inference time: the Decoder works on the *predicted* Target sequence.\n",
    "\n",
    "In the diagram below, we illustrate (lower right) how the Decoder input changes between Training and Test/Inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Sequence to Sequence: training (teacher forcing) + inference: No attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_seq2seq.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Multi-head attention: two heads are better than one\n",
    "\n",
    "Perhaps when generating the output for position $\\tt$ of the output sequence\n",
    "- we need to attend to *more than one* position of the sequence being attended to\n",
    "    - need to know both gender and plurality of subject\n",
    "- that is: we want an Attention layer to output multiple items.\n",
    "\n",
    "We can attend to $n$ positions\n",
    "- by creating $n$ separate Attention mechanisms\n",
    "- each one called a *head*\n",
    "\n",
    "This behavior is referred to as *Multi-head attention*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This type of behavior is common to many layer types in a Neural Network\n",
    "- a Dense layer $l$ may produce a vector $\\y_\\llp$ where $n_\\llp \\gt 1$\n",
    "- a Convolutional layer $l$ may produce outputs (for each spatial location) for many channels\n",
    "\n",
    "We have referred to this as layer $\\ll$ producing $n_\\llp$ *features*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It would be natural for an Attention layer to output many \"features\" to enable attention to many positions.\n",
    "\n",
    "In practice, this is sometimes (always ?) not done\n",
    "- Model architectures (e.g., the Transformer) are simplified when the inputs/outputs of each sub-component\n",
    "- have the same length\n",
    "- often denoted as $d$ or $d_\\text{model}$ in the Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When a Transformer needs to attend to $n$ positions\n",
    "- it uses $n$ Attention heads\n",
    "- each outputting a vector of length $\\frac{d}{n}$\n",
    "- which are concatenated together to produce a single output of length $d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When we have $n$ heads\n",
    "- Rather than having one Attention head operating on vectors of length $d$\n",
    "    - producing an output of length $d$ (weighted sum of values in the CSM)\n",
    "- We create $n$ Attention heads operating on vectors (keys, values, queries) of length $d \\over n$.\n",
    "    - Output of these smaller heads are values, and hence also of length $d \\over n$\n",
    "- The final output concatenates these $n$ outputs into a single output of length $d$\n",
    "    - identical in length to the single head\n",
    "- we project each of these length $d$ vector into vectors of length $d \\over n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The picture shows $n$ Attention heads.\n",
    "\n",
    "Note that each head is working on vectors of length $\\frac{d}{n}$ rather than\n",
    "original dimensions $d$.\n",
    "- variables with superscript $(j)$ are of fractional length\n",
    "\n",
    "Details are deferred to the module [Attention lookup](Attention_Lookup.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each head $j$ uniquely transforms the query $\\h_\\tp$ and the key/value pairs $\\bar{\\h}_{(1)} \\ldots \\bar{\\h}_{(\\bar{T})}$ being queried.\n",
    "- into $\\h^{(j)}_\\tp$ and the key/value pairs $\\bar{\\h}^{(j)}_{(1)} \\ldots \\bar{\\h}^{(j)}_{(\\bar{T})}$\n",
    "- Such that each head attends to a separate item\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Decoder Multi-head Attention</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Multihead_attention.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transformers\n",
    "\n",
    "There is a new model (the Transformer) that processes sequences much faster than RNN's.\n",
    "\n",
    "It is an Encoder/Decoder architecture that uses multiple forms of Attention\n",
    "- Self Attention in the Encoder\n",
    "    - to tell the Encoder the relevant parts of the input sequence $\\x$ to attend to\n",
    "- Decoder/Encoder attention\n",
    "    - to tell the Decoder which Encoder state $\\bar{\\h}_{(\\tt')}$ to attend to when outputting $\\y_\\tp$\n",
    "- Masked Self-Attention in the Decoder\n",
    "    - to prevent the Decoder from looking ahead into inputs that have not yet been generated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "We recognized that the Decoder function responsible for generating Decoder output $\\hat{\\y}_\\tp$\n",
    "$$\n",
    "\\hat{\\y}_\\tp = D( \\h_\\tp; \\mathbf{s})\n",
    "$$\n",
    "\n",
    "was quite rigid when it ignored argument $\\mathbf{s}$.\n",
    "\n",
    "This rigidity forced Decoder latent state $\\h_\\tp$ to assume the additional responsibility of including Encoder context.\n",
    "\n",
    "Attention was presented as a way to obtain Encoder context through argument $\\mathbf{s}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
