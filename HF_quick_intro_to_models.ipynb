{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "A8rdySnCAsDh",
        "outputId": "06140ddb-e48a-4ec3-9f21-7aab0a91a571"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Latex object>"
            ],
            "text/latex": "$$\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\tx}{\\tilde{\\x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\b}{\\mathbf{b}}\n\\newcommand{\\c}{\\mathbf{c}}\n\\newcommand{\\e}{\\mathbf{e}}\n\\newcommand{\\z}{\\mathbf{z}}\n\\newcommand{\\h}{\\mathbf{h}}\n\\newcommand{\\u}{\\mathbf{u}}\n\\newcommand{\\v}{\\mathbf{v}}\n\\newcommand{\\w}{\\mathbf{w}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\newcommand{\\W}{\\mathbf{W}}\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\KL}{\\mathbf{KL}}\n\\newcommand{\\E}{{\\mathbb{E}}}\n\\newcommand{\\Reals}{{\\mathbb{R}}}\n\\newcommand{\\ip}{\\mathbf{{(i)}}}\n%\n% Test set\n\\newcommand{\\xt}{\\underline{\\x}}\n\\newcommand{\\yt}{\\underline{\\y}}\n\\newcommand{\\Xt}{\\underline{\\X}}\n\\newcommand{\\perfm}{\\mathcal{P}}\n%\n% \\ll indexes a layer; we can change the actual letter\n\\newcommand{\\ll}{l}\n\\newcommand{\\llp}{{(\\ll)}}\n%\n\\newcommand{Thetam}{\\Theta_{-0}}\n\n% CNN\n\\newcommand{\\kernel}{\\mathbf{k}} \n\\newcommand{\\dim}{d}\n\\newcommand{\\idxspatial}{{\\text{idx}}}\n\\newcommand{\\summaxact}{\\text{max}}\n\\newcommand{idxb}{\\mathbf{i}}\n%\n%\n\n% RNN\n% \\tt indexes a time step\n\\newcommand{\\tt}{t}\n\\newcommand{\\tp}{{(\\tt)}}\n%\n%\n\n% LSTM\n\\newcommand{\\g}{\\mathbf{g}}\n\\newcommand{\\remember}{\\mathbf{remember}}\n\\newcommand{\\save}{\\mathbf{save}}\n\\newcommand{\\focus}{\\mathbf{focus}}\n%\n%\n% NLP\n\\newcommand{\\Vocab}{\\mathbf{V}}\n\\newcommand{\\v}{\\mathbf{v}}\n\\newcommand{\\offset}{o}\n\\newcommand{\\o}{o}\n\\newcommand{\\Emb}{\\mathbf{E}}\n%\n%\n\\newcommand{\\loss}{\\mathcal{L}}\n\\newcommand{\\cost}{\\mathcal{L}}\n%\n%                     \n\\newcommand{\\pdata}{p_\\text{data}}\n\\newcommand{\\pmodel}{p_\\text{model}}\n%\n% SVM\n\\newcommand{\\margin}{{\\mathbb{m}}}\n\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n%\n%\n% Functions with arguments\n\\def\\xsy#1#2{#1^#2}\n\\def\\rand#1{\\tilde{#1}}\n\\def\\randx{\\rand{\\x}}\n\\def\\randy{\\rand{\\y}}\n\\def\\trans#1{\\dot{#1}}\n\\def\\transx{\\trans{\\x}}\n\\def\\transy{\\trans{\\y}}\n%\n\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n%\n\\def\\pr#1{\\mathcal{p}(#1)}\n\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n\\def\\cnt#1{\\mathcal{count}_{#1}}\n\\def\\node#1{\\mathbb{#1}}\n%\n\\def\\loc#1{{\\text{##} {#1}}}\n%\n\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n%\n% Expectation operator\n\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n%\n% VAE\n\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n\\def\\qr#1{\\mathcal{q}(#1)}\n\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n%\n% Reinforcement learning\n\\newcommand{\\Actions}{{\\mathcal{A}}} \n\\newcommand{\\actseq}{A}\n\\newcommand{\\act}{a}\n\\newcommand{\\States}{{\\mathcal{S}}}   \n\\newcommand{\\stateseq}{S}  \n\\newcommand{\\state}{s}\n\\newcommand{\\Rewards}{{\\mathcal{R}}}\n\\newcommand{\\rewseq}{R}\n\\newcommand{\\rew}{r}\n\\newcommand{\\transp}{P}\n\\newcommand{\\statevalfun}{v}\n\\newcommand{\\actvalfun}{q}\n\\newcommand{\\disc}{\\gamma}\n%\n%\n\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n%\n%\n$$\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "%run Latex_macros.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "983g5CcnKy9_"
      },
      "source": [
        "Ken Perry attribution:\n",
        "- Derived from [HuggingFace Course, Chapt 2, \"Putting it all together\"](https://huggingface.co/course/chapter2/6?fw=tf)\n",
        "  - [Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section6_tf.ipynb)\n",
        "      - link to Github repo no longer works\n",
        "      - repo probably updated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmyrsKheIEzT"
      },
      "source": [
        "# Putting it all together (TensorFlow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHNBaqjxIEzW"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yady_uquIEzW",
        "outputId": "a23339f9-7867-4941-8418-f83565e342d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vL-9sUQLTlB"
      },
      "source": [
        "# Tokenize the input\n",
        "\n",
        "The Transformer's inputs are sequences of *token identifiers* (of type integer)\n",
        "- Need to convert text into tokens (\"word parts\")\n",
        "- Need to convert the tokens to token identifiers\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8_rvy7nL0h1"
      },
      "source": [
        "A *model* is identified by a **checkpoint**\n",
        "  - string identifying the model architecture and state at which training was ended\n",
        "    - n.b., if you train for longer, the weights will change (resulting in a different checkpoint)\n",
        "\n",
        "A pre-trained model is usually paired with the Tokenizer on which it was trained.\n",
        "\n",
        "We can obtain the Tokenizer from a checkpoint via `AutoTokenizer.from_pretrained(checkpoint)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PfJiKXMlIEzX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "337e9566-a6a6-4d51-bce3-a674eef60141"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qRYhJpkMZDn"
      },
      "source": [
        "Let's understand the Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXLtXUyELyrq",
        "outputId": "81d7c433-9a63-46fc-86d1-a6c2e2b1450d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model inputs:  {'input_ids': [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "Model inputs (input_ids):  [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n"
          ]
        }
      ],
      "source": [
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "model_inputs = tokenizer(sequence)\n",
        "\n",
        "print(\"Model inputs: \", model_inputs)\n",
        "\n",
        "print(\"Model inputs (input_ids): \", model_inputs[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "291_qipoNAuI"
      },
      "source": [
        "The `input_ids` key are the *token identifiers*.\n",
        "\n",
        "Out of curiousity, we can obtain the token identifiers in 2 sub-steps\n",
        "- convert text to tokens\n",
        "- convert tokens to token identifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ywxrl5dLa-z",
        "outputId": "11958991-503e-4a94-b8e1-4937e9151d3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text:  I've been waiting for a HuggingFace course my whole life.\n"
          ]
        }
      ],
      "source": [
        "print(\"Text: \", sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCzBBbURNQOR",
        "outputId": "1ccf2a60-570b-4b35-d8a6-40b1fa93394e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text:  I've been waiting for a HuggingFace course my whole life.\n",
            "\n",
            "First step: Manually convert sequence of characters to sequence of tokens\n",
            "Tokens:  ['i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'hugging', '##face', 'course', 'my', 'whole', 'life', '.']\n",
            "\n",
            "Second step: Manually convert tokens to ids\n",
            "Token identifiers:  [1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n",
            "\n",
            "Verified ! token_ids == model_inputs[\"input_ids\"][1:-1]\n",
            "\n",
            "\tThat is: model_inputs has bracketed the token_ids with the special start and end tokens\n",
            "\n",
            "\n",
            "Decoded model inputs (input_ids):  [CLS] i've been waiting for a huggingface course my whole life. [SEP]\n",
            "Decoded token identifiers:  i've been waiting for a huggingface course my whole life.\n"
          ]
        }
      ],
      "source": [
        "print(\"Text: \", sequence)\n",
        "\n",
        "print(\"\\nFirst step: Manually convert sequence of characters to sequence of tokens\")\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "print(\"Tokens: \", tokens)\n",
        "\n",
        "print(\"\\nSecond step: Manually convert tokens to ids\")\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(\"Token identifiers: \", token_ids)\n",
        "\n",
        "# Verify that the sequence of token ids created manually is identical to that created by the one-step process\n",
        "model_inputs = tokenizer(sequence)\n",
        "\n",
        "assert(token_ids == model_inputs[\"input_ids\"][1:-1])\n",
        "print('\\nVerified ! token_ids == model_inputs[\"input_ids\"][1:-1]')\n",
        "print('\\n\\tThat is: model_inputs has bracketed the token_ids with the special start and end tokens')\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Decoded model inputs (input_ids): \", tokenizer.decode(model_inputs[\"input_ids\"]))\n",
        "print(\"Decoded token identifiers: \", tokenizer.decode(token_ids) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmUFZFlcQtnH"
      },
      "source": [
        "You can see that the\n",
        "- `input_ids` has the special token `[CLS]` added at the start and `[SEP]` added at the end of the text\n",
        "- These special tokens are required by the Transformer model\n",
        "\n",
        "`token_ids` is identical to `input_ids` except for these special tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8HHsFH9Rhss"
      },
      "source": [
        "The Tokenizer's behavior can be modified.\n",
        "\n",
        "When dealing with more than one example, the example lengths (after tokenization) may have different lengths.\n",
        "\n",
        "The Tokenizer can adapt it's behavior.\n",
        "\n",
        "We just list the behavior without going further into it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cDLhAvvbIEzZ"
      },
      "outputs": [],
      "source": [
        "# Will pad the sequences up to the maximum sequence length\n",
        "model_inputs = tokenizer(sequence, padding=\"longest\")\n",
        "\n",
        "# Will pad the sequences up to the model max length\n",
        "# (512 for BERT or DistilBERT)\n",
        "model_inputs = tokenizer(sequence, padding=\"max_length\")\n",
        "\n",
        "# Will pad the sequences up to the specified max length\n",
        "model_inputs = tokenizer(sequence, padding=\"max_length\", max_length=8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "74leV8LeIEzZ"
      },
      "outputs": [],
      "source": [
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "# Will truncate the sequences that are longer than the model max length\n",
        "# (512 for BERT or DistilBERT)\n",
        "model_inputs = tokenizer(sequences, truncation=True)\n",
        "\n",
        "# Will truncate the sequences that are longer than the specified max length\n",
        "model_inputs = tokenizer(sequences, max_length=8, truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr8A2tHkIEzb",
        "outputId": "a6527891-d7d0-4e1a-9589-a5c009be21d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "output = model(**tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yom9MtlIJkPw",
        "outputId": "f007e5cb-4fdb-47a4-84ea-9a6cd85e6bb6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
              "array([[-1.560696 ,  1.6122813],\n",
              "       [-3.6183183,  3.9137495]], dtype=float32)>, hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWU37gpFSbmI"
      },
      "source": [
        "The output is a `Tensor`\n",
        "- they are the `logits` (scores, **not** probabilities) of the Binary Classification model\n",
        "\n",
        "Convert them to probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvZvqZgBJnUG",
        "outputId": "6facff20-e31a-4f5a-9415-e7da4c18ef81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 0: Class 1 with probability 0.96\n",
            "Example 1: Class 1 with probability 1.00\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "probs = tf.nn.softmax(output[\"logits\"]).numpy()\n",
        "\n",
        "ex_classes = np.argmax(probs, axis=1)\n",
        "\n",
        "for i, prob in enumerate(probs):\n",
        "  ex_class = ex_classes[i]\n",
        "  print(f\"Example {i}: Class {ex_class:d} with probability {probs[i, ex_class]:3.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX1e6KLpAsDn"
      },
      "source": [
        "# Classifier model output type: logits vs probabilities\n",
        "\n",
        "There is a **subtle but important** way to pass Loss function names into Keras when using HuggingFace.\n",
        "\n",
        "Recall that some Classifiers, e.g., Logistic Regression, work by\n",
        "- computing a score/logit\n",
        "$$\n",
        "\\text{logit} = \\Theta \\cdot \\x\n",
        "$$\n",
        "- converting the logit to a probability\n",
        "    - by applying a `softmax` to the logits\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7ZE5FSRAsDn"
      },
      "source": [
        "Our practice has been to assume that\n",
        "- the model output\n",
        "$$\\y = \\text{model}(\\x)$$\n",
        "- is a *probability* vector\n",
        "    - Given possible labels/classes\n",
        "    $$ C = \\{ c_1, \\ldots, c_\\text{#C} \\}$$\n",
        "    - $y_j$ is the probability that input $\\x$ is from class $c_j$\n",
        "    \n",
        "**However**: the HuggingFace standard is that $\\y$ are **logits** rather than probabilities\n",
        "- values *before* applying a softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHNY1Ap8AsDn"
      },
      "source": [
        "The import of the difference is that\n",
        "- the *loss function* must know\n",
        "- that the model is returning *logits*, rather than *probabilities* (the Keras default)\n",
        "\n",
        "In Keras, we can pass the loss either\n",
        "- as a function object\n",
        "    - e.g., `tf.keras.losses.SparseCategoricalCrossentropy`\n",
        "- or a *string* denoting the function\n",
        "    - e.g., `sparse_categorical_crossentropy`\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_EL0KRiAsDn"
      },
      "source": [
        "To conform to the HuggingFace standard\n",
        "- we should specify the loss as a function\n",
        "- passing in an (optional) argument indicating that the model output are logits\n",
        "    - e.g., `SparseCategoricalCrossentropy(from_logits=True)`\n",
        "\n",
        "So the typical compile statement should look like\n",
        "\n",
        "        model.compile(\n",
        "              ...,\n",
        "              loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "              ...)\n",
        "              \n",
        "rather than\n",
        "\n",
        "\n",
        "        model.compile(\n",
        "              ...,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              ...)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K026g7hAsDn"
      },
      "source": [
        "See the [warning for common pitfall](https://huggingface.co/learn/nlp-course/chapter3/3?fw=tf)\n",
        "\n",
        "```\n",
        "Note a very common pitfall here — you can just pass the name of the loss as a string to Keras, but by default Keras will assume that you have already applied a softmax to your outputs. Many models, however, output the values right before the softmax is applied, which are also known as the logits. We need to tell the loss function that that’s what our model does, and the only way to do that is to call it directly, rather than by name with a string.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2f1gGKLAsDo"
      },
      "source": [
        "**Remember**\n",
        "\n",
        "- the Loss function must be compatible with the type of the model output\n",
        "    - logits or probabilties"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examining the model\n",
        "\n",
        "- inspect the `__init__` and `call` methods\n",
        "\n",
        "`__init__` will show the model components\n",
        "- we can recursively inspect the components\n",
        "\n",
        "`call` will show you how the model transforms input to output"
      ],
      "metadata": {
        "id": "uzQDNNnHE1qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.__init__??"
      ],
      "metadata": {
        "id": "sX1DfyqGJp4L"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will recursively examine the `distilbert` attribute which is a `TFDistilBertMainLayer`\n",
        "\n",
        "But first, let's examine the `call` method, which will help us understand how the components are connected."
      ],
      "metadata": {
        "id": "2ekJbjiPK_9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.call??"
      ],
      "metadata": {
        "id": "443MgN51Km1a"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you scroll through all the description, you will find the body of the call\n",
        "- the `distilbert` component is called, result assigned to `distilbert_output`\n",
        "- the `distilbert_output` is passed through several layers\n",
        "- before passing through a Classifier `self.classifier`\n",
        "  - which produces logits"
      ],
      "metadata": {
        "id": "vOtJEQOHLYEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's work our way down the model, starting with the `distilbert` attribute is what we want"
      ],
      "metadata": {
        "id": "eQw8rF52FI08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.distilbert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrLvObUhCTVr",
        "outputId": "c13903d2-e6a3-4474-c4d4-b4fe473fe281"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertMainLayer at 0x7e4979a15e40>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that it is of type `TFDistilBertMainLayer`\n",
        "\n",
        "Use the Jupyter notebook ? and ?? tools to inspect\n",
        "- the signature\n",
        "- the code"
      ],
      "metadata": {
        "id": "UoIjXJ29FT7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.distilbert?"
      ],
      "metadata": {
        "id": "kHPv9Oo7F-T9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.distilbert.__init__??"
      ],
      "metadata": {
        "id": "kiHo_RzMGFvC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.distilbert.call??"
      ],
      "metadata": {
        "id": "dWvgmcmsGc5j"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `__init__` method shows you the components\n",
        "- let's examine `self.transformer`"
      ],
      "metadata": {
        "id": "_6_JzIGHGlES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.distilbert.transformer.__init__??"
      ],
      "metadata": {
        "id": "R_H0dlu-G1p9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that it has a `layer` attribute which is the array of `TFTransformerBlock`\n",
        "\n",
        "Let's examine one block"
      ],
      "metadata": {
        "id": "7BQ4Qsm1G-SY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l = model.distilbert.transformer.layer[0]\n",
        "\n",
        "l.__init__??"
      ],
      "metadata": {
        "id": "fIEshWmxHHLu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You see that the Feed Forward Network at the `.ffn` attribute"
      ],
      "metadata": {
        "id": "yAhcFYvCIGgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l.ffn.__init__??"
      ],
      "metadata": {
        "id": "tXdWNJ3QIR8T"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will see two `Dense` layers at attributes `lin1` and `lin2`"
      ],
      "metadata": {
        "id": "eHBws45QIW3j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puvPuZNvAsDo",
        "outputId": "271663f0-689d-4c91-dba8-d11ab75544b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "print(\"Done\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Slideshow",
    "colab": {
      "name": "HF_quick_intro_to_models.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "HuggingFace",
      "language": "python",
      "name": "huggingface"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}