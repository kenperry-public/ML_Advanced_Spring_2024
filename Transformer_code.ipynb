{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [TensorFlow Transformer Tutorial](https://www.tensorflow.org/text/tutorials/transformer)\n",
    "\n",
    "We will take a look at the actual code of a Transformer.\n",
    "\n",
    "There are many pieces, which we will examine individually.\n",
    "\n",
    "We will proceed starting with a high level view and descend to a lower level\n",
    "- means reading the code from bottom to top\n",
    "\n",
    "There are many subtle points which we will highlight with the tag <font color=red><strong>SUBTLETY</strong></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One of the key components of a Transformer is the Attention mechanism.\n",
    "\n",
    "In the code we examine, the base Attention class is via a `MultiHeadAttention` layer type\n",
    "- we will study this layer separately\n",
    "- so as not to distract from the other details of the Transformer architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [The Model: Transformer](https://www.tensorflow.org/text/tutorials/transformer#the_transformer)\n",
    "\n",
    "- The Transformer is a Model: a subclass of `tf.keras.Model`\n",
    "- The initializer creates\n",
    "    - An Encoder\n",
    "    - A Decoder\n",
    "    - a `final_layer` which converts the vector at each position into logits over the distribution of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "        class Transformer(tf.keras.Model):\n",
    "          def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
    "                       input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
    "            super().__init__()\n",
    "            self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                                   num_heads=num_heads, dff=dff,\n",
    "                                   vocab_size=input_vocab_size,\n",
    "                                   dropout_rate=dropout_rate)\n",
    "\n",
    "            self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                                   num_heads=num_heads, dff=dff,\n",
    "                                   vocab_size=target_vocab_size,\n",
    "                                   dropout_rate=dropout_rate)\n",
    "\n",
    "            self.final_layer = tf.keras.layers.Dense(target_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The model overrides the `call` method\n",
    "- defines what happens when we pass an input to the Transformer\n",
    "- passes the `context`input to the Encoder\n",
    "- the Encoder output is passed to the Decoder\n",
    "- the Decoder output (logits) is passed through a layer to produce a logit (at each position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "          def call(self, inputs):\n",
    "            # To use a Keras model with `.fit` you must pass all your inputs in the\n",
    "            # first argument.\n",
    "            context, x  = inputs\n",
    "\n",
    "            context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
    "\n",
    "            x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
    "\n",
    "            # Final linear layer output.\n",
    "            logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "            try:\n",
    "              # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "              # b/250038731\n",
    "              del logits._keras_mask\n",
    "            except AttributeError:\n",
    "              pass\n",
    "\n",
    "            # Return the final output and the attention weights.\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [The Encoder](https://www.tensorflow.org/text/tutorials/transformer#the_encoder_layer)\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/Encoder.png\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Confusion warning**\n",
    "\n",
    "The `Encoder` object is the *stack* of encoder blocks (which are called `EncoderLayer`'s)\n",
    "\n",
    "The `Encoder` is a Layer: sub-class of `tf.keras.layers.Layer`)\n",
    "\n",
    "- The initializer creates the sub-components of the Encoder\n",
    "    - Positional Embedding\n",
    "    - A sub-component (confusingly named `EncoderLayer`) which is an **array** of blocks whose elements are objects containing\n",
    "        - Self-Attention\n",
    "        - Feed-forward network\n",
    "    - This array (of length `num_layers`) is the *stack* of blocks\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "        class Encoder(tf.keras.layers.Layer):\n",
    "          def __init__(self, *, num_layers, d_model, num_heads,\n",
    "                       dff, vocab_size, dropout_rate=0.1):\n",
    "            super().__init__()\n",
    "\n",
    "            self.d_model = d_model\n",
    "            self.num_layers = num_layers\n",
    "\n",
    "            self.pos_embedding = PositionalEmbedding(\n",
    "                vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "            self.enc_layers = [\n",
    "                EncoderLayer(d_model=d_model,\n",
    "                             num_heads=num_heads,\n",
    "                             dff=dff,\n",
    "                             dropout_rate=dropout_rate)\n",
    "                for _ in range(num_layers)]\n",
    "            self.dropout = tf.keras.layers.Dropout(dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `call` method defines how the layer behaves when presented with input\n",
    "- calls the Positional Embedding on the Encoder input\n",
    "- passes the result to the stacked  `EncoderLayer`'s\n",
    "    - Self-Attention followed by Feed Forward\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "          def call(self, x):\n",
    "            # `x` is token-IDs shape: (batch, seq_len)\n",
    "            x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "            # Add dropout.\n",
    "            x = self.dropout(x)\n",
    "\n",
    "            for i in range(self.num_layers):\n",
    "              x = self.enc_layers[i](x)\n",
    "\n",
    "            return x  # Shape `(batch_size, seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `EncoderLayer`\n",
    "- initializer creates sub-components\n",
    "- the `call` method is over-ridden to pass inputs through the sub-components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "        class EncoderLayer(tf.keras.layers.Layer):\n",
    "          def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "            super().__init__()\n",
    "\n",
    "            self.self_attention = GlobalSelfAttention(\n",
    "                num_heads=num_heads,\n",
    "                key_dim=d_model,\n",
    "                dropout=dropout_rate)\n",
    "\n",
    "            self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "          def call(self, x):\n",
    "            x = self.self_attention(x)\n",
    "            x = self.ffn(x)\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [The Decoder](https://www.tensorflow.org/text/tutorials/transformer#the_decoder)\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/Decoder.png\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Confusion warning**\n",
    "\n",
    "The `Decoder` object is the *stack* of decoder blocks (which are called `DecoderLayer`'s)\n",
    "\n",
    "The `Decoder` is a Layer: sub-class of `tf.keras.layers.Layer`)\n",
    "- The initializer creates the sub-components of the Decoder\n",
    "    - Positional Embedding\n",
    "    - A sub-component (confusingly named `DecoderLayer`) which is an **array** of blocks whose elements are objects containing\n",
    "        - Self-Attention\n",
    "        - Cross-Attention\n",
    "        - Feed-forward network\n",
    "    - This array (of length `num_layers`) is the *stack* of blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "        class Decoder(tf.keras.layers.Layer):\n",
    "          def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "                       dropout_rate=0.1):\n",
    "            super(Decoder, self).__init__()\n",
    "\n",
    "            self.d_model = d_model\n",
    "            self.num_layers = num_layers\n",
    "\n",
    "            self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                                     d_model=d_model)\n",
    "            self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "            self.dec_layers = [\n",
    "                DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                             dff=dff, dropout_rate=dropout_rate)\n",
    "                for _ in range(num_layers)]\n",
    "\n",
    "            self.last_attn_scores = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `call` method defines how the layer behaves when presented with input\n",
    "- calls the Positional Embedding on the Decoder input\n",
    "- passes the result to the stacked  `DecoderLayer`'s\n",
    "    - *Causal* Self-Attention followed by \n",
    "    - Cross-Attention followed by\n",
    "    Feed Forward\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "          def call(self, x, context):\n",
    "            # `x` is token-IDs shape (batch, target_seq_len)\n",
    "            x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "            x = self.dropout(x)\n",
    "\n",
    "            for i in range(self.num_layers):\n",
    "              x  = self.dec_layers[i](x, context)\n",
    "\n",
    "            self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "            # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `DecoderLayer`\n",
    "- initializer creates sub-components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "        class DecoderLayer(tf.keras.layers.Layer):\n",
    "          def __init__(self,\n",
    "                       *,\n",
    "                       d_model,\n",
    "                       num_heads,\n",
    "                       dff,\n",
    "                       dropout_rate=0.1):\n",
    "            super(DecoderLayer, self).__init__()\n",
    "\n",
    "            self.causal_self_attention = CausalSelfAttention(\n",
    "                num_heads=num_heads,\n",
    "                key_dim=d_model,\n",
    "                dropout=dropout_rate)\n",
    "\n",
    "            self.cross_attention = CrossAttention(\n",
    "                num_heads=num_heads,\n",
    "                key_dim=d_model,\n",
    "                dropout=dropout_rate)\n",
    "\n",
    "            self.ffn = FeedForward(d_model, dff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `call` method is over-ridden to pass inputs through the sub-components\n",
    "\n",
    "          def call(self, x, context):\n",
    "            x = self.causal_self_attention(x=x)\n",
    "            x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "            # Cache the last attention scores for plotting later\n",
    "            self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "            x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "            return x\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us focus on the two forms of Attention\n",
    "- `context` is the Encoder output\n",
    "- `x` is the Decoder input\n",
    "\n",
    "The *Causal* Self-Attention uses query `x` (at each position) to attend to entire sequence `x`.\n",
    "- the attention is *causual*: for each position, future positions *may not* be attended to\n",
    "\n",
    "The Cross-Attention uses query `x` (at each position) to attend to Encoder output `context`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Add and Normalize](https://www.tensorflow.org/text/tutorials/transformer#add_and_normalize)\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/Add+Norm.png\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Review: Layer Normalization**\n",
    "\n",
    "- The variance of outputs tends to grow from layer to layer\n",
    "- Large variance causes gradient updates to become unstable\n",
    "- [Layer Normalization](https://proceedings.neurips.cc/paper_files/paper/2019/file/2f4fe03d77724a7217006e5d16728874-Paper.pdf) reduces the variance of the input distribution to unit variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<font color=red><strong>SUBTLETY</strong></font>\n",
    "\n",
    "The output of Attention layers (both Self Attention and Cross Attention) are feed into an `Add & Norm` block.\n",
    "\n",
    "In what seems to be a \"coding convenience\"\n",
    "- the code creates a common base class `BaseAttention(for both Self Attention and Cross Attention\n",
    "- which facilitates the processing of Attention output through an `Add & Norm` block.\n",
    "\n",
    "This is much more subtle than \"coding convenience\" !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The initializer creates sub-components\n",
    "- Attention\n",
    "- Layer Normalization\n",
    "- Add\n",
    "\n",
    "\n",
    "        class BaseAttention(tf.keras.layers.Layer):\n",
    "          def __init__(self, **kwargs):\n",
    "            super().__init__()\n",
    "            self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "            self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "            self.add = tf.keras.layers.Add()\n",
    "   \n",
    "**but** doesn't actually perform the normalization or addition.\n",
    "- there is no `call` method of the base class\n",
    "- these are left to the child (Attention) classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before we examine the child classes, let's examine the **purpose** of the `Add & Norm` block.\n",
    "\n",
    "The \"obvious\" purpose is to normalize the Attention outputs\n",
    "- using a `tf.keras.layers.LayerNormalization` layer\n",
    "- that is the `Norm` part of `Add & Norm`\n",
    "\n",
    "It is *easy to miss* the role of the `Add` part.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Mechanically: the `Add` is uninteresting.\n",
    "\n",
    "The `Add` part adds the block's two inputs (i.e, Attention input and Attention output)\n",
    "- before Normalization\n",
    "- In both the Self-Attention and Cross Attention children, the `call` method performs the `Add` and `Norm`\n",
    "via statements\n",
    "\n",
    "\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    \n",
    "- where `x` is the Attention input and `attn_output` is the Attention output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But what is the **purpose** of adding Attention input and Attention output ?\n",
    "\n",
    "This creates a *residual* or *skip* connection\n",
    "- on the forward pass, the input to Attention can \"skip over\" the Attention block\n",
    "- more importantly: on the backward pass: the loss gradient can skip over the Attention block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**[Review: Residual connections](RNN_Residual_Networks.ipynb#Residual-connections:-a-gradient-highway)**\n",
    "- Gradients can vanish or explode as they traverse an increasing number of layers during back propagation\n",
    "- A zero gradient causes the Gradient update step to leave weights unchanged\n",
    "    - the model can't \"learn\"\n",
    "- The skip connection prevents gradients from vanishing or exploding by allowing them to by-pass one or more layers in the backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So `Add & Norm` is much more than \"good coding\"\n",
    "- observing that Attention outputs are always fed into common blocks\n",
    "\n",
    "It is also the mechanism by which the residual connections are implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention\n",
    "\n",
    "The Self Attention (the class is called `GlobalSelfAttention`)  and Cross Attention blocks\n",
    "are both derived from `BaseAttention` \n",
    "- which we explained in the section on \"Add and Norm\".\n",
    "\n",
    "The sub-components (including the class `MultiHeadAttention` that implements Attention) are created\n",
    "by the parent class.\n",
    "\n",
    "The child classes mainly implement the `call` method\n",
    "- that invokes the sub-components in sequence\n",
    "- and implement the residual connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For Self-Attention, the `call` is\n",
    "\n",
    "\n",
    "        class GlobalSelfAttention(BaseAttention):\n",
    "                  def call(self, x):\n",
    "                    attn_output = self.mha(\n",
    "                        query=x,\n",
    "                        value=x,\n",
    "                        key=x)\n",
    "                    x = self.add([x, attn_output])\n",
    "                    x = self.layernorm(x)\n",
    "                    return x\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For Cross Attention, the `call` is\n",
    "\n",
    "        class CrossAttention(BaseAttention):\n",
    "          def call(self, x, context):\n",
    "            attn_output, attn_scores = self.mha(\n",
    "                query=x,\n",
    "                key=context,\n",
    "                value=context,\n",
    "                return_attention_scores=True)\n",
    "\n",
    "            # Cache the attention scores for plotting later.\n",
    "            self.last_attn_scores = attn_scores\n",
    "\n",
    "            x = self.add([x, attn_output])\n",
    "            x = self.layernorm(x)\n",
    "\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Feed forward](https://www.tensorflow.org/text/tutorials/transformer#the_feed_forward_network)\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/FeedForward.png\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The purpose of the Feed Forward block \n",
    "- is to transform the Decoder Cross Attention output at each position into a \"prediction\"\n",
    "    - of the next token (that is the Language Model objective\n",
    "\n",
    "The typical Feed Forward network is two `Dense` layers\n",
    "- the first has $d_\\text{ff}$ units\n",
    "    - creating $d_\\text{ff}$  synthetic features from the $d_\\text{model}$ features of the Attention output\n",
    "- the second has $d_\\text{model}$ units\n",
    "    - re-sizing the output to the standard $d_\\text{model}$ output size of all blocks in a Transformer\n",
    "through two `Dense` layers.\n",
    "\n",
    "In the original paper\n",
    "$$\n",
    "d_\\text{ff} = 4 * d_\\text{model}\n",
    "$$\n",
    "and this seems to have become a common choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is the code:\n",
    "\n",
    "        class FeedForward(tf.keras.layers.Layer):\n",
    "          def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "            super().__init__()\n",
    "            self.seq = tf.keras.Sequential([\n",
    "              tf.keras.layers.Dense(dff, activation='relu'),\n",
    "              tf.keras.layers.Dense(d_model),\n",
    "              tf.keras.layers.Dropout(dropout_rate)\n",
    "            ])\n",
    "            self.add = tf.keras.layers.Add()\n",
    "            self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "          def call(self, x):\n",
    "            x = self.add([x, self.seq(x)])\n",
    "            x = self.layer_norm(x) \n",
    "            return x\n",
    "\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<font color=red><strong>SUBTLETY</strong></font>\n",
    "\n",
    "The Feed Forward output is passed to an `Add & Norm` block\n",
    "- which has **two inputs**\n",
    "    - Feed Forward output and Feed Forward input\n",
    "    - the Feed Forward input is a residual connection\n",
    "- similar to the residual connection we saw in the \"Add and Normalize\" section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The residual connection is implemented in the `call` via the statements\n",
    "\n",
    "\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x) \n",
    "        \n",
    "where \n",
    "- `x` is the input to the Feed Forward block \n",
    "- `self.seq(x)` is the output of the Feed Forward block\n",
    "    - the input passed through the two `Dense`layers, implemented as a `Sequential` model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Training](https://www.tensorflow.org/text/tutorials/transformer#training)\n",
    "\n",
    "### Teacher forcing\n",
    "\n",
    "<font color=red><strong>SUBTLETY</strong></font>\n",
    "    \n",
    "A Generative task (like the LLM objective) is exhibits Autoregressive behavior\n",
    "- the Decoder output $\\hat\\y_{(\\tt-1)}$ at position $(\\tt-1)$ is fed back as *input* for position $\\tt$.\n",
    "\n",
    "In the Transformer, the position $(\\tt-1)$ output is appended to all previous outputs.\n",
    "\n",
    "Thus, at *inference* time: the input for position $\\tt$ is $\\hat\\y_{([1:\\tt-1])}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But, this **exact** behavior is not conducive to learning.\n",
    "- Suppose $\\hat\\y_{(\\tt-1)}$ is incorrect and not equal to correct label $\\y_{(\\tt-1)}$\n",
    "- this error cascades into the prediction of all subsequent positions $\\hat\\y_{([\\tt:])}$\n",
    "\n",
    "So, during **training** time: the input for position $\\tt$ is $\\y_{([1:\\tt-1])}$\n",
    "- the *correct* sequence\n",
    "- rather than the *predicted* sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is called *Teacher Forcing* at training time\n",
    "- but *not* at inference time\n",
    "\n",
    "It's very easy to *not notice* Teacher Forcing when it occurs because it is subtle.\n",
    "\n",
    "Can you see where it occurs ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is in the *construction* of the Training examples\n",
    "- the input for position $\\tt$ are the features of example $\\tt$: $\\y_{([1:\\tt-1])}$\n",
    "    - *not* the Autoregressive constructed $\\hat\\y_{([\\tt:])}$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "      i  & \\x^\\ip  & \\y^\\ip \\\\\n",
    "      \\hline \\\\\n",
    "      1 & \\y_{(0) }  & \\y_{(1)} \\\\\n",
    "      2 & \\y_{([0:1]) }  & \\y_{(2)} \\\\\n",
    "      \\vdots \\\\\n",
    "      \\tt & \\y_{([1:\\tt-1]) }  & \\y_\\tp \\\\\n",
    "      \\vdots \\\\\n",
    "      T & \\y_{([1:T-1]) }  & \\y_{(T)} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "During training, each example trains for one \"step\"\n",
    "- so we don't see the effect of $\\hat\\y_{(\\tt-1)}$ being fed back to the input for the next step $\\tt$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Custom Learning Rate Schedule\n",
    "\n",
    "A custom learning rate schedule (subclassed from `tf.keras.optimizers.schedules.LearningRateSchedule`) is created\n",
    "- varies learning rate $\\alpha$ of Gradient update by epoch\n",
    "$$\n",
    "\\W_{(\\text{epoch}+1)}  = \\W_{(\\text{epoch})} -  \\alpha * \\frac{\\partial \\loss_{(\\text{epoch})}}{\\partial \\W_{(\\text{epoch})} }\n",
    "$$\n",
    "    - a warm-up period where $\\alpha$ increases\n",
    "    - a post-warm-up period where $\\alpha$ decays\n",
    "    \n",
    "<img src=\"https://www.tensorflow.org/static/text/tutorials/transformer_files/output_Xij3MwYVRAAS_1.png\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [Loss and metrics](https://www.tensorflow.org/text/tutorials/transformer#set_up_the_loss_and_metrics)\n",
    "\n",
    "Since the targets are Categorical values, Cross Entropy is used as a loss.\n",
    "\n",
    "**But:** the target is a sequence with *padding*\n",
    "- the padding should not figure into the Loss\n",
    "- so the loss is \"masked\" whenever the target `label` is a padding token (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarly the Accuracy metric is modified so that padding characters don't participate in the calculation.\n",
    "\n",
    "        def masked_loss(label, pred):\n",
    "          mask = label != 0\n",
    "          loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='none')\n",
    "          loss = loss_object(label, pred)\n",
    "\n",
    "          mask = tf.cast(mask, dtype=loss.dtype)\n",
    "          loss *= mask\n",
    "\n",
    "          loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "          return loss\n",
    "\n",
    "\n",
    "        def masked_accuracy(label, pred):\n",
    "          pred = tf.argmax(pred, axis=2)\n",
    "          label = tf.cast(label, pred.dtype)\n",
    "          match = label == pred\n",
    "\n",
    "          mask = label != 0\n",
    "\n",
    "          match = match & mask\n",
    "\n",
    "          match = tf.cast(match, dtype=tf.float32)\n",
    "          mask = tf.cast(mask, dtype=tf.float32)\n",
    "          return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where do all the weights come from ?\n",
    "\n",
    "Ignoring the weights associated with the various embeddings, the weights come from\n",
    "- Attention\n",
    "- Feed forward Network\n",
    "\n",
    "This is for *each* Transformer block\n",
    "- we will stack $n_\\text{layer}$ such blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For Attention, the weights/parameters are in the matrices $\\W_Q, \\W_K, \\W_V$ and $\\W_O$\n",
    "- all of size $\\OrderOf{d_\\text{model}^2}$, total:\n",
    "$$\n",
    "4 * \\OrderOf{d_\\text{model}^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the Feed forward network, there are two `Dense` layers\n",
    "- the first mapping attention output of size $d_\\text{model}$ to size $d_\\text{ff}$\n",
    "- the second mapping size $d_\\text{ff}$ to standard output size $d_\\text{model}$\n",
    "- total Feed forward weights are $2 * (d_\\text{model} * d_\\text{ff})$\n",
    "\n",
    "Using the standard \n",
    "$$\n",
    "d_\\text{ff} = 4 * d_\\text{model}\n",
    "$$\n",
    "total Feed forward weights per block\n",
    "$$\n",
    "2 * (d_\\text{model} * 4* d_\\text{model}) = 8* \\OrderOf{d_\\text{model}^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice\n",
    "- that $\\frac{1}{3}$ of the total weights\n",
    "- come from *linear* projections\n",
    "    - the matrices associated with Attention\n",
    "- rather than non-linearities\n",
    "    - confined to Feed forward network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus the total weights *per Transformer block is\n",
    "$$\n",
    "12 * \\OrderOf{d_\\text{model}^2}\n",
    "$$\n",
    "\n",
    "This gets multiplied by the number $n_\\text{layer}$ stacked blocks.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For GPT-3\n",
    "- $n_\\text{layer} = 96$\n",
    "- $d_\\text{model} = 12* 1024$\n",
    "\n",
    "Total Transformer (non-embedding) weights\n",
    "$$\n",
    "96 * 12 * (12*1024)^2 = 174 \\text{ billion}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Second example: Mini-GPT](https://keras.io/examples/generative/text_generation_with_miniature_gpt/)\n",
    "\n",
    "We will examine a notebook that builds a miniature version of GPT:\n",
    "[tutorial view](https://keras.io/examples/generative/text_generation_with_miniature_gpt/)\n",
    "- [Colab notebook](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/text_generation_with_miniature_gpt.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We first see a definition of the constants:\n",
    "    \n",
    "    vocab_size = 20000  # Only consider the top 20k words\n",
    "    maxlen = 80  # Max sequence size \n",
    "    embed_dim = 256  # Embedding size for each token\n",
    "    num_heads = 2  # Number of attention heads\n",
    "    feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
    "    \n",
    "Relating the variable names to our notation\n",
    "\n",
    "Notation | variable | value\n",
    ":---|:---|:---\n",
    "$d_\\text{model}$ | embed_dim | 256\n",
    "$T$              | max_len   | 80\n",
    "$n_\\text{heads}$  | num_heads | 2\n",
    "                  | vocab_size | 20,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And the Decoder model:\n",
    "    \n",
    "    def create_model():\n",
    "        inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "        embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        x = embedding_layer(inputs)\n",
    "        transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "        x = transformer_block(x)\n",
    "        outputs = layers.Dense(vocab_size)(x)\n",
    "        model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        model.compile(\n",
    "            \"adam\", loss=[loss_fn, None],\n",
    "        )  # No loss and optimization based on word embeddings from transformer block\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is the plot:\n",
    "\n",
    "<img src=\"images/model_text_generation_with_miniature_gpt.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Examining each layer\n",
    "- `Input`\n",
    "    - sequence (length $T = 80$) of integers (index of a character within vocabulary) $\\y_{(1:T)}$\n",
    "- `TokenAndPositionEmbedding`\n",
    "    - maps sequence (length $T = 80$) of integers (index of character)\n",
    "    - into sequence (length $T = 80$) of $d_\\text{model} = 256$ size representations\n",
    "- `TransformerBlock`\n",
    "    - maps sequence (length $T = 80$) into sequence of latents $\\h_{(1:T)}$\n",
    "        - one latent per position in input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- `Dense`\n",
    "    - Classifier layer\n",
    "    - maps sequence of latents\n",
    "    - to sequence of probability vectors\n",
    "        - each position is a probability vector of length `vocab_size` $= 20000$\n",
    "        - position $i$: probability that output is element $i$ of vocabulary\n",
    "        - sum across positions in each vector is 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss function\n",
    "\n",
    "The `create_model` method also defines the Loss Function\n",
    "\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    \n",
    "as Cross Entropy, as is common for a Classifier\n",
    "\n",
    "Notice that the `SparseCategoricalCrossentropy` takes a vector (of length `vocab_size`) of **logits** rather than **probabilities**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TransformerBlock\n",
    "\n",
    "Let's examine the [TransformerBlock](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/text_generation_with_miniature_gpt.ipynb#scrollTo=wWOa4u5u7Z-b) in more detail\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "    class TransformerBlock(layers.Layer):\n",
    "        def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "            super().__init__()\n",
    "            self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "            self.ffn = keras.Sequential(\n",
    "                [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "            )\n",
    "            self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.dropout1 = layers.Dropout(rate)\n",
    "            self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            input_shape = tf.shape(inputs)\n",
    "            batch_size = input_shape[0]\n",
    "            seq_len = input_shape[1]\n",
    "            causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "            attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "            attention_output = self.dropout1(attention_output)\n",
    "            out1 = self.layernorm1(inputs + attention_output)\n",
    "            ffn_output = self.ffn(out1)\n",
    "            ffn_output = self.dropout2(ffn_output)\n",
    "            return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can see that the TransformerBlock is implemented as a Layer (`layers.Layer`)\n",
    "- so it will translate its input into output via a `call` method\n",
    "\n",
    "The class `__init__` method defines the components of the Transformer\n",
    "- stores them in instance variables: \n",
    "    - Attention: `self.att`\n",
    "    - Feed Forward Network FFN: `self.ffn`\n",
    "    - Other: Layer Norms, Dropouts\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `call` method does the actual work\n",
    "-  Masked self-attention to $\\y_{(1:T)}$\n",
    "    - Creates casual mask `causal_mask` to prevent peeking ahead at not-yet-generated output\n",
    "        - `seq_len` is current length $\\tt$ of $\\y_{1:\\tt)}$\n",
    "    - Attention block `self.att` applied to causally-masked input\n",
    "    \n",
    "    `attention_output = self.att(inputs, inputs, attention_mask=causal_mask)`\n",
    "- Dropout `self.dropout1` and LayerNorm `layernorm1` applied to attention output\n",
    "- Result passed through Feed Forward Network `self.ffn`   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TokenAndPositionEmbedding\n",
    "\n",
    "Let's examine the [TokenAndPositionEmbedding](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/text_generation_with_miniature_gpt.ipynb#scrollTo=2roAYZPM7Z-c)\n",
    "\n",
    "    class TokenAndPositionEmbedding(layers.Layer):\n",
    "        def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "            super().__init__()\n",
    "            self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "            self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "        def call(self, x):\n",
    "            maxlen = tf.shape(x)[-1]\n",
    "            positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "            positions = self.pos_emb(positions)\n",
    "            x = self.token_emb(x)\n",
    "            return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can see that it too is implemented as a Layer.\n",
    "\n",
    "The `call` method\n",
    "- translates the input sequence\n",
    "    - each position in the sequence is an integer index within the vocabulary\n",
    "- into a sequence of pairs\n",
    "    - first element: token embedding\n",
    "    \n",
    "        `x = self.token_emb(x)`\n",
    "    \n",
    "    - second element: position embedding\n",
    "    ```\n",
    "    positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "    positions = self.pos_emb(positions)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As explained [in a prior module](Transformer_PositionalEmbedding.ipynb#Representing-the-combined-token-and-positional-encoding)\n",
    "- The output is not actually a sequence of *pairs*\n",
    "    - it is a sequence of numbers\n",
    "    - the token and positional emeddings are *added* not concatenated\n",
    "        - concatenation would double the length\n",
    "        - all layers in Transformer preserve output length equal input length = $d_\\text{model}$\n",
    "- See the module's explanation as to why addition works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dense (Feed Forward Network)\n",
    "\n",
    "We can see that the Feed Forward Network are two Dense layers\n",
    "\n",
    "    self.ffn = keras.Sequential(\n",
    "                [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "            )\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We may have been expecting the final layer of `TransformerBlock` to be outputting a probability vector (over the Vocabulary)\n",
    "- a vector of length `vocab_size`\n",
    "    - position $i$ is probability that output is element $i$ of the Vocabulary\n",
    "- using a `softmax` activation\n",
    "   - to make sure sum (across the `vocab_size` elements of the vector) of probabilities is `00%\n",
    "\n",
    "But we see that the output is\n",
    "- a singleton (not a vector)\n",
    "- of size equal to `embed_dim` = $d_\\text{model}$\n",
    "\n",
    "That is:\n",
    "- the `Dense` component of the `TransformerBlock` is outputing the embedding of $\\hat{\\y}_\\tp$ rather than a probability vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we will see\n",
    "- there is a layer in the Model *after* the `TransformerBlock`\n",
    "- that produces the probability vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skip connections\n",
    "\n",
    "Here is a more detailed view of the Transformer\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer (Encoder/Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Attention_is_all_u_need_Transformer.png\" width=40%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In particular, please focus on the arrows *into the \"Add & Norm\" layers*.\n",
    "\n",
    "These are *skip connections* that bypass the Attention layers.\n",
    "- *Residual Networks*\n",
    "\n",
    "Where is this reflected in the code ?\n",
    "\n",
    "It is a little subtle and easy to miss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With the `call` method of the `TransformerBlock` please notice the statement\n",
    "\n",
    "    out1 = self.layernorm1(inputs + attention_output)\n",
    "\n",
    "- `inputs` is the input to the Attention layer\n",
    "\n",
    "    attention_output = self.att(inputs, inputs, attention_mask=causal_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So the addition\n",
    "\n",
    "    inputs + attention_output\n",
    "    \n",
    "is joining (via addition)\n",
    "- the output of the Attetnion layer\n",
    "- the input of the Attention layer\n",
    "\n",
    "This is the skip connection !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similar code appears\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "where \n",
    "- the input to the FFN (i.e., `out1`)\n",
    "- is joined (via addition) to the output of the FFN (i.e., `ffn_output`)\n",
    "\n",
    "        out1 + ffn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model\n",
    "\n",
    "By examining the `create_model` function, we see that the output of the `TransformerBlock` \n",
    "- is fed into a `Dense` layer \n",
    "- which outputs a vector of length `vocab_size` (the correct length of a probability vector)\n",
    "- and the output of this `Dense` layer is the output of the **model**\n",
    "    - not the output of the `TransformerBlock`\n",
    "   ``` \n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "    ```\n",
    "- Technically: the output vector is of *un-normalized logits* rather than probabilities\n",
    "    - the logit vector can be turned into a probability vector via a `softmax`\n",
    "    -\n",
    "Thus, the Model outputs a vector of logits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can see how a token is sampled\n",
    "- by converting the logit vector into a probability vector\n",
    "- with the `sample_from` method of the `TextGenerator` callback\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "            logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "            indices = np.asarray(indices).astype(\"int32\")\n",
    "            preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "            preds = np.asarray(preds).astype(\"float32\")\n",
    "            return np.random.choice(indices, p=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Rather than outputting a probability vector\n",
    "- which would require the user choosing one element from the vector (a word in the vocabulary)\n",
    "- what is output is the *embedding* of the chosen word in the vocabulary\n",
    "\n",
    "Since this output is compared against the correct label (i.e, $\\y_{(\\tt+1)}$ for position $\\tt$)\n",
    "- we should also see that the *labels* used are embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "A [`TextGenerator`](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/text_generation_with_miniature_gpt.ipynb#scrollTo=eK_xkEAw7Z-f) call-back is used during training\n",
    "- at the end every `self.print_every` epochs\n",
    "- a sample of $\\hat{\\y}_{(1:T)}$ will be drawn\n",
    "- to illustrate what the model output would be up to that point in training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The heart of the call-back\n",
    "\n",
    "    while num_tokens_generated <= self.max_tokens:\n",
    "        ...\n",
    "        y, _ = self.model.predict(x)\n",
    "        sample_token = self.sample_from(y[0][sample_index])\n",
    "        ...\n",
    "        \n",
    "- is a loop over positions $\\tt$\n",
    "- that extends a fixed input (prefix of text) `start_tokens`\n",
    "- to full length $T$\n",
    "- by sampling a token from the output for position $\\tt$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is useful\n",
    "- to see whether our model is learning as epochs advance\n",
    "- to confirm the shape and type of the model output is a vector of logits\n",
    "    - the model output for position $\\tt$:  `y, _ = self.model.predict(x)`\n",
    "    - is passed to `sample_from`\n",
    "    - which samples from the probability distribution derived from the logits (model output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
