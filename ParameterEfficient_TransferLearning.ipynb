{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "The goal of Transfer Learning is to adapt a Pre-Trained model for a Source task\n",
    "(the \"base\" model) to solve a new Target task.\n",
    "\n",
    "Adapting a base model is typically performed by Fine-Tuning\n",
    "- allowing the weights of the base model (and any additional \"head\") layers to adapt\n",
    "- by training with a relatively small number of examples from the Target task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Although Fine-Tuning is effective, there is a problem, especially with LLM base models\n",
    "- LLM models can have a very large number $N$ of parameters\n",
    "- They are increasingly deep: number of stacked Transformer blocks $n_\\text{layers}$ is growing\n",
    "    - latency in training\n",
    "    \n",
    "Even training on a small number of Target task examples is expensive in time and memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The question we address in this module\n",
    "-  Can we adapt a base model *without* modifying *all* of the\n",
    "parameters of the base model ?\n",
    "\n",
    "We will refer to this problem as *Parameter Efficient Transfer Learning* \n",
    "- or *Parameter Efficient Fine-Tuning* when Fine-Tuning is used as the method for adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We want the number of *adapted* parameters to be small relative to the total number of\n",
    "base model parameters.\n",
    "\n",
    "We will use this fraction as a metric in comparing adaptation methods.\n",
    "\n",
    "We note that the number of parameters in a Transformer is $N = \\OrderOf{ n_\\text{layers} * d^2}$\n",
    "- where $d$ is the internal dimension of the Transformer\n",
    "- calculations may be found in [our notebook](Transformer.ipynb#Number-of-parameters) and [here](https://arxiv.org/pdf/2001.08361.pdf#page=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivation for Parameter Efficient Transfer Learning\n",
    "\n",
    "A base model may have a large number of parameters (e.g., an LLM)\n",
    "- Adapting *all* the parameters may require large quantities of time and space\n",
    "- Reducing the number of adapted parameters may have efficiency advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Beyond the obvious efficiency advantage\n",
    "- there is a space advantage\n",
    "- the specialization of the Base Model to a Target Task can be represented by\n",
    "the small number of adapted parameters\n",
    "\n",
    "This means that the parameters of the same base model can be *shared*\n",
    "- across models for different Target tasks\n",
    "- with one set of separate (but small) adapted parameters for each Target\n",
    "\n",
    "This is also potentially a way to enable per-user instances of a Target task\n",
    "- with user-specific training examples kept private to each user's instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adapters\n",
    "\n",
    "**References**\n",
    "\n",
    "- [Parameter Efficient Transfer Learning for NLP](https://arxiv.org/pdf/1902.00751.pdf)\n",
    "- [LLM Adapters](https://arxiv.org/pdf/2304.01933.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Adapters are modules (implemented as Neural Networks)\n",
    "- that are inserted into the existing modules (layers) of the base model.\n",
    "\n",
    "In the general case: \n",
    "- we can insert one or more adapters *anywhere* within the NN comprising the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Within a *single* Transformer block, typical arrangements are\n",
    "\n",
    "- Series\n",
    "    - Adapter inserted between modules\n",
    "- Parallel\n",
    "    - Adapter inserted parallel to a module\n",
    "        - provided an alternate path *by-passing* the module\n",
    "\n",
    "<table>\n",
    "    <center><strong>Various Adapter designs</strong></center>\n",
    "    <img src=\"images/LLM_adapters.png\" width=70%>\n",
    "    <br>\n",
    "    Attribution: https://arxiv.org/pdf/2304.01933.pdf#page=2\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a diagram of a common adapter\n",
    "<br><br>\n",
    "<table>\n",
    "    <center><strong>Adapter</strong></center>\n",
    "    <img src=\"images/Adapter_diag.png\" width=50%>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The dimensions of the input and output of the adapter\n",
    "- are the same $d$ (common vector dimension) used for all layers in a Transformer\n",
    "- facilitates inserting adapters anywhere in the Transformer\n",
    "\n",
    "The usual architecture\n",
    "- usually two modules, with a bottleneck of dimension $a \\lt d$\n",
    "    - Project down to reduced dimension; Project up to original dimension\n",
    "- skip connection around the two projection modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We are already familiar with adaptation via Adapter-like modules\n",
    "- adding a new \"head\" layer to a head-less base model\n",
    "    - often a Classifier to adapt the base model to the particular Target classes\n",
    "- [Feature based transfer learning](NLP_Language_Models.ipynb#Other-uses-of-a-Language-Model:-Feature-based-Transfer-Learning) \n",
    "    - feeding the representation created by the base model to another module.\n",
    "- these are not technically adapters\n",
    "    - input and output dimensions don't match\n",
    "    - architecture may differ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Regardless of where Adapters are placed\n",
    "- they derive a new function $g$ from the function $f$ computed by the base model\n",
    "\n",
    "Formally:\n",
    "- $f_\\Theta$ denotes the function computed by the base model which is parameterized by $\\Theta$ \n",
    "- $g_{\\Theta, \\Phi}(\\x)$ denotes the function computed by the adapted model\n",
    "    - $\\Phi$ are the Adapter parameters\n",
    "    - $\\Theta$ are the base model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Adapter Tuning* occurs when we train only the parameters $\\Phi$ of the Adapter modules\n",
    "- on a small number of examples from the Target task\n",
    "- freezing the parameters of the base model\n",
    "\n",
    "During epoch $\\tt$ of Adapter Tuning, we learn $\\Phi_\\tp$\n",
    "- initialing $\\Phi_{(0)}$ such that\n",
    "$$g_{\\Theta, \\Phi_{(0)}}(\\x) \\approx f_\\Theta(\\x)$$\n",
    "- can be achieved by setting $\\Phi = 0$\n",
    "    - because of the skip connection, the adapter output becomes $f_\\Theta(\\x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bottleneck size\n",
    "\n",
    "Since Adapter Tuning does not change base model parameters $\\Theta$,\n",
    "- the space used depends on the size of $\\Phi$\n",
    "- this is the key to adapting the base model using a small number of parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The number of parameters of the projection components of the Adapter are $\\OrderOf{ d*a }$, multiplied by the number $k$ of Adapters.\n",
    "\n",
    "Recall that a  number of parameters in a Transformer are $\\OrderOf{n_\\text{layers} * d^2}$.\n",
    "\n",
    "Expressing the size of $\\Phi$ as a fraction of the size of $\\Theta$:\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "r & = & \\frac{|\\Phi|}{|\\Theta|} \\\\\n",
    "  & \\approx & \\frac{d * a * n_\\text{layers}}{ n_\\text{layers} * d^2 } & \\text{since} \\\\\n",
    "  & &                    | \\Phi | = \\OrderOf{ d*a * n_\\text{layers}} \\text{ assuming } k = n_\\text{layers}\\\\\n",
    "  & &                   | \\Theta | = \\OrderOf{n_\\text{layers} * d^2} \\text{ for a Transformer} \\\\\n",
    "& \\approx & \\frac{ a  }{ d } \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For reference, $d = 12,288$ for GPT-3; $a$ is chosen to satisfy a target for $r$\n",
    "- e.g., $r = 0.1 \\%$, results in bottleneck size $a = 12$\n",
    "\n",
    "In [experiments](https://arxiv.org/pdf/1902.00751.pdf#page=4), the the botttleck was varied\n",
    "$$\n",
    "a \\in \\{ 2, 4, 8, 16, 32, 64 \\} \n",
    "$$\n",
    "so typical $a$ is a fraction of $1 \\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[The effect of varying $a$](https://arxiv.org/pdf/1902.00751.pdf#page=7) are shown in the orange line\n",
    "in the diagram below\n",
    "- the horizontal axis is the total number of trainable parameters, which is linear in $a$\n",
    "- it seems to show that increasing the size of the bottleneck does not impact performance greatly\n",
    "\n",
    "The table also compares adaptation via Adapters to adaptation by Fine-Tuning only the top layers of the base model\n",
    "- the total number of trainable parameters increases with the number of top layers fine-tuned\n",
    "- the results show that adaptation via Adapters is better than Fine Tuning top layers\n",
    "    - *unless* we Fine-Tune *many* top layers\n",
    "    \n",
    "<table>\n",
    "    <center><strong>Adapter vs Fine Tuning</strong></center>\n",
    "    <img src=\"images/Adapter_size_vs_FineTuning.png\" width=70%>\n",
    "    <br>\n",
    "    Attribution: https://arxiv.org/pdf/1902.00751.pdf#page=7\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adapter placement\n",
    "\n",
    "Recall that Transformer blocks are usually stacked into $n_\\text{layers}$ in a Transformer for an LLM.\n",
    "\n",
    "Initially, Adapters were placed at *each* level of the stack.\n",
    "\n",
    "However, [experiments](https://arxiv.org/pdf/1902.00751.pdf#page=8)  show that the most impactful adapters are located at the *top* of the stack.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the study, adapters are *removed* within a span of levels of the stacked blocks.\n",
    "- the models are **not re-trained** after removing the adapters\n",
    "\n",
    "The horizontal/vertical axes indexes the *end/start* of the span.\n",
    "\n",
    "Columns $7$ and beyond indicates the removing adapters does not decrease performance\n",
    "- until the adapter at level 7 is removed\n",
    "\n",
    "The last column indicates that the largest performance decrease occurs\n",
    "- when removing the single adapater at the top level\n",
    "\n",
    "<table>\n",
    "    <center><strong>Adapter placement</strong></center>\n",
    "    <img src=\"images/Adapter_layer_ablation.png\" width=110%>\n",
    "    <br>\n",
    "    Attribution: https://arxiv.org/pdf/1902.00751.pdf#page=8\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is interesting\n",
    "- Recall, our hypothesis of Deep learning is that increasing levels of abstraction of the inputs\n",
    "are created as layers become deeper\n",
    "- The early layers create representations that transfer across most tasks\n",
    "- The deepest layer representations are most task-specific\n",
    "\n",
    "The decrease in performance corresponding to deeper layers \n",
    "- may indicate that the Target task specific adaptation\n",
    "- occurs in the region which we associate most with the Source task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LoRA\n",
    "\n",
    "**References**\n",
    "- [LoRA:Low Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "\n",
    "**Additional reading**\n",
    "- [Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning](https://arxiv.org/abs/2012.13255)\n",
    "\n",
    "**Videos**\n",
    "\n",
    "Code is PyTorch but *idea* is portable to Keras.\n",
    "\n",
    "- [video: paper](https://www.youtube.com/watch?v=dA-NhCtrrVE)\n",
    "- [video: code](https://www.youtube.com/watch?v=iYr1xZn26R8)\n",
    "    - [colab](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbkl5aUlSeC1tQ3JmaEUtRTAwTmhCWkMyYTJkUXxBQ3Jtc0tuUUZhVG5CZHd1dF90SWJPRkhqdTFVOWVXSlVnWXYxSmV0c19OWmdLQUdFcEtSeHo2aUJiOGR5bGV3MEVETEhwYWx2WlN5SU1qMWtoMU15OEhwUVk2bHBDSGx3X1pNemwzU3pHMll3aXUxQmhIWkNaMA&q=https%3A%2F%2Fcolab.research.google.com%2Fdrive%2F1iERDk94Jp0UErsPf7vXyPKeiM4ZJUQ-a%3Fusp%3Dsharing&v=iYr1xZn26R8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Adapter method of Fine Tuning uses a module involving\n",
    "- Down projecting to a lower dimension\n",
    "- Up projecting back to the original dimension\n",
    "- with an intervening non-linearity\n",
    "- where the projections are achieved via Dense layers\n",
    "\n",
    "We now show the Low Rank Adaptation (LoRA) method that is similar\n",
    "- Down and Up Projections\n",
    "- without an intervening non-linearity\n",
    "- where the projections are achieved via matrix multiplication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let $\\W$ denote the parameters of the Pre-Trained Model.\n",
    "\n",
    "Fine-Tuning updates the parameters to\n",
    "$$\n",
    "\\W' = \\W + \\Delta \\W\n",
    "$$\n",
    "\n",
    "The usual method is to use Gradient Descent to create a sequence of parameter updates\n",
    "- one per mini-batch\n",
    "- equal to negative one times the learning-rate scaled  gradient of the Loss\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\W_{(0)} & = & \\W \\\\\n",
    "\\text{update}_\\tt & = & - \\alpha_\\tt  \n",
    "* \\frac{\\partial \\loss_{\\W_{(\\tt-1)} }}{\\partial \\W_{(\\tt-1)}} \\\\\n",
    "\\W_\\tp & = & \\W_{(\\tt-1)} +  \\text{update}_\\tt \\\\\n",
    "\\\\\n",
    "\\Delta \\W = \\sum_{\\tt} \\text{update}_\\tt\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "LoRA uses a different method \n",
    "- using Gradient Descent to approximate the *cumulative* change $\\Delta \\W$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Illustrating LoRA on the embedding matrices of an Attention Layer\n",
    "\n",
    "Although the method works on all types of layers, it is easiest to illustrate in a very particular \n",
    "sub-component of an Attention layer.\n",
    "\n",
    "This is a component that \n",
    "- implements the multiplication of\n",
    "- vector $\\x$ of dimension $d = d_\\text{model}$\n",
    "- by a matrix $\\W$  of dimensions $(d \\times d)$\n",
    "- where $\\W$ are updatable *parameters* of the model\n",
    "$$\n",
    "h = \\W * \\x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This component appears multiple times in an Attention layer.\n",
    "\n",
    "Recall that an Attention layer matches query $q$ against each key $k$ of key/value pair $(k,v)$\n",
    "in a Soft Lookup, producing an output $o$.\n",
    "\n",
    "But each of $q, k, v, o$ can be projected/embedded by matrices\n",
    "[query, key, values](Attention_Lookup.ipynb#Projecting-queries,-keys-and-values), and [output](Attention_Lookup.ipynb#Projecting-the-lookup-result) respectively\n",
    "\n",
    "$$\\begin{array} \\\\\n",
    "q &  = \\W_Q * q \\\\\n",
    "k &  = \\W_K * k\\\\\n",
    "v &  = \\W_V * v\\\\\n",
    "o &  = \\W_O * o\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Each of these projections is an instance of the operation that we are illustrating.\n",
    "\n",
    "We will use $\\W$ to denote the $(d \\times d)$ matrix and $\\x$ to denote the value being embedded.\n",
    "\n",
    "- i.e., $\\W$ will be one of $ \\{ \\W_Q, \\W_K, \\W_V, \\W_O \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Aside**\n",
    "\n",
    "Here are the equations for Multi Head Attention, for reference\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\text{MultiHead}(QK,V) &  = & \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_{n_\\text{head}}) \\; \\W_O \\\\\n",
    "\\text{head}_j & = & \\text{Attention}( Q * \\W_Q^{(j)}, K * \\W_K^{(j)}, V * \\W_V^{(j)}) \\\\\n",
    "\\text{Attention}(Q,K,V) & = & \\text{softmax} \\left(\n",
    "\\frac{ Q * K^ T }{ \\sqrt{d} } \\right) V \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Computing $\\Delta \\W$\n",
    "\n",
    "The Pre-Trained model has $\\W$ equal to an initial value\n",
    "$$\n",
    "\\W = \\W_0\n",
    "$$\n",
    "\n",
    "After Fine-Tuning, $\\W$ becomes\n",
    "$$\n",
    "\\W' = \\W_0 + \\Delta \\W\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "LoRA does not **learn** $\\Delta \\W$ directly.\n",
    "\n",
    "Instead, it creates two learnable parameter matrices $A, B$:\n",
    "\n",
    "out  &nbsp;  &nbsp;  &nbsp;  &nbsp; | &nbsp; | down project &nbsp; &nbsp;  &nbsp;  &nbsp; | &nbsp;  | up project &nbsp;  &nbsp;  &nbsp;|\n",
    ":--:|:-:|:-:|:-:|:-:\n",
    "$\\Delta \\W$ | = | $A$| * |$B$ |\n",
    "$(d \\times d)$ | | $(d \\times r)$ | | $(r \\times d)$\n",
    "\n",
    "where $r \\le \\text{rank}(\\Delta \\W)$\n",
    "\n",
    "That is, it *factors* $\\Delta \\W$ into the product of two low rank matrices $A, B$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <center><strong>LoRA adapting Pre-Trained matrix W</strong></center>\n",
    "    <img src=\"images/LoRA_arch.png\" width=30%>\n",
    "    <br>\n",
    "    Attribution: https://arxiv.org/pdf/2106.09685.pdf#page=1\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This arrangement results in\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "h & = & \\W_0 * \\x  & \\text{the left branch} \\\\\n",
    "  &   & \\,\\, + \\, (A * B)  *\\x & \\text{the sum operator on top} \\\\\n",
    "& = &  \\W_0 * \\x + \\Delta \\W *  \\x & \\Delta \\W = A * b \\\\\n",
    "& = & (\\W_0 + \\Delta \\W) * \\x & \\text{distributive property} \\\\\n",
    "& = & \\W' * \\x & \\W' = \\W_0 + \\Delta \\W \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Thus, the output is $\\W' * \\x$, satisfying the goal of adapting $\\W$ to $\\W'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The resulting number of parameters\n",
    "- is $2 * d * r$ parameters\n",
    "- rather than $d^2$\n",
    "\n",
    "So, not only is the representation of $\\Delta \\W$ smaller, there are fewer parameters to Fine-Tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Matrix $B$ is initialized to $0$ so that\n",
    "- when Fine-Tuning begins\n",
    "- the initial $\\Delta \\W = A * B = 0$\n",
    "- $A, B$ get updated during Fine-Tuning\n",
    "    - by gradient descent on the elements of the matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note the similarity to the Adapter used in a Parallel arrangement.\n",
    "\n",
    "The advantage of the Parallel arrangement compared to a Series arrangement\n",
    "- the Series introduces an added layer\n",
    "- each time it appears\n",
    "- which slows *inference*\n",
    "\n",
    "The Parallel arrangement used in LoRA does not introduce latency at inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How big does $r$ have to be ?\n",
    "\n",
    "Not much ! Values of $r \\le 2$ seem to do very well in an experiment\n",
    "\n",
    "The accuracy reported when $r=2$ is almost the same as when $r = 64$\n",
    "<br>\n",
    "<table>\n",
    "    <center><strong>LoRA: accuracy versus rank $r$</strong></center>\n",
    "    <img src=\"images/LoRA_by_rank.png\">\n",
    "    <br>\n",
    "        Attribution: https://arxiv.org/pdf/2106.09685.pdf#page=10\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "How do the various adaptation methods compare according to the authors ?\n",
    "\n",
    "LoRa with 37.7MM parameters ($.02 \\%$ of GPT-3) *outperforms* full Fine-Tuning.\n",
    "\n",
    "<br>\n",
    "<table>\n",
    "    <center><strong>LoRA: Performance, by method of adaptation</strong></center>\n",
    "    <img src=\"images/LoRA_results.png\">\n",
    "    <br>\n",
    "        Attribution: https://arxiv.org/pdf/2106.09685.pdf#page=8\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Performance of various forms of adaptation\n",
    "\n",
    "The following table compares various forms of adaptation\n",
    "- Fine-tuning (model tuning)\n",
    "- Adapter\n",
    "- Prefix Tuning\n",
    "\n",
    "The number in parenthesis next to the name of the adaptation is\n",
    "- the size of the adapted parameters as a fraction of base model parameters.\n",
    "- note that for all metrics except TER, a bigger performance number is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can see that Prefix Tuning\n",
    "- using only a small number of adapted parameters ($0.1 \\%$ of base model parameters)\n",
    "- performs similarly *or better* than full Fine-Tuning for many tasks\n",
    "    - evaluated on base models which are the Medium and Large variants of GPT-2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <center><strong>Performance, by method of adaptation</strong></center>\n",
    "    <img src=\"images/PrefTuning_compare.png\">\n",
    "    <br>\n",
    "    <center>n.b., for the TER metric: smaller is better</center>\n",
    "    <br>\n",
    "    Attribution: https://arxiv.org/pdf/2101.00190.pdf#page=7\n",
    "    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prefix length\n",
    "\n",
    "How long does the prefix need to be ?\n",
    "- how many pseudo tokens in the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The results of several experiments show\n",
    "- a small number (10) of pseudo tokens achieves most of the performance\n",
    "- hence, the number of Target task specific parameters does not need to be large\n",
    "\n",
    "<table>\n",
    "    <center><strong>Effect of Prefix Length on Adaptation via Prefix Tuning</strong></center>\n",
    "    <img src=\"images/PrefTuning_length.png\" width=70%>\n",
    "    <br>\n",
    "    <center>n.b., for the TER metric: smaller is better</center>\n",
    "    <br>\n",
    "    Attribution: https://arxiv.org/pdf/2101.00190.pdf#page=8\n",
    "    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Performance as a function of base model size\n",
    "\n",
    "The general ordering of adapted models, from best to worst is\n",
    "- Fine-tuning (model tuning)\n",
    "- Prompt tuning\n",
    "- Prompt Design (Prompt Engineering)\n",
    "\n",
    "*However*: the gap between Model Tuning and Prompt Tuning *disappears* as we use larger base models.\n",
    "\n",
    "<table>\n",
    "    <center><strong>Adaptation by base model size</strong></center>\n",
    "    <img src=\"images/PEFT_Scale_compare_results.png\">\n",
    "    <br>\n",
    "    Attribution: https://arxiv.org/pdf/2104.08691.pdf#page=1\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# BitFit\n",
    "\n",
    "**References**\n",
    "- [BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/pdf/2106.10199.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The goal of this paper is\n",
    "- to select a *subset* of a model's existing parameters for fine-tuning\n",
    "    - contrast to other methods which *add* Neural Networks (with additional parameters)\n",
    "    - to existing architecture\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Surprisingly: just fine-tuning the *bias* terms (\"intercept\") works pretty well !\n",
    "\n",
    "To be specific: the bias parameters of Attention lookup layers are modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Recall 1**\n",
    "\n",
    "From the [Attention Lookup module](Attention_Lookup.ipynb#Projecting-queries,-keys-and-values)\n",
    "- Attention creates queries, keys, and values\n",
    "    - based on the sequences (states) produced by earlier layers of the Transformer\n",
    "- Rather than using the raw states of the Transformer\n",
    "as queries (resp., keys/values)\n",
    "- we can map them through projection/embedding *matrices* $\\W_Q, \\W_K, \\W_V$\n",
    "    - each mapping matrix shape is $(d \\times d)$\n",
    "    - thus, the mapping preserves the shapes of $Q, K, V$\n",
    "\n",
    "\n",
    "- Mapping through these matrices:\n",
    "\n",
    "out  &nbsp;  &nbsp;  &nbsp;  &nbsp; | &nbsp; | left &nbsp; &nbsp;  &nbsp;  &nbsp; | &nbsp;  | right &nbsp;  &nbsp;  &nbsp;|\n",
    ":--:|:-:|:-:|:-:|:-:\n",
    "$Q$ | = | $Q$| * |$\\W_Q$ |\n",
    "$(T \\times d)$ | | $(T \\times d)$ | | $(d \\times d)$\n",
    "&nbsp;\n",
    "$K$ | = | $K$| * |$\\W_K$ |\n",
    "$V$ | = | $V$| * |$\\W_V$ |\n",
    "$(\\bar T \\times d)$ | | $(\\bar T \\times d)$ | | $(d \\times d)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Recall 2**\n",
    "\n",
    "Our notational practice in dealing with the \"bias\" term\n",
    "- when computing a dot product $\\w \\cdot \\x$ we add\n",
    "    - a constant \"1\" as first element of $\\x$ (let's call the augmented vector $\\x'$)\n",
    "    - the bias parameter $b$ as the first element of $\\w$ (let's  call this $\\w'$)\n",
    "\n",
    "So\n",
    "$$\n",
    "\\w \\cdot \\x + b = \\w' \\cdot \\x'\n",
    "$$\n",
    "\n",
    "This paper\n",
    "- keeps $\\w$ frozen\n",
    "- modifies $b$\n",
    "\n",
    "where these terms are parts of  $\\W_Q, \\W_K, \\W_V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On small to medium fine-tuning datasets\n",
    "- performance comparable to fine-tuning *all* parameters\n",
    "\n",
    "on large fine-tuning datasets\n",
    "- performance comparable to other sparse methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion: Fine-Tuning is easy for everyone !\n",
    "\n",
    "Fine-Tuning a huge model like GPT-3 seemed out of the realm of possibility for individuals or small organizations.\n",
    "- huge memory requirements\n",
    "- time intensive\n",
    "    - even with the *much smaller* number of examples in the Fine-Tuning dataset compared to the Pre-Training datasets\n",
    "    \n",
    "Parameter Efficient Transfer learning shows\n",
    "- Fine-Tuning is now accessible on consumer grade hardware\n",
    "- Without negligible loss of performance (maybe even better) than full Fine-Tuning    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our module on [Transformer Scaling](Transformers_Scaling.ipynb)\n",
    "- highlighted a trend\n",
    "- to *smaller* Large Language Models\n",
    "- with performance matching very large models (like GPT-3).\n",
    "\n",
    "Combined with Parameter Efficient Fine-Tuning\n",
    "- it is [now possible to Fine-Tune a model (LLaMA 7B)](https://arxiv.org/pdf/2303.16199.pdf)\n",
    "- with performance equivalent to GPT-3 (175B parameters)\n",
    "- using 8 A100 GPU's\n",
    "- in one hour !\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
