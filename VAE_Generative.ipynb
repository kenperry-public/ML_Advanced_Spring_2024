{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variational Autoencoder (VAE)\n",
    "\n",
    "Like the plain Autoencoder that we have already encountered\n",
    "a *Variational Autoencoder (VAE)* is comprised of an Encoder and a Decoder\n",
    "\n",
    "In both cases\n",
    "- the Encoder produces a latent representation $\\z^\\ip$ of its input $\\x^\\ip$\n",
    "- the Decoder attempts to reconstruct $\\x^\\ip$ from $\\z^\\ip$\n",
    "\n",
    "However, the behavior of the Decoder is undefined when presented with a latent $\\z$ that did\n",
    "not arise from a training example.\n",
    "- We can only hope that the output is reasonable\n",
    "\n",
    "As we saw, this has implications as to our ability to use the AE as a means of generating synthetic examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Decoder part of a VAE is identical to that of the plain Autoencoder.\n",
    "\n",
    "But the Encoder part of a VAE is different in an important way.  Given input $\\x$\n",
    "- It creates a *distribution* for the the latent representation $\\z$\n",
    "- Rather than creating a unique $\\z$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Encoder part of a VAE, given input $\\x$\n",
    "- Produces the parameters (e.g., $\\mu^\\ip, \\sigma^\\ip$) of a distributional form\n",
    "- Draws a sample from the distribution as its output $\\z^\\ip$\n",
    "\n",
    "Thus, the latent representation $\\z$ of a given $\\x$ is a probability distribution $\\qr{\\z | \\x}$.\n",
    "\n",
    "This may address one of the concerns we raised with using a plain Autoencoder in a generative manner\n",
    "- that a slight perturbation of the latent $\\z^\\ip$ obtained from input $\\x^\\ip$\n",
    "- might have the Decoder produce $\\tilde{\\x}^\\ip$ that is not similar to $\\x^\\ip$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Variational Autoencoder (VAE)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_VAE.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Note**\n",
    "\n",
    "$\\mu^\\ip$ and $\\sigma^\\ip$ are \n",
    "- vectors\n",
    "- computed values (and hence, functions of $\\x^\\ip$) and **not** parameters\n",
    "- so training learns a *function* from $\\x^\\ip$ to $\\mu^\\ip$ and $\\sigma^\\ip$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is not just straightforward engineering.\n",
    "\n",
    "In fact: the architecture of the VAE was obtained from the math rather than vice-versa !\n",
    "\n",
    "We provide a brief overview of the mathematics.\n",
    "\n",
    "The interested reader is referred to a highly recommended [VAE tutorial](https://arxiv.org/pdf/1606.05908.pdf)\n",
    "for a detailed presentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Details\n",
    "\n",
    "**Notation summary**\n",
    "\n",
    "term &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| dimension &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | meaning \n",
    ":---|:---|:---\n",
    "$\\x$ | $n$ | Random variable for Input\n",
    "$\\tilde\\x$ | $n$ | Output: reconstructed $\\x$\n",
    "$\\z$ | $n' << n$ | Random variable for Latent representation\n",
    "$E$  | $\\mathbb{R}^n \\rightarrow \\mathbb{R}^{n'}$ | Encoder\n",
    "            | | $E(\\x) = \\z $\n",
    "$D$  | $\\mathbb{R}^{n'} \\rightarrow \\mathbb{R}^n$ | Decoder\n",
    "            | | $\\tilde\\x = D(\\z) $\n",
    "            | | $\\tilde\\x = D( E(\\x) )$\n",
    "            | | $\\tilde\\x \\approx \\x$\n",
    "$\\pr{\\x}$ | prob. distribution | *prior* distribution of Inputs\n",
    "          | | intractable.  Only have empirical.\n",
    "$\\qr{\\z}$ | prob. distribution | *prior* distribution of Latents\n",
    "$\\qr{\\z \\mid \\x}$ | prob. distribution| *posterior* marginal distribution of Latents given Input\n",
    "                  | | intractable\n",
    "$\\qrs{\\z \\mid \\x}{\\Phi}$ | Neural Network| NN to approximate $\\qr{\\z \\mid \\x}$ \n",
    "                         | | Encoder\n",
    "$\\prs{\\x \\mid \\z}{\\Theta}$ | Neural Network| NN to approximate $\\pr{\\x \\mid \\z}$ \n",
    "                  | | Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's pretend that we don't already know the architecture of a VAE\n",
    "- that the latent $\\z^\\ip$ is generated as a probability function of $\\mu^\\ip$ and $\\sigma^\\ip$ given input $\\x^\\ip$.\n",
    "\n",
    "Instead let\n",
    "- $\\x$ denote a random variable representing an Input\n",
    "    - the random variable is from the (unknown) distribution $\\pr{\\x}$\n",
    "- $\\z$ denote a random variable representing a Latent\n",
    "    - the random variable is from the (unknown) distribution $\\qr{\\z}$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Because $\\x$ and $\\z$ are related, there is also a joint distribution of $(\\x,\\z)$ from which we\n",
    "can define the marginal distributions\n",
    "- $\\qr{\\z | \\x}$: the marginal distribution of Latent, given an Input\n",
    "- $\\pr{\\x | \\z}$: the marginal distribution of Input, given a Latent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But there's a problem !\n",
    "- The distribution $\\pr{\\x}$ is *intractable*\n",
    "    - e.g., Who can say what the distribution of images is in the physical world ?\n",
    "    - At best: we have an empirical distribution (our training dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will side-step the intractability issues by defining Neural Networks to learn\n",
    "an approximation.\n",
    "- $\\qrs{\\z | \\x}{\\Phi}$: Neural Network, with weights $\\Phi$ to approximate $\\qr{\\z \\mid \\x}$\n",
    "    - The Encoder\n",
    "- $\\prs{\\x | \\z}{\\Theta}$:Neural Network, with weights $\\Theta$, to approximate $\\pr{\\x \\mid \\z}$\n",
    "    - The Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The mapping from Latent to reconstructed Input is not necessarily unique,\n",
    "thus we marginalize $\\x$ over $\\z$\n",
    "$$\n",
    "\\pr{\\x} = \\int_{\\z \\in \\qr{\\z}}{ \\pr{\\x | \\z} \\; \\qr{\\z} }\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some obvious concerns about the integral\n",
    "- It may be very expensive to draw many samples of $\\z$ from $\\qr{\\z}$ for each training example $\\x^\\ip$\n",
    "- There are many random choices of $\\z$ from $\\qr{\\z}$ which can't reconstruct $\\x^\\ip$\n",
    "    - i.e., $\\prs{\\x^\\ip | \\z'}{\\Theta} = 0$ for many $\\z'$\n",
    "\n",
    "\n",
    "We can improve our sampling by considering only those choices of $\\z$ that could generate $\\x$\n",
    "and re-write the objective as\n",
    "\n",
    "$$\n",
    "\\pr{\\x} = \\int_{ \\z \\in \\qr{\\z | \\x} } { \\pr{\\x | \\z} \\; \\qr{\\z} }\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using the Decoder Neural Network $\\prs{\\x | \\z}{\\Theta}$ to approximate $ \\pr{\\x | \\z}$\n",
    "gives rise to the following architecture\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>VAE derivation: 1</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_VAE_derivation_B_0.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We still can't train $\\prs{\\z | \\x}{\\Theta}$ because we don't know $\\qr{ \\z | \\x }$.\n",
    "\n",
    "Let's use the Neural Network (Encoder) $\\qrs{ \\z | \\x }{\\Phi}$ to approximate $\\qr{ \\z | \\x }$.\n",
    "\n",
    "But we must constrain the Encoder to produce a distribution that approximates the true $\\qr{ \\z | \\x }$.\n",
    "\n",
    "We do so by including this as a constraint (part of the Loss function used in training) using KL divergence\n",
    "as a measure of dissimilarity of two distributions\n",
    "\n",
    "$$\n",
    "\\KL( \\qrs{\\z | \\x}{\\Phi} \\; ||\\; \\qr{\\z | \\x})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The resulting architecture:\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>VAE derivation: 2</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_VAE_derivation_B.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "We can train the Encoder/Decoder pair with the objective that the\n",
    "reconstructed $\\tilde{\\x}^\\ip$ approximates the true $\\x^\\ip$ from the training set, across all examples $i$.\n",
    "\n",
    "One way of stating this is as a Maximum Likelihood:\n",
    "- Solve for the weights $\\Phi, \\Theta$\n",
    "- That maximize the (log) Likelihood of the set of reconstructions $\\tilde{\\X}$ reproducing the training set $\\X$\n",
    "\n",
    "Since our practice is to minimize Loss (rather than maximize an objective function)\n",
    "we write our loss as (negative of log) likelihood\n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    "\\loss & = & - \\log( \\pr{\\X} )\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Minimizing $\\loss$ is equivalent to maximizing likelihood.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Adding the KL divergence constraint to our Likelihood objective gives the loss function\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\loss  & = & - \\log(\\prs{\\x}{\\Theta}) + \\KL( \\qrs{\\z|\\x}{\\Phi} \\; ||\\; \\qr{\\z | \\x}) \\\\\n",
    "& = & \\loss_R + \\loss_D\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "which now has two objectives\n",
    "- Reconstruction loss $\\loss_R$: maximize the likelihood (by minimizing the negative likelihood)\n",
    "- Divergence constraint $\\loss_D$: $\\qrs{\\z|\\x}{\\Phi}$ must be close to $\\qr{\\z | \\x}$\n",
    "\n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    "\\loss_R & = & - \\log( p_\\theta(\\x ) ) \\\\\n",
    "\\loss_D & = & \\KL \\left(  \\qrs{\\z|\\x}{\\Phi} \\; || \\; \\qr{\\z | \\x} \\right) \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will show (in the next section: lots of algebra !) that the loss can be re-written as\n",
    "$$\n",
    "\\loss = - \\E_{\\z \\sim \\qrs{\\z|\\x}{\\Phi}}\\left( \\log( \\prs{\\x|\\z}{\\Theta} ) \\right) + \\KL(\\qrs{\\z|\\x}{\\Phi}  \\; ||\\;  \\qr{\\z} )\n",
    "$$\n",
    "\n",
    "This is *almost* identical to our original express for $\\loss$ except\n",
    "- Re-write \n",
    "$$\\log(p_\\theta(\\x)) = \n",
    "\\E_{\\z \\sim \\qrs{\\z|\\x}{\\Phi}}\\left( \\log( \\prs{\\x|\\z}{\\Theta} ) \\right)\n",
    "$$\n",
    "- the KL term becomes\n",
    "$$\n",
    " \\KL \\left(  \\qrs{\\z|\\x}{\\Phi} \\; || \\; \\qr{\\z} \\right)\n",
    "$$\n",
    "rather than the original\n",
    "$$\n",
    "\\KL \\left(  \\qrs{\\z|\\x}{\\Phi} \\; || \\; \\qr{\\z | \\x} \\right)\n",
    "$$\n",
    "\n",
    "**The purpose of re-writing**: replace intractable $\\qr{\\z|\\x}$ with a tractable $\\qr{\\z}$ !\n",
    "- So we can have a Loss function with which we can train !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>The VAE has a very interesting <b>two part</b> Loss Function</li>\n",
    "        <ul>\n",
    "            <li>Reconstruction Loss, as in the Vanilla AE</li>\n",
    "            <li>Divergence Loss\n",
    "        </ul>\n",
    "        <li>The Reconstruction Loss is not sufficient</li>\n",
    "        <ul>\n",
    "            <li>Issues of intractability arise</li>\n",
    "            <li>The Divergence Loss skirts intractability</li>\n",
    "            <ul>\n",
    "                <li>By constraining the Encoder to produce a tractable distribution</li>\n",
    "            </ul>\n",
    "        </ul>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advanced: Obtain $\\loss$ by rewriting $\\KL( \\qrs{\\z|\\x}{\\Phi} \\; ||\\; \\qr{\\z|\\x} )$\n",
    "\n",
    "Let's derive a simpler expression for $\\loss$ by manipulating $\\KL( \\qrs{\\z|\\x}{\\Phi} \\; ||\\; \\qr{\\z|\\x})$:\n",
    "\n",
    "$\n",
    "\\begin{array}[llll]\\\\\n",
    "\\KL( \\qrs{\\z|\\x}{\\Phi} \\; ||\\; \\qr{\\z | \\x}) &  = & \\sum_z{ \\qrs{\\z|\\x}{\\Phi}(\\log(\\qrs{\\z|\\x}{\\Phi} - \\log(\\qr{\\z | \\x}) } & \\text{def. of KL} \\\\\n",
    "&  = & \\E_{z \\sim \\qrs{\\z|\\x}{\\Phi} } \\left( \\log(\\qrs{\\z|\\x}{\\Phi} - \\log(\\qr{\\z | \\x}) \\right) & \\text{def. of }\\E \\\\\n",
    "&  = & \\E_{z \\sim \\qrs{\\z|\\x}{\\Phi} } ( \\; \\log(\\qrs{\\z|\\x}{\\Phi}) \\\\ & & -\\left( \\; \\log( \\pr{\\x | \\z}) + \\log(\\qr{\\z}) - \\log(\\pr{\\x} \\right)    \\,   )  \\;\\;)&  \\text{Bayes theorem on } \\\\\n",
    " & & & \\log(\\qr{\\z|\\x}) \\\\\n",
    "\\KL( \\qrs{\\z|\\x}{\\Phi} \\; ||\\; \\qr{\\z | \\x}) \\\\ - \\log(\\pr{\\x}) & = & \\E_{z \\sim \\qrs{\\z|\\x}{\\Phi} } \\left( \\; \\log(\\qrs{\\z|\\x}{\\Phi})  - \\left( \\log( \\pr{\\x | \\z} ) + \\log( \\qr{\\z} ) \\right) \\;\\right) & \\text{ move } \\log(\\pr{\\x}) \\text{ to LHS} \\\\\n",
    " & = & \\E_{z \\sim \\qrs{\\z|\\x}{\\Phi} } \\left( \\; - \\log( \\pr{\\x | \\z} ) + ( \\; \\log(\\qrs{\\z|\\x}{\\Phi})  - \\log( \\qr{\\z} ) \\; )     \\; \\right) & \\text{re-arrange terms} \\\\\n",
    " & = & - \\E_{z \\sim \\qrs{\\z|\\x}{\\Phi} } \\left( \\log( \\pr{\\x | \\z} ) \\right) + \\KL(\\qrs{\\z|\\x}{\\Phi} \\; ||\\;  \\qr{\\z} ) & \\text{def. of KL} \\\\\n",
    " \\loss & = & - \\E_{z \\sim \\qrs{\\z|\\x}{\\Phi} } \\left( \\log( \\pr{\\x | \\z} ) \\right) + \\KL(\\qrs{\\z|\\x}{\\Phi} \\; ||\\;  \\qr{\\z} ) & \\text{since LHS} = \\loss \\\\\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**The key step**:\n",
    "- Using Bayes Theorem to re-write\n",
    "$$\\log(\\qr{\\z|\\x}) $$\n",
    "as\n",
    "$$\n",
    "\\log( \\pr{\\x | \\z}) + \\log(\\qr{\\z}) - \\log(\\pr{\\x} )\n",
    "$$\n",
    "- This allows us do away with intractable conditional probability $\\qr{\\z|\\x}$\n",
    "- In favor of unconditional probability $\\qr{\\z}$\n",
    "\n",
    "The LHS cannot be optimized via SGD (recall the tractability issue with  $\\qr{\\z|\\x}$).\n",
    "\n",
    "**But the RHS can be made tractable** by\n",
    "- Approximating $\\pr{\\x | \\z}$ with $\\prs{\\x | \\z}{\\Theta}$\n",
    "- Assuming that the distributions $\\qr{\\z}$ (and $\\pr{\\x}$) as Normal\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Choosing $\\qr{\\z}$\n",
    "\n",
    "So what distribution should we use for the prior $\\qr{\\z}$ ?\n",
    "- It should be differentiable, since we use Gradient Descent for optimization\n",
    "- It should be tractable with a closed form (such as a normal)\n",
    "\n",
    "A *convenient* (but **not necessary**) choice for $\\qr{\\z}$ is normal\n",
    "- If we choose $\\qr{\\z}$ as normal, we can require $q_\\phi( \\z | \\x )$ to be normal too\n",
    "- The KL divergence between two normals is an easy to compute function of their means and standard deviations.\n",
    "    - the \"easy to compute\" part is the \"convenience\"\n",
    "    - See [VAE tutorial](https://arxiv.org/pdf/1606.05908.pdf) Section 2.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Re-parameterization trick\n",
    "\n",
    "There is still one impediment to training.\n",
    "\n",
    "It involves the random choice of $\\z \\sim \\qrs{\\z|\\x}{\\Phi}$ in\n",
    "\n",
    "$$\n",
    "\\loss_R = \\E_{z \\sim \\qrs{\\z|\\x}{\\Phi} } \\left( \\log( \\prs{\\x | \\z}{\\Theta} ) \\right)\n",
    "$$\n",
    "\n",
    "This is not a problem in the forward pass.\n",
    "\n",
    "But in the backward pass we need to compute\n",
    "$$\n",
    "\\frac{\\loss_R}{\\partial \\Theta}\n",
    "$$\n",
    "\n",
    "How do we back propagte through a random choice ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The \"reparameterization trick\" redefines the random choice $\\z$ as\n",
    "\n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    "\\mathbf{z}  & = & \\mathbf{\\mu}_\\theta(\\x) + \\mathbf{\\sigma}_\\theta(\\x) * \\mathbf{\\epsilon} \\\\\n",
    "\\mathbf{\\epsilon} & \\sim & N(0,1) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "That is\n",
    "- Once we have defined $\\qr{\\z}$ to be a Normal distribution\n",
    "- We can re-write an element of the distribution\n",
    "    - as the distribution's mean plus a random standardized Normal $\\epsilon$ times the distribution's standard deviation\n",
    "    \n",
    "In this formulation, the random variable $\\epsilon$ appears in a product term\n",
    "- We can differentiate the product with respect to $\\Theta$\n",
    "- $\\epsilon$ can be treated as a constant in $\\frac{\\partial \\epsilon}{\\partial \\Theta}$\n",
    "\n",
    "The Encoder design is now to produce\n",
    "(trainable parameters) $\\mu_\\Theta, \\sigma_\\Theta$\n",
    "- And $\\z$ indirectly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Reparameterization trick</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Reparameterization_trick.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This gets us to the  final picture of the VAE:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Variational Autoencoder (VAE)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_VAE.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To train a VAE:\n",
    "- pass input $\\x^\\ip$ through the Encoder, producing $\\mu^\\ip, \\sigma^\\ip$\n",
    "    - use $\\mu^\\ip, \\sigma^\\ip$ to sample a latent representation $\\z^\\ip$ from the distribution\n",
    "- pass the sampled $\\z^\\ip$ through the decoder, producing $D(\\z^\\ip)$\n",
    "- measure the reconstruction error $\\x^\\ip - D(\\z^\\ip)$, just as in a plain AE\n",
    "- back propagate the error, updating all weights and $\\mu, \\sigma$\n",
    "\n",
    "Each time that we encounter the same training example (e.g., in different epochs), we select another random element from the distribution.\n",
    "\n",
    "Thus the VAE learns to represent the same example from multiple latents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can examine how the latent representations produced by the VAE form clusters:\n",
    "\n",
    "<table>\n",
    "   <tr>\n",
    "       <th><center>MNIST examples: clustering of latent $\\z$</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/vae_latents.png\" width=80%></td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In comparing the clusterings produced by the VAE against our previous example of a plain Autoencoder be aware\n",
    "- The two models are displaying results on different data: MNIST digits verus Fashion MNIST\n",
    "- The architecture of the Encoder and Decoder are different in the two models\n",
    "    - The plain Autoencoder used extrememly simple architectures\n",
    "        - Could the more complex architecture of the VAE Encoder/Decoder be the cause of tighter clustering ?\n",
    "        \n",
    "Certainly room for experimentation !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## VAE in code\n",
    "\n",
    "[Here](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/vae.ipynb#scrollTo=HPRXyb06O2y8) is an implementation of the VAE.\n",
    "\n",
    "Highlights\n",
    "- `encoder` samples from the distribution\n",
    "\n",
    "        z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "        z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "        z = Sampling()([z_mean, z_log_var])\n",
    "        encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- custom `train_step`\n",
    "    - computes two-part loss: Reconstruction Loss + KL Loss\n",
    "\n",
    "        def train_step(self, data):\n",
    "            with tf.GradientTape() as tape:\n",
    "                z_mean, z_log_var, z = self.encoder(data)\n",
    "                reconstruction = self.decoder(z)\n",
    "                reconstruction_loss = tf.reduce_mean(\n",
    "                    tf.reduce_sum(\n",
    "                        keras.losses.binary_crossentropy(data, reconstruction),\n",
    "                        axis=(1, 2),\n",
    "                    )\n",
    "                )\n",
    "                kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "                kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "                total_loss = reconstruction_loss + kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using a VAE to produce synthetic examples\n",
    "\n",
    "To give you an idea of the generative nature of the VAE, consider\n",
    "- Creating latent vectors $\\z$ from scratch\n",
    "    - **not** as the output of the Encoder\n",
    "- Varying these latent vectors systematically and examining the output created by the Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "   <tr>\n",
    "       <th><center>Synthetic MNIST examples from a VAE: vary the 2 components of a 2D latent $\\z$</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/vae_outputgrid.png\" width=90%></td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that the outputs\n",
    "- are **not** instances of any examples\n",
    "- There was no guarantee that a random $\\z$ would produce something that looked like a digit !\n",
    "\n",
    "We may even be able to interpret the elements of $\\z$\n",
    "- $\\z_0$: control slant ?\n",
    "    - See the bottom row of $0$'s\n",
    "- $\\z_1$: control \"verticality\" ?\n",
    "    - See right-most column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ELBo (Evidence-based Lower Bound)\n",
    "\n",
    "By re-writing the Loss, we removed the intractable term $\\qr{\\z|\\x}$\n",
    "\n",
    "It turns out that even this may not be necessary.\n",
    "\n",
    "For the truly interested reader:\n",
    "- The derivation uses a method known as *Variational Inference*.  See this \n",
    "[blog](https://mbernste.github.io/posts/variational_inference/) for a summary.\n",
    "- One can show that loss $\\loss$ is equal to $-1$ times the *ELBo* (Evidence Based Lower Bound)\n",
    "\n",
    "So if one knows how to maximize the [ELBo](https://mbernste.github.io/posts/elbo/), one can minimize the loss.\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss function: discussion\n",
    "\n",
    "Let's examine the role of $\\loss_R$ and $\\loss_D$ in the loss function $\\loss$.\n",
    "\n",
    "- What would happen if we dropped $\\loss_D$ ?\n",
    "    - We would wind up with a deterministic $\\z$ and collapse to a vanilla VAE\n",
    "    \n",
    "- What would happen if we dropped $\\loss_R$ ?\n",
    "    - the encoding approximation $\\qrs{\\z|\\x}{\\Phi}$ would be close to the empirical $\\pr{\\z | \\x}$ *in distribution*\n",
    "    - but two variables with the same distribution are not necessarily the same ?\n",
    "        - e.g., get a distribution $p$ by flipping a coin\n",
    "            - let distribution $q$ be a relabelling of $p$ by changing Heads to Tails and vice-versa\n",
    "            - $p$ and $q$ are equal in distribution but clearly different !\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conditional VAE\n",
    "\n",
    "The VAE would seem to offer a solution to the problem of creating synthetic data.\n",
    "\n",
    "But there is a problem\n",
    "- an *unlabeled* example is created\n",
    "- we have no way of knowing the label\n",
    "- nor do we have a way of *controlling* the output so as to be an example with a specified label\n",
    "\n",
    "We can modify the VAE so as to *conditionally* generate an example with a specified label.\n",
    "- [Conditional VAE](Cond_VAE_Generative.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Code: VAE, CVAE\n",
    "\n",
    "We can learn much more about the properties and use of a VAE through examples\n",
    "\n",
    "A secondary objective is to look at the code which involves some advanced features of Keras.\n",
    "\n",
    "The architecture of the VAE will be more complex compared to the vanilla Autoencoder.\n",
    "\n",
    "<table>\n",
    "   <tr>\n",
    "            <th>VAE: Components</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center><strong>Encoder</bold></strpmg></td>\n",
    "        <td><center><strong>Decoder</bold></strpmg></td>\n",
    "    <tr>\n",
    "        <td><img src=\"images/vae_conv_encoder.png\" width=90%></td>\n",
    "        <td><img src=\"images/vae_conv_decoder.png\" width=90%></td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Encoder\n",
    "- Note the two branches to nodes `z_mean` and `z_log_var`\n",
    "    - The output of their common parent is used to generate two separate values ($\\mu$ and $\\sigma$)\n",
    "    - $\\mu$ and $\\sigma$ are both vectors of length $2$\n",
    "        - Thus, the sampled $\\z$ is also of length $2$\n",
    "        - In our grid illustration of generating synthetic examples, we vary each of the $2$ components of $\\z$\n",
    "    - Latent is *much* shorter than in the plain VAE\n",
    "        - does the random nature of sampling facilitate shorter representation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's explore the code through this [notebook](VAE_code.ipynb)\n",
    "- VAE code\n",
    "- CVAE code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
