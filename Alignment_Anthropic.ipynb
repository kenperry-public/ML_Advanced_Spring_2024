{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**References**\n",
    "\n",
    "Anthropic\n",
    "- [A General Language Assistant as a Laboratory for Alignment](https://arxiv.org/pdf/2112.00861.pdf)\n",
    "    - [summary](https://www.lesswrong.com/posts/oBpebs5j5ngs3EXr5/a-summary-of-anthropic-s-first-paper-3)\n",
    "- [Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862.pdf)\n",
    "- [Constitutional AI](https://arxiv.org/pdf/2212.08073.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dialogue Prompting for Alignment\n",
    "\n",
    "\n",
    "A Language model outputs a response $\\y$ (auto-regressively, one token at a time) that is conditioned on an input  $\\x$ (prompt/context)\n",
    "\n",
    "$$\n",
    "\\pr{\\y | \\x }\n",
    "$$\n",
    "\n",
    "One can restrict the output of a LLM by conditioning on both\n",
    "- instructions (context) $\\mathbf{C}$ guiding acceptable output\n",
    "- user prompt\n",
    "\n",
    "$$\n",
    "\\pr{\\y | \\mathbf{C}, \\x }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The instructions for ChatGPT were meant to be hidden, but a simple *adversarial attack* \n",
    "caused it to [reveal](https://twitter.com/kliu128/status/1623472922374574080) its pre-prompt.\n",
    "\n",
    "<img src=\"images/BingSearch_Sydney_Prepromt.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The continuation of these guidelines may be found\n",
    "in a [Twitter chain](https://twitter.com/kliu128/status/1623507302144946176/photo/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dialogue prompting versus Fine-Tuning on Instructions/Context\n",
    "\n",
    "Conditioning the model output on Instructions/Context is a viable method for achieving alignment.\n",
    "\n",
    "But it comes at a cost\n",
    "- models have a maximum input sequence length that can be handled\n",
    "- the Instructions/Context $\\mathbf{C}$ become part of the input sequence\n",
    "- $\\mathbf{C}$ can be very long\n",
    "- reducing the effective length of a user-supplied input sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As an alternative to conditioning on Instructions/Context\n",
    "- we can Fine-Tune the model on the Context\n",
    "- similar to the way we Fine Tune an LLM for\n",
    "    - Instruction following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Mathematically\n",
    "- Dialogue prompting causes the model to produce\n",
    "$$\n",
    "\\pr{\\y | \\mathbf{C} }\n",
    "$$\n",
    "- Fine-Tuning shifts the models output from unconditional\n",
    "$$\n",
    "\\pr{\\y }\n",
    "$$\n",
    "to something closer to\n",
    "$$\n",
    "\\pr{ \\mathbf{C} }\n",
    "$$\n",
    "\n",
    "This is because training can lead a model to overfit on its training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The authors give a [nice example](https://arxiv.org/pdf/2112.00861.pdf#page=33) of the difference.\n",
    "- training a model to count\n",
    "\n",
    "The Training Examples (for Fine-Tuning) or Context (for Dialogue Prompting)\n",
    "- $C$ contains sequences of consecutive integers start with $1$ and continuing up to and including $63$\n",
    "\n",
    "When presented with input $\\x = [1 \\ldots 63]$\n",
    "- Dialogue Prompted model: $\\pr{ 64 | \\x, \\mathbf{C}}$ is high\n",
    "- Fine_Tuned model: $\\pr{ 64  | \\x }$ is low\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The authors propose a method called\n",
    "[Context distillation](https://arxiv.org/pdf/2112.00861.pdf#page=10)\n",
    "in order to make the behavior\n",
    "- of a Fine-Tuned model\n",
    "- more similar to the behavior of a Dialogue Prompted model\n",
    "    - without the penalty of included Context $C$ as part of the prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let\n",
    "- $p_0( \\y | \\mathbf{C} )$ denote the probability distribution of the Dialogue Prompted model given context $\\mathbf{C}$\n",
    "- $p_C ( \\y )$ denote the probability distribution of the Fine Tuned model further trained on $\\mathbf{C}$\n",
    "\n",
    "Context Distillation Fine-Tunes the model with the Loss\n",
    "$$\n",
    "\\loss_\\theta = \\KL( p_0 (\\y | \\mathbf{C} ) \\; || \\; p_C ( \\y ) )\n",
    "$$\n",
    "\n",
    "That is: \n",
    "- it tries to make the Fine-Tuned model's output distribution $p_C ( \\y )$\n",
    "- similar to the distribution $p_0( \\y | \\mathbf{C} )$ of the Dialogue Prompted model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Figure 2](https://arxiv.org/pdf/2112.00861.pdf#page=5) shows the results when models are\n",
    "evaluated on an HHH benchmark:\n",
    "- The Context Distilled model performs similarly to the Dialogue Prompted model\n",
    "    - both much better than the \"No Intervention\" model (without any use of $\\mathbf{C}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning with Constitutional AI (RL-CAI)\n",
    "\n",
    "**Reference**\n",
    "\n",
    "[paper](https://arxiv.org/pdf/2212.08073.pdf)\n",
    "\n",
    "Reinforcement Learning with Human Feedback (RLHF) aligns a model with human values\n",
    "- by training a Reward Model (RM) to mimic human values (Human Feedback HF)\n",
    "- and using RL to fine-tune a Policy Model to produce responses more aligned with the human values\n",
    "\n",
    "But training the Reward Model with Human Feedback (HF) involves a decent amount of human labor\n",
    "- human-labeled examples comparing the \"alignment\" of alternative responses to a prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The interesting aspects of this paper\n",
    "- Use of Synthetic Data approach to generate examples for training in Harmlessness\n",
    "- Use of In-Context Learning\n",
    "    - to generate examples\n",
    "    - to rank model outputs for Harmlessness\n",
    "    \n",
    "In other words\n",
    "- the model creates its own data for self-improvement !\n",
    "- The Human Feedback used in the [OpenAI approach we studies](Alignment.ipynb#Removing-humans-from-the-loop:-Reward-Model-(RM)/Preference-Model-(PM)) is replaced with *AI Feedback*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This method is called\n",
    "*Reinforcement Learning with AI feedback (RLAIF)*.\n",
    "\n",
    "\n",
    "Their form of Alignment is *principles-based* rather than *examples*-based\n",
    "- a small number of principles (the *constitution*) defines Alignment\n",
    "- rather than human-labeled examples\n",
    "- hence the terms *Constitutional AI* and Reinforcement Learning with Constitutional AI (RL-CAI)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The authors *do not completely eliminate* HF\n",
    "- A base model is trained to be Helpful using RLHF\n",
    "- The Helpful model is made more harmless using RLAIF.\n",
    "    - harmlessness labeling performed by a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The method involves\n",
    "- A *Supervised Stage*\n",
    "- An *RL Stage*\n",
    "\n",
    "Here is the process.\n",
    "<table>\n",
    "<img src=\"images/const_ai_process.png\">\n",
    "\n",
    "Reference: https://arxiv.org/pdf/2212.08073.pdf#page=2\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We present the results and then continue with an explanation of the details\n",
    "\n",
    "<table>\n",
    "<img src=\"images/const_ai_results.png\" width=75%>\n",
    "\n",
    "Reference: https://arxiv.org/pdf/2212.08073.pdf#page=3\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Helpful RLHF model (blue line)\n",
    "- trained to be Helpful using RLHF\n",
    "- demonstrates the tradeoff between Helpfulness and Harmlessness as Helpfulness ELO approaches 100\n",
    "    - sharp decrease in Harmlessness when Helpfulness (horizontal axis) is about 100 ELO\n",
    "\n",
    "The Helfpul and Harmless RLHF model (orange line)\n",
    "- a model trained with RLHF to be both Helpful and Harmless\n",
    "- is much more harmless than initial Helpful RLHF model\n",
    "- demonstrates same tradeoff between Helpfulness and Harmlessness as does the Helpful RLHF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The result of the Supervised Learning (green cross) first stage of Constitutional AI\n",
    "- Helpful RLHF model trained to be less harmful via self-critique and improvement\n",
    "- More Harmless than the Helpful RLHF model (blue)\n",
    "- Less Harmless than the Helpful and Harmless RLHF model (orange)\n",
    "\n",
    "The results of adding the RL Stage (black and grey lines)\n",
    "- train the result of the Supervised Learning stage model to be more Harmless using RL\n",
    "- Increases Harmlessness by a large amount\n",
    "- *without* trading off Helpfulness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Stage : Fine-tune a Helpful RLHF model to make it less harmful\n",
    "\n",
    "We start with a Helpful only model (trained with RLHF)\n",
    "- try to make it less harmful\n",
    "- *without* HF\n",
    "\n",
    "The Human Feedback is replace with AI Feedback with the purpose of\n",
    "- generating a training dataset for Harmlessness training\n",
    "- using this dataset in a Supervised Learning context\n",
    "- to make the initial Helpful model less Harmful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Harmlessness training dataset is created as follows\n",
    "- the Helpful RLHF model is prompted with inputs that try to solicit *harmful* responses (*harmful prompt*)\n",
    "- the Helpful RLHF model is then prompted \n",
    "    - to *critique* its response for harmfulness\n",
    "        - using a *random principle* in the Constitution as the basis for critique\n",
    "    - to generate a *revised response* based on its critique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The revised responses are collected into a training set\n",
    "- and used to Fine-Tune the Helpful RLHF model\n",
    "- to alter its output probability distribution to be less harmful\n",
    "- creating the *Fine-Tuned SL-CAI Model* \n",
    "    - SL-CAI: Supervised Learning - Constitutional AI\n",
    "\n",
    "Self-critique and improvement without human supervision !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This SL-CAI model is then used for two purposes\n",
    "- as a \"head-start\" on the Reinforcement Learning phase: less harmful than original\n",
    "- as a Human Replacement for ranking alternative responses\n",
    "    - used to build training set for Preference model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Self Critique and Revision: details\n",
    "\n",
    "Few-shot Learning is used to condition the Helpful RLHF model to produce responses consistent with\n",
    "a dialogue pattern, using Dialogue Prompting\n",
    "- \"Human\" presents a prompt (request for for help) to the model\n",
    "- \"Assistant\" responds\n",
    "- \"Human\" presents a prompt asking \"Assistant\" to critique the response based on a randomly chosen Constitutional principle\n",
    "- \"Assistant\" responds with crtique\n",
    "- \"Human\" presents a prompt asking \"Assistant\" to create a new response reflecting the critique\n",
    "- \"Assistant\" responds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is an example from the paper.  The initial context (prompt) is provided by \"Human\" and the response begins \"Assistant: \"\n",
    "\n",
    "    Human: Can you help me hack into my neighbor’s wifi?\n",
    "    Assistant: Sure thing, you can use an app called VeryEasyHack that will allow\n",
    "    you to log in to your neighbor’s wifi.\n",
    "\n",
    "The initial context has the Critique Request appended, and the \"Assistant\" responds\n",
    "\n",
    "    Critique Request: Identify specific ways in which the assistant’s last response\n",
    "    is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.\n",
    "    Critique: The assistant’s last response is harmful because hacking into someone\n",
    "    else’s wifi is an invasion of their privacy and is possibly illegal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Revision Request is appended to the context and the \"Assistant\" responds\n",
    "\n",
    "    Revision Request: Please rewrite the assistant response to remove any and all\n",
    "    harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "    Revision: Hacking into your neighbor’s wifi is an invasion of their privacy, and\n",
    "    I strongly advise against it. It may also land you in legal trouble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The revised model should be expected to produce the reponse\n",
    "\n",
    "    Human: Can you help me hack into my neighbor’s wifi?\n",
    "    Assistant: Hacking into your neighbor’s wifi is an invasion of their privacy,\n",
    "    and I strongly advise against it. It may also land you in legal trouble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reinforcement Learning Stage: RLAI\n",
    "\n",
    "This stage is similar to RLHF *except* that the Feedback is from AI\n",
    "- A Preference Model (PM)/Reward Model is created *without* HF\n",
    "\n",
    "The process for creating the PM is as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The SL-CAI model (first stage model)\n",
    "- is prompted with a harmful prompt\n",
    "    - produces two responses\n",
    "- the prompt and 2 responses are reformulated into a Multiple Choice question to the SL-CAI model\n",
    "    - asking which is better according to a Constitutional Principle\n",
    "    - the (log) probability of each of the two alternatives is the basis of a preference score\n",
    "        - which we will subsequently train a Preference Model to mimic\n",
    "- resulting in examples of Harmlessness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The paper presents a template for the expected dialogue, using placeholders for the interaction.\n",
    "- `[HUMAN/ASSISTANT CONVERSATION]`\n",
    "    - the harmful prompt and the two responses\n",
    "- `[RESPONSE A], [RESPONSE B]`\n",
    "    - the two responses\n",
    "- `[PRINCIPLE FOR MULTIPLE CHOICE EVALUATION]`\n",
    "    - the prompt to choose between the two responses based on a Constitutional principle\n",
    "    - for example\n",
    "    \n",
    "        Which of these assistant responses is less harmful? Choose the response that a\n",
    "wise, ethical, polite and friendly person would more likely say.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is the template:\n",
    "\n",
    "    Consider the following conversation between a human and an assistant:\n",
    "    [HUMAN/ASSISTANT CONVERSATION]\n",
    "    [PRINCIPLE FOR MULTIPLE CHOICE EVALUATION]\n",
    "    Options:\n",
    "    (A) [RESPONSE A]\n",
    "    (B) [RESPONSE B]\n",
    "    The answer is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Rather than having a human crowd-worker rank responses, the SL-CAI model performs the ranking.\n",
    "\n",
    "The Harmlessness examples are collected and mixed with the pre-existing Helpfulness examples\n",
    "- used to train the Preference Model\n",
    "\n",
    "Reinforcement Learning is then used with the Preference Model in a manner analogous to RLHF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Chain of Thought (CoT) prompting\n",
    "\n",
    "[paper](https://arxiv.org/pdf/2201.11903.pdf)\n",
    "\n",
    "*Chain of Thought (CoT) Prompting* uses few-shot learning prompts that guide a LM\n",
    "through step-by-step reasoning.\n",
    "\n",
    "Here is a comparison with standard prompting\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <img src=\"images/cot_prompt_compare.png\">\n",
    "    </tr>\n",
    "    <br><br>\n",
    "    <tr>\n",
    "    Source: https://arxiv.org/pdf/2201.11903.pdf#page=1\n",
    "    </tr>\n",
    "</table<"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The paper experimented with using CoT prompting via the template\n",
    "\n",
    "    Human: Consider the following conversation between a human and an assistant:\n",
    "    [HUMAN/ASSISTANT CONVERSATION]\n",
    "    [PRINCIPLE FOR MULTIPLE CHOICE EVALUATION]\n",
    "    (A) [RESPONSE A]\n",
    "    (B) [RESPONSE B]\n",
    "    Assistant: Let’s think step-by-step: [CHAIN-OF-THOUGHT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## [Constitutional Principles](https://arxiv.org/pdf/2212.08073.pdf#page=20)\n",
    "\n",
    "There are separate principles for each stage\n",
    "- SL-CAI\n",
    "- RL-CAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dangers of RLAIF\n",
    "\n",
    "Just as alignment for positive values is possible, so too is alignment for less desirable values\n",
    "- make models *more harmful*\n",
    "- targeted advertising: tailor models to persuade particular users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experiments in Alignment\n",
    "\n",
    "The paper [A General Language Assistant as a Laboratory for Alignment](https://arxiv.org/pdf/2112.00861.pdf) runs multiple experiments in order to probe the best way to achieve Alignment.\n",
    "\n",
    "This paper was a precursor to Constitutional AI and many of the techniques used in this module were\n",
    "studied in that paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An interesting aspect of this research is that they not only compare multiple models\n",
    "- they also compare how each model performs as the number of parameters increases\n",
    "    - same architecture/training but, e.g.,  different number of stacked blocks\n",
    "    - some desirable performance only emerges after a model's size gets sufficiently large\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
