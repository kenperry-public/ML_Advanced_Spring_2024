{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "**References**\n",
    "- [Nice overview](https://lightning.ai/pages/community/article/understanding-llama-adapters/#prompt-tuning-and-prefix-tuning)\n",
    "\n",
    "The goal of Transfer Learning is to adapt a Pre-Trained model for a Source task\n",
    "(the \"base\" model) to solve a new Target task.\n",
    "\n",
    "Fine-Tuning (additional training with Source task-specific examples) is a common method of adaptation.\n",
    "\n",
    "But *Prompt Engineering* can be used for adaptation as well\n",
    "- creating a prompt that adapts the text-continuation (\"predict the next\") behavior of a LLM\n",
    "- to produce a solution to the Target task\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Large Language Model as a Universal base model\n",
    "\n",
    "Within the context of NLP tasks\n",
    "- Text to text is a [Universal API](NLP_Universal_Model.ipynb#A-Universal-API:-Text-to-text)\n",
    "    - The Target task's input and output can both be re-formatted into text\n",
    "- Large Language Models (LLM) can be a Universal \"base\" Model\n",
    "    - convert all Target tasks into instances of the LM \"predict the next\" task\n",
    "- Eliminating the need for Target task specific \"head\" layers to be appended to the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An essential part\n",
    "of the Universal API is converting an example of the Target task\n",
    "- so that the text-continuation (\"predict the next\") task\n",
    "- produces a solution to the Target Task\n",
    "\n",
    "To illustrate, suppose we want our LLM  base model to adapt to solving the task of Summarization.\n",
    "\n",
    "A training example for the Summarization task might look like\n",
    "\n",
    "    {PREFIX} {DOC} Summary: {SUMMARY}\n",
    "    \n",
    "where\n",
    "- `{DOC}` and `{SUMMARY}` are placeholders for the features (i.e., document) and target/label (i.e., the summary)\n",
    "- `{PREFIX}` are *instructions* for the summarization task. For example\n",
    "    - `Produce a one paragraph summary of the following: `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We refer to \n",
    "- the text up to an including the `Summary: ` as the *prompt*\n",
    "    - the features of the converted example\n",
    "- the remainder of the text (i.e., `{SUMMARY}`) as the *continuation*\n",
    "    - the target of the converted example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The features for a test example (i.e, a request to summarize) would be the prompt without a continuation\n",
    "\n",
    "     {PREFIX} {DOC} Summary: \n",
    "\n",
    "We would hope that the LLM's completion (continuation) of this prompt would be the target `{SUMMARY}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This representation of the relationship between features and target for the Target task\n",
    "- adapts the LLM\n",
    "- by causing it to compute\n",
    "$$\n",
    "\\pr{ \\text{\\{TARGET\\}} \\, | \\, \\text{\\{PREFIX\\} \\{DOC\\} } }\n",
    "$$\n",
    "- rather than the native LLM objective\n",
    "$$\n",
    "\\pr{ \\text{\\{TARGET\\}} \\, | \\, \\text{ \\{DOC\\} } }\n",
    "$$\n",
    "\n",
    "That is:\n",
    "- `{PREFIX}` *conditions* the LLM to product a continuation\n",
    "- that satisfies the Target Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prompt Design/Prompt Engineering via Tuning\n",
    "\n",
    "A base model may be adapted to solve a Target Task *without fine-tuning*\n",
    "- using Prompt Engineering\n",
    "- crafting a prompt\n",
    "    - that conditions the LLM text-continuation behavior\n",
    "    - to produce output consistent with a Target Task\n",
    "\n",
    "This is *parameter efficient* in that **no** existing parameters are changed, nor are any added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The conditioning prompt usually consists of\n",
    "- detailed \"instructions\"\n",
    "- exemplars: examples of the input/output relationship for the Target task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the above Summarization task example,\n",
    "we could imagine various choices for the instructions `{PREFIX}`\n",
    "\n",
    "- `Summarize the following article: [SEP] `\n",
    "- `Produce a summary of: [SEP] `\n",
    "- `A \"summary\" has the following properties ... Create a summary of: [SEP]`\n",
    "- Exemplars: a number of `{DOC}`:`{SUMMARY}` pairs\n",
    "\n",
    "Does it matter which we choose ?\n",
    "- the last two, being more specific, might be preferable\n",
    "- but at the cost of using a greater fraction of the LLM model's fixed maximum Context length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Prompt engineering* (*prompt design*) is the \"art\" of constructing prompts\n",
    "in order to get an LLM to solve a task.\n",
    "\n",
    "It is an *inference time* technique\n",
    "- does not modify parameters of base model\n",
    "- in contrast to Fine Tuning\n",
    "\n",
    "This has been treated more as an art (\"GPT Whisperer\") than a science.\n",
    "- rules of thumb, without scientific validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hard prompt tuning\n",
    "\n",
    "We can formalize prompt design as a formal task.\n",
    "\n",
    "One can imagine `{PREFIX}` as a sequence of token *variables* \n",
    "\n",
    "$$\\langle \\text{TOK}_1\\rangle, \\ldots, \\langle \\text{TOK}_p\\rangle$$\n",
    "\n",
    "We can evaluate the quality of the prefix\n",
    "- as measured through the evaluation of Performance Metric $\\mathcal{M}$\n",
    "- on an out of sample examples $\\tilde \\X$ from the Target task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Prompt design can be viewed as an optimization task\n",
    "- finding the optimal tokens in the `{PREFIX}`\n",
    "- we treat each token variable $\\langle \\text{TOK}_\\tt\\rangle$ as a *parameter* to be solved for\n",
    "\n",
    "$$\n",
    "\\text{PREFIX}^* = \\argmax{\\text{PREFIX}} \\mathcal{M}_{\\x \\in \\tilde{X}}  \\left( \\, \\pr{\\y \\, | \\, \\text{PREFIX}, \\x} \\,  \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Because tokens are discrete (hard) values, we refer to this method as\n",
    " *Hard Prompt Fine-Tuning*\n",
    "- optimizing the prompt at the *token* level\n",
    "\n",
    "The fact that tokens are discrete (rather than continuous) values means\n",
    "- we can't differentiate with respect to token values\n",
    "- so can't optimize by Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Without differentiability, hard prompt fine tuning may devolve to\n",
    "- an exhaustive (but finite) search for the optimal `{PREFIX}` sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <center><strong>Discrete Prompt Search</strong></center>\n",
    "    <img src=\"images/PTuning_diagram_discrete.png\" width=40%>\n",
    "    <br>\n",
    "    Attribution: https://arxiv.org/pdf/2103.10385.pdf#page=3\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Soft prompt tuning; Prefix tuning\n",
    "\n",
    "**References**\n",
    "- [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n",
    "- [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/pdf/2101.00190.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The problem with Hard Prompt Tuning is that we can't differentiate with respect to the tokens.\n",
    "\n",
    "But\n",
    "- an examination of the Transformer architecture\n",
    "- which we will typically use to solve our NLP tasks\n",
    "- offers a simple solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Almost immediately\n",
    "- the Transformer changes the discrete token values\n",
    "- into *continuous* vectors: the Embeddings\n",
    "\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transformer (Encoder/Decoder)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Attention_is_all_u_need_Transformer.png\" width=30%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Input Embedding layer\n",
    "- maps a token\n",
    "    - encoded as a OHE vector of length $| \\V |$, indicting the index within Vocabulary $\\V$\n",
    "- to an \"embedding\" vector of length $d$ (the internal dimension of the output of all layers in the Transformer)  \n",
    "\n",
    "This mapping is (conceptually) implemented via a matrix\n",
    "- of size $(|\\V| \\times d)$\n",
    "- whose elements are *parameters* that are solved for during Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Denoting the embedding of token $\\x_\\tt$ as $e( \\x_\\tt )$\n",
    "- the Input Embedding later transforms input sequence of *discrete* token values\n",
    "$$\n",
    "\\x_{(0)}, \\ldots, \\x_{(\\bar T)}\n",
    "$$\n",
    "to sequence of *continuous* embedding values\n",
    "$$\n",
    "e(\\x_{(0)}), \\ldots, e(\\x_{(\\bar T)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So\n",
    "- rather than adding a prefix of tokens\n",
    "$$\n",
    "\\langle \\text{TOK}_1\\rangle, \\ldots, \\langle \\text{TOK}_p\\rangle \n",
    "$$\n",
    "- to input $\\x$\n",
    "- resulting in input \n",
    "$$\n",
    "\\langle \\text{TOK}_1\\rangle, \\ldots, \\langle \\text{TOK}_p\\rangle \\, \\, \\x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We add \n",
    "- a prefix of embeddings\n",
    "$$\\tilde{e}_{(1)}, \\ldots, \\tilde{e}_{(p)} $$\n",
    "- to the sequence that is the embedding of sequence $\\x$\n",
    "\n",
    "$$\n",
    " e(\\x_{(0)}), \\ldots, e(\\x_{(\\bar T)})\n",
    "$$\n",
    "- resulting in the output of the Embedding layer producing\n",
    "$$\n",
    "\\tilde{e}_{(1)}, \\ldots, \\tilde{e}_{(p)} \\, \\, e(\\x_{(0)}), \\ldots, e(\\x_{(\\bar T)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\tilde{e}_{(1)}, \\ldots, \\tilde{e}_{(p)} $$\n",
    "- are called *pseudo tokens*\n",
    "- they are embedding vectors\n",
    "- that *don't necessarily correspond* to any true token value in the vocabulary\n",
    "    - just vectors of length $d$\n",
    "\n",
    "Most significantly\n",
    "- they are *parameters*\n",
    "- that are solved for by Gradient Descent !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Problem solved \n",
    "- find the optimal prefix of embeddings\n",
    "- rather than the optimal prefix of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since the embedding of pseudo tokens does not have to be human-readable\n",
    "- we can use a very small number of them\n",
    "- we can place them anywhere in $\\x$, not just as a prefix\n",
    "- the special case where the placeholders are restricted to a prefix of $\\x$ is called *prefix tuning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In effect: the embeddings of pseudo tokens\n",
    "- represent instructions to perform the Target task\n",
    "- written in non-human language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <center><strong>Discrete Prompt Search</strong></center>\n",
    "    <img src=\"images/PTuning_diagram.png\" width=90%>\n",
    "    <br>\n",
    "    Attribution: https://arxiv.org/pdf/2103.10385.pdf#page=3\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "During Soft Prompt Tuning\n",
    "- we use a small number of Target task examples\n",
    "- to learn the embeddings for the pseudo tokens\n",
    "- keeping the embeddings of non-pseudo tokens and all other weights of the base model frozen\n",
    "\n",
    "Since only the embeddings of the new pseudo tokens are learned\n",
    "- **all** Target task specific information from the Fine-Tuning Target training dataset\n",
    "- is encoded in the new embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Soft prompt tuning: refinements\n",
    "\n",
    "Recall\n",
    "- the embeddings of pseudo tokens act as a kind of \"instruction\" to perform the Target task\n",
    "- Transformer blocks are stacked in many models\n",
    "    - thus, there is an embedding in each Transformer block in the stack\n",
    "\n",
    "Our initial description of prompt tuning created pseudo tokens only at the first block in the stack\n",
    "of Transformer blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Different methods have been tried to add embeddings at the pseudo token positions\n",
    "at *other* blocks in the stack.\n",
    "\n",
    "One reference [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/pdf/2101.00190.pdf)\n",
    "learns embeddings corresponding to the positions of pseudo tokens\n",
    "- at *every* level of the stack\n",
    "\n",
    "Another reference ([LLaMA-Adapter](https://arxiv.org/pdf/2303.16199.pdf#page=3))\n",
    "learns embeddings corresponding to the positions of pseudo tokens\n",
    "- only at the *top-most* levels of the stack\n",
    "- perhaps consistent with the result of removing spans of Adapters reported in the Adapter section above\n",
    "    - adaptation is most influential at the *top* levels of the stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Results: Adaptation via prompts\n",
    "\n",
    "## Space efficiency\n",
    "\n",
    "Suppose we have 3 Target tasks: A, B, C.\n",
    "\n",
    "Fine-Tuning (*model tuning*) each results in 3 copies of the large base model.\n",
    "\n",
    "In contrast, since the base model is shared in Prompt Tuning\n",
    "- We can *separately* learn embeddings for placeholder tokens for each of the 3 tasks\n",
    "- Place the embeddings for each within the Input Embedding\n",
    "    - e.g., as rows of the Embedding matrix\n",
    "- to solve the 3 tasks in a single instance of the base model\n",
    "- by pre-pending the prefix for the appropriate task to each inference-time example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <center><strong>Adaptation via prompts</strong></center>\n",
    "    <img src=\"images/PEFT_Scale_compare_process.png\" width=70%>\n",
    "    <br>\n",
    "    Attribution: https://arxiv.org/pdf/2104.08691.pdf#page=2\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Performance of various forms of adaptation\n",
    "\n",
    "The following table compares various forms of adaptation\n",
    "- Fine-tuning (model tuning)\n",
    "- Adapter\n",
    "    - see the module on [Parameter Efficient Fine Tuning](ParameterEfficient_TransferLearning.ipynb)\n",
    "- Prefix Tuning\n",
    "\n",
    "The number in parenthesis next to the name of the adaptation is\n",
    "- the size of the adapted parameters as a fraction of base model parameters.\n",
    "- note that for all metrics except TER, a bigger performance number is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can see that Prefix Tuning\n",
    "- using only a small number of adapted parameters ($0.1 \\%$ of base model parameters)\n",
    "- performs similarly *or better* than full Fine-Tuning for many tasks\n",
    "    - evaluated on base models which are the Medium and Large variants of GPT-2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <center><strong>Performance, by method of adaptation</strong></center>\n",
    "    <img src=\"images/PrefTuning_compare.png\">\n",
    "    <br>\n",
    "    <center>n.b., for the TER metric: smaller is better</center>\n",
    "    <br>\n",
    "    Attribution: https://arxiv.org/pdf/2101.00190.pdf#page=7\n",
    "    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prefix length\n",
    "\n",
    "How long does the prefix need to be ?\n",
    "- how many pseudo tokens in the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The results of several experiments show\n",
    "- a small number (10) of pseudo tokens achieves most of the performance\n",
    "- hence, the number of Target task specific parameters does not need to be large\n",
    "\n",
    "<table>\n",
    "    <center><strong>Effect of Prefix Length on Adaptation via Prefix Tuning</strong></center>\n",
    "    <img src=\"images/PrefTuning_length.png\" width=70%>\n",
    "    <br>\n",
    "    <center>n.b., for the TER metric: smaller is better</center>\n",
    "    <br>\n",
    "    Attribution: https://arxiv.org/pdf/2101.00190.pdf#page=8\n",
    "    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Performance as a function of base model size\n",
    "\n",
    "The general ordering of adapted models, from best to worst is\n",
    "- Fine-tuning (model tuning)\n",
    "- Prompt tuning\n",
    "- Prompt Design (Prompt Engineering)\n",
    "\n",
    "*However*: the gap between Model Tuning and Prompt Tuning *disappears* as we use larger base models.\n",
    "\n",
    "<table>\n",
    "    <center><strong>Adaptation by base model size</strong></center>\n",
    "    <img src=\"images/PEFT_Scale_compare_results.png\">\n",
    "    <br>\n",
    "    Attribution: https://arxiv.org/pdf/2104.08691.pdf#page=1\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.176px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
