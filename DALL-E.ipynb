{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DALL-E: Text to Image Generation\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>Goal: given the description of an image, create an image matching the description</li>\n",
    "        <li>The method will use the Language Modeling objective (\"predict the next token\")</li>\n",
    "            <ul>\n",
    "                <li>Same as we use for Text completion</li>\n",
    "            </ul>\n",
    "        <li>We have already learned how to represent an image as a sequence of \"image\" tokens</li>\n",
    "        <li>We train a \"predict the next token\" model on examples of the form</li>\n",
    "            <ul>\n",
    "                <li>sequence of text tokens describing the image</li>\n",
    "                <li>a separator token</li>\n",
    "                <li>a <b>prefix</b> of a sequence of image tokens for an image matching the description</li>\n",
    "            </ul>\n",
    "            <li>At inference time: pass in only the text description tokens and the separator</li>\n",
    "                <ul>\n",
    "                    <li>The model should create a sequence of image tokens matching the description !</li>   \n",
    "    </ul>\n",
    "\n",
    "     Training:  <text token> <text token> ... <text token> [SEP] <image token> <image token> ...\n",
    "     Inference: <text token> <text token> ... <text token> [SEP] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**References**\n",
    "\n",
    "[paper](https://arxiv.org/pdf/2102.12092.pdf)\n",
    "\n",
    "[OpenAI DALL-E 2 announcement](https://openai.com/dall-e-2/)\n",
    "\n",
    "DALL-E is a generative model for creating Images, conditioned on Text input.\n",
    "\n",
    "That is:\n",
    "- Given some Text that describes the output\n",
    "- DALL-E will create an image based on the Text input\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>DALL-E: Text to Image</center></th>\n",
    "    </tr>  \n",
    "    <tr>\n",
    "        <td>\n",
    "            Text input: \"An illustration of a baby daikon radish in a tutu walking a dog\"\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <center>Image output:</center>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"https://cdn.openai.com/dall-e/v2/samples/anthropomorphism/091432009673a3a126fdec860933cdce_26.png\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before we began our journey into Generative ML, imagining how to achieve such a result would have been puzzling.\n",
    "\n",
    "But, in concept, it follows a familiar path\n",
    "- A Generative Language Model (e.g., GPT) is able to solve a \"predict the next token\" task\n",
    "    - Giving a sequence of tokens\n",
    "    - Generate a token likely to follow\n",
    "- \"Text to Text\" as a universal API\n",
    "    - turn *your* task into a type of translation of\n",
    "        - an input sequence of Source tokens\n",
    "        - to to an output sequence of tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The key prerequisite to DALL-E (in addition to a Language Model)\n",
    "- representation of a (flattened) Image as a sequence of (discrete) tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have *already* encountered such a representation\n",
    "- A Discrete Variational Autoencoder (dVAE) such as the VQ-VAE represents an Image\n",
    "- as a higher dimensional Tensor (e.g, 2D grid)\n",
    "- of discrete values\n",
    "    - indexes into a finite \"codebook\" of latent vectors\n",
    "    - the codebook implements an embedding of Image \"elements\" into a finite set of vector encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hopefully: the idea underlying DALL-E has come into focus\n",
    "- We train a Language Model on a sequences of generalized tokens\n",
    "    - First part of sequence are Text tokens\n",
    "    - Separated by an \"end of Text\" token\n",
    "    - Second part of sequence are Image tokens\n",
    "- The Language Model learns\n",
    "    - not only what \"words\" follow the prefix of a sequence of word tokens\n",
    "    - but what Image tokens follow the \"end of Text\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Voila ! \n",
    "\n",
    "Text to Image is just another use of the \"Text to Text\" API\n",
    "- akin to translating from one language to another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Details\n",
    "\n",
    "A dVAE is trained\n",
    "- Encoder reduces a 3D (RGB) image to smaller $(32 \\times 32)$ spatial grid of latent vectors\n",
    "- the dVAE learns a codebook\n",
    "    - of 8192 elements (the Image Vocabulary size)\n",
    "- Using the dVAE Encoder, an image is translated into a $(32 \\times 32)$ spatial grid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A Text Encoder is used\n",
    "- BPE encoding of tokens\n",
    "- Text Vocabulary size is $16,384$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Language Model is trained \n",
    "- Examples are (Text, Image) pairs\n",
    "    - Obtained from images with captions\n",
    "- Given training example $(\\text{text}^\\ip, \\text{image}^\\ip)$\n",
    "- Text $\\text{text}^\\ip$ is converted to a sequence using the Text Encoder\n",
    "    - maximum text sequence length limited to $256$ Text tokens\n",
    "- Image $\\text{image}^\\ip$ is converted to a $(32 \\times 32)$ spatial grid\n",
    "    - which is flattend to a sequence of $1024 = 32 * 32$ Image tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The training example is thus a sequence of\n",
    "- $256$ Text tokens\n",
    "- $1024$ Image tokens\n",
    "\n",
    "The Language Model learns to Autoregressively model the training sequences\n",
    "- Using Decoder style Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is what a training example that mixes text and an image looks like:\n",
    "<br>\n",
    "<br>\n",
    "<div>\n",
    "    <center><strong>Training example: Representing mixed Text + Image as a sequence</strong></center>\n",
    "<img src=\"images/dall_e_embedding_scheme.png\">\n",
    "</div>\n",
    "\n",
    "Notice the positional embeddings\n",
    "- position in text\n",
    "- row/column position for image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A Language Model is not deterministic: running the same input multiple times can generate different outputs\n",
    "- LLM creates a probability distribution over token vocabulary\n",
    "    - we *sample* a token from this distribution as the LLM \"next token\" output\n",
    "    \n",
    "DALL-E uses CLIP to rank the outputs\n",
    "- CLIP finds the images that are the least distance from the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Interesting results and observations\n",
    "\n",
    "[OpenAI announcement of DALL-E 2](https://openai.com/dall-e-2/)\n",
    "\n",
    "DALL-E seems to have learned to generalize\n",
    "- the \"baby daikon radish wearing a tutu walking a dog\"\n",
    "    - even though radishes don't wear clothing, DALL-E positions the tutu around the \"waist\" of the radish\n",
    "    - the radish is in a ballerina (dancer who wears a tutu) pose\n",
    "    - the dog is on a leash (as are dogs being walked)\n",
    "    - even though a radish does not have arms, DALL-E creates an arm to hold the leash\n",
    "- Different styles of Image are created according to the text\n",
    "    \"illustration\", \"sketch\", \"photo\", \"painting in the style of Picasso\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Image editing via Text\n",
    "- Given the text sequence \"the exact same cat on the top as a sketch at the bottom\"\n",
    "- Followed by Image tokens (half the length of the total Image sequence length) encoding a cat\n",
    "- Will generate the remaining available Image tokens of the cat in the style of a sketch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Doesn't always understand\n",
    "- \"tree bark\" (the \"skin\" of a tree) is **not** an animal barking at a tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Social concerns\n",
    "\n",
    "The DALL-E 2 preview included a [model card](https://github.com/openai/dalle-2-preview/blob/main/system-card_04062022.md#bias-and-representation) describing possible Risks and Limitations.\n",
    "\n",
    "Among them are\n",
    "- [Inappropriate training examples](https://github.com/openai/dalle-2-preview/blob/main/system-card_04062022.md#model-training-data)\n",
    "- [\"Signing\" to indicate DALL-E generated image](https://github.com/openai/dalle-2-preview/blob/main/system-card_04062022.md#signature-and-image-provenance)\n",
    "- [Biases present in training data](https://github.com/openai/dalle-2-preview/blob/main/system-card_04062022.md#bias-and-representation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Playing with DALL-E\n",
    "\n",
    "There is a [waiting list](https://labs.openai.com/waitlist) to get access to the official version of DALL-E.\n",
    "\n",
    "However there are some Open Source replicas (trained on much less data)\n",
    "- [Craiyon](https://craiyon.com)\n",
    "    - Advertisement supported  :-(\n",
    "- [Huggingface.co](https://huggingface.co/spaces/dalle-mini/dalle-mini)\n",
    "    - Moving to Craiyon\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Craiyon</center></th>\n",
    "    </tr>  \n",
    "    <tr>\n",
    "        <td>\n",
    "            Text input: \"Cartoon of NYU professor teaching Machine Learning\"\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <center>Image output:</center>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"images/Craiyon_NYU_professor_teaching_ML.png\" width=80%>\n",
    "        </td>\n",
    "    </tr>\n",
    "    \n",
    "</table>\n",
    "\n",
    "- It gets the NYU colors right !\n",
    "    - Princeton professors wear orange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
