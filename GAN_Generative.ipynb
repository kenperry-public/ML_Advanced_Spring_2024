{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb\n",
    "%run beautify_plots.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "When evaluating the quality of synthetic data, it might be reasonable to speculate whether one could\n",
    "- Train a NN to distinguish between real and synthetic data\n",
    "\n",
    "We will call a NN designed for that purpose a *Discriminator*.\n",
    "\n",
    "We will call the NN designed to *generate* synthetic data the *Generator*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It's easy to train a weak Discriminator\n",
    "- one that distinguishes between real data and noise (random data)\n",
    "\n",
    "We can train a stronger Discriminator if we have access to higher quality (than noise) synthetic data.\n",
    "\n",
    "The higher the quality of the synthetic data, the stronger the Discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But how do we construct a Generator that might be able to create synthetic data good enough to fool the Discriminator ?\n",
    "\n",
    "Using the NN for the Discriminator\n",
    "- given an input $\\x$ created by the Generator\n",
    "- we can compute the Gradient \n",
    "    - of the logit (the Discriminator output indicating Real or Not Real)\n",
    "    - with respect to $\\x$\n",
    "- the Generator can modify $\\x$ using the Gradient in the direction that moves the logit toward \"Real\"  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One can imagine an iterative process in which\n",
    "- feedback from the Discriminator improves the Generator\n",
    "- the resulting higher quality synthetic data from the Generator can be used to train a stronger Discriminator\n",
    "\n",
    "This \"adversarial\" training is the basis for a *Generative Adversarial Network (GAN)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Aside**\n",
    "\n",
    "The [GAN](https://arxiv.org/pdf/1406.2661.pdf) was invented by Ian Goodfellow in one night, following a party at a [bar](https://www.technologyreview.com/2018/02/21/145289/the-ganfather-the-man-whos-given-machines-the-gift-of-imagination/) !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Details \n",
    "\n",
    "**Notation summary**\n",
    "\n",
    "\n",
    "text | meaning                       \n",
    ":----|:---|\n",
    "<img width=100 /> | <img width=300 /> \n",
    "$p_\\text{data}$ | Distribution of real data \n",
    "$\\x \\in p_\\text{data}$  | Real sample \n",
    "$p_\\text{model}$ | Distribution of fake data \n",
    "$\\hat{\\x}$ | Fake sample\n",
    "           | $\\hat{\\x} \\not\\in p_\\text{data}$ \n",
    "           | $\\text{shape}(\\hat{\\x}) = \\text{shape} ( \\x ) $\n",
    "           $\\tilde{\\x}$ | Sample (real or fake)\n",
    "             | $\\text{shape} ( \\tilde{\\x} ) =\\text{shape}(\\x)$\n",
    "$D_{\\Theta_D}$ | Discriminator NN, parameterized by $\\Theta_D$ \n",
    "               | Binary classifier:  $\\tilde{\\x} \\mapsto \\{ \\text{Real}, \\text{Fake} \\} $\n",
    "               | $D_{\\Theta_D}(\\tilde{x}) \\in \\{ \\text{Real}, \\text{Fake} \\} \\text{ for } \\text{shape}(\\tilde{\\x}) = \\text{shape}(\\x)$ \n",
    "$\\z$ | vector or randoms with distribution $p_\\z$\n",
    "$G_{\\Theta_G}$  | Generator NN, parameterized by $\\Theta_G$  \n",
    "                | $\\z \\mapsto \\hat{\\x}$\n",
    "                | $\\text{shape}( G(\\z) ) = \\text{shape}(\\x)$\n",
    "                | $G(\\z) \\in p_\\text{model}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our goal is to generate new *synthetic* examples.\n",
    "\n",
    "Let\n",
    "- $\\x$ denote a *real* example\n",
    "    - vector of length $n$\n",
    "- $\\pdata$ be the distribution of real examples\n",
    "   - $\\x \\in \\pdata$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will create a Neural Network called the *Generator*\n",
    "\n",
    "Generator $G_{\\Theta_G}$ (parameterized by $\\Theta_G$) will\n",
    "- take a vector $\\z$ of random numbers from distribution $p_\\z$ as input\n",
    "- and outputs $\\hat{\\x}$ \n",
    "- a *synthetic/fake* example\n",
    "    - vector of length $n$\n",
    "\n",
    "Let\n",
    "- $\\pmodel$ be the distribution of fake examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>GAN Generator</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/GAN_generator.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Generator will be paired with another Neural Network called the *Discriminator*.\n",
    "\n",
    "The Discriminator $D_{\\Theta_D}$ (parameterized by $\\Theta_D$) is a binary Classifier\n",
    "- takes a vector $\\tilde{\\x} \\in \\pdata \\cup \\pmodel$\n",
    "\n",
    "**Goal of Discriminator**\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "D( \\tilde{\\x} ) & = & \\text{Real} & \\text{ for } \\tilde{\\x} \\in p_\\text{data} \\\\\n",
    "D (\\tilde{\\x} ) & = &\\text{Fake}  &\\text{ for } \\tilde{\\x} \\in p_\\text{model}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "That is\n",
    "- the Discriminator tries to distinguish between Real and Fake examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>GAN Discriminator</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/GAN_discriminator.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In contrast, the goal of the Generator\n",
    "\n",
    "**Goal of Generator**\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "D (\\hat{\\x} ) & = & \\text{Real} & \\text{ for } \\hat{\\x} = G_{\\Theta_G}(\\z)  \\in p_\\text{model}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "That is\n",
    "- the Generator tries to create fake examples that can fool the Discriminator into classifying as Real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How is this possible ?\n",
    "\n",
    "We describe a training process (that updates $\\Theta_G$ and $\\Theta_D$)\n",
    "- That follows an *iterative* game\n",
    "- Train the Discriminator to distinguish between \n",
    "    - Real examples\n",
    "    - and the Fake examples produced by the Generator on the prior iteration\n",
    "- Train the Generator to produce examples better able to fool the updated Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Sounds reasonable, but how do we get the Generator to improve it's fakes ?\n",
    "\n",
    "We will define loss functions \n",
    "- $\\loss_G$ for the Generator\n",
    "- $\\loss_D$ for the Discriminator\n",
    "\n",
    "Then we can improve the Generator (parameterized by $\\Theta_G$) by Gradient Descent\n",
    "- updating $\\Theta_G$ by $- \\frac{\\partial\\loss_G}{\\partial {\\Theta_G}}$\n",
    "- since $\\Theta_G$ controls production of $\\hat{\\x}$, we modify $\\Theta_G$ rather than $\\hat{\\x}$ directly\n",
    "\n",
    "That is\n",
    "- The Discriminator will indirectly give \"hints\" to the Generator as to why a fake example failed to fool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>GAN Generator training</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/GAN_generator_train.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>GAN Discriminator training</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/GAN_discriminator_train.png\"</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After enough rounds of the \"game\" we hope that the Generator and Discriminator battle to a stand-off\n",
    "- the Generator produces realistic fakes\n",
    "- the Discriminator has only a $50 \\%$ chance of correctly labeling a fake as Fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loss functions\n",
    "\n",
    "The goal of the generator can be stated as\n",
    "- Creating $\\pmodel$ such that\n",
    "- $\\pmodel \\approx \\pdata$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " \n",
    "There are a number of ways to measure the dis-similarity of two distributions\n",
    "- KL divergence\n",
    "    - equivalent to Maximum Likelihood estimation\n",
    "- Jensen Shannon Divergence (JSD)\n",
    "- Earth Mover Distance (Wasserstein GAN)\n",
    "\n",
    "The original paper choose the minimization of the KL divergence, so we illustrate with that measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To be concrete. let the Discriminator uses labels\n",
    "- $1$ for Real\n",
    "- $0$ for Fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " \n",
    "The Discriminator tries to maximize per example $\\loss_D$ (by minimizing the $- \\loss_D$)\n",
    "\n",
    "$$\n",
    "- \\loss_D = \n",
    "\\begin{cases} \n",
    "\\log D(\\tilde{\\x}) & \\text{ when } \\tilde{\\x} \\in \\pdata \\\\\n",
    "1 - \\log D(\\tilde{\\x}) & \\text{ when } \\tilde{\\x} \\in \\pmodel \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "That is\n",
    "- Classify real $\\x$ as Real\n",
    "- Classify fake $\\hat{\\x}$ as Fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In training the Discriminator, we present it with batches of examples\n",
    "- half real, half fake\n",
    "\n",
    "The Discriminator tries to maximize (over the batch) the negative of the loss over the batch\n",
    "$$\n",
    "\\begin{array} \\\\ \n",
    "\\loss_D & = & - \\left( \\frac{1}{2}  \\E_{\\x^\\ip \\in \\pdata } { \\log D(\\x^\\ip) }  +  \\frac{1}{2} \\E_{z \\in P_z} { \\log \\left( 1 - D(G(\\z))  \\right) } \\right) & \\text{leading minus sign to turn this into minimization } \\\\\n",
    "& = &  - \\left( \\frac{1}{2}  \\E_{\\x^\\ip \\in \\pdata } { \\log D(\\x^\\ip) }  +  \\frac{1}{2} \\E_{\\x^\\ip  \\in \\pmodel} { \\log \\left( 1 - D(\\x^\\ip)  \\right) } \\right) & D(G(\\z)) = \\x^\\ip \\text{ for fake examples}\\\\\n",
    "& = & - \\frac{1}{2}  \\sum_{\\x^\\ip \\in \\pdata}  { \\pdata (\\x^\\ip) \\log D(\\x^\\ip)   - \\frac{1}{2}  \\sum_{\\x^\\ip \\in \\pmodel} \\pmodel (\\x^\\ip) \\log ( 1 - D(\\x^\\ip)) }   \\\\\n",
    "\\end{array}\n",
    "$$ \n",
    "\n",
    "You will recognize this term as Binary Cross Entropy (BCE)\n",
    "- hence, you will see BCE used as the Loss Function in the code\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\loss_G = - \\loss_D & \\text{Zero sum game} \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The per-example Loss for the Generator is \n",
    "$$\\loss_G = 1 - \\log D(G(\\z))$$\n",
    "\n",
    "which is minimized when the fake example \n",
    "$$D(G(\\z)) = 1$$\n",
    "\n",
    "That is\n",
    "- the Discriminator mis-classifies the fake example as Real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Generator takes batches of $\\z$ (and hence sees only fake examples, not an even mix of real and fake as does the Discriminator.\n",
    "\n",
    "Since the game is zero sum\n",
    "$$\n",
    "\\loss_G = - \\loss_D \n",
    "$$\n",
    "and you will similarly see BCE as the Loss for the Generator\n",
    "- except the \"true\" labels passed to BCE will be an array of \"Real\"\n",
    "- as opposed to a mix of \"Real\" and \"Fake\" labels in the BCE of the Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So the iterative game seeks to solve a minimax problem\n",
    "\n",
    "$$\n",
    "\\min{G}\\max{D} \\left( { \\mathbb{E}_{\\x \\in p_\\text{data}} \\log D(\\x) + \\mathbb{E}_{\\z \\in p_z} ( 1 - \\log D(G(\\z))} \\right)\n",
    "$$\n",
    "- $D$ tries to \n",
    "    - make $D(\\x)$ big: correctly classify (with high probability) real $\\x$\n",
    "    - and $D(G(\\z))$ small: correctly classify (with low probability) fake $G(\\z))$\n",
    "- $G$ tries to\n",
    "    - make $D(G(\\z))$ high: fool $D$ into a high probability for a fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that the Generator improves \n",
    "- by updating $\\Theta_G$\n",
    "- so as to increase $D(G(\\z))$\n",
    "    - the mis-classification of the fake as Real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optimal Discriminator Loss\n",
    "\n",
    "Can minimize per example $\\loss_D$ wrt $D(\\x)$ by taking derivative and setting to $0$\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\frac{ \\partial \\loss_D}{\\partial D(\\x)} & = & - \\frac{1}{2} \\left( \n",
    " \\pdata(\\x) * \\frac{1}{\\log_e 10} \\frac{1}{D(\\x)} + \n",
    " \\pmodel(\\x) * \\frac{1}{\\log_e 10} \\frac{1}{1 - D(\\x)} * -1  \\right) & \\text{Definition:}\\log_e a = \\frac{\\log_e a}{\\log_e 10};  \\\\\n",
    " & & & \\text{Derivative of } \\log_e a = \\frac{1}{a}\\\\\n",
    " & = & - \\frac{1}{2 * \\log_e 10} \\frac{ \\pdata(\\x) * (1 - D(\\x)) - \\pmodel(\\x)* D(\\x)}{D(\\x) * (1 - D(\\x)) } \\\\\n",
    " & = & \\frac{1}{c} \\frac{ \\pdata (\\x) - D(\\x) ( \\pmodel(\\x) +\\pdata(\\x))}{D(\\x) * (1 - D(\\x))}\n",
    " \\\\\n",
    "\\frac{ \\partial \\loss_D}{\\partial D(\\x)} & = 0 &\\mapsto D^*(\\x) = \\frac{\\pdata (\\x)}{ \\pmodel(\\x) +\\pdata(\\x)}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So the optimal Discriminator succeeds with probability\n",
    "\n",
    "$$\n",
    "\\frac{\\pdata (\\x)}{ \\pmodel(\\x) +\\pdata(\\x)}\n",
    "$$\n",
    "\n",
    "The optimal Generator results in\n",
    "$$\n",
    "\\pmodel(\\x) = \\pdata(\\x)\n",
    "$$\n",
    "\n",
    "Thus, if the minimax optimization succeeds\n",
    "$$\n",
    "D^*(\\x) = \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "Nothing better than a coin toss !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Training\n",
    "\n",
    "We will train Generator $G_{\\Theta_G}$ Discriminator $D_{\\Theta_D}$ by turns\n",
    "- creating sequence of updated parameters\n",
    "    - $\\Theta_{G, (1)} \\ldots \\Theta_{G,(T)}$\n",
    "    - $\\Theta_{D, (1)} \\ldots \\Theta_{D,(T)}$\n",
    "- Trained *competitively*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Competitive training**\n",
    "\n",
    "Iteration $\\tt$\n",
    "\n",
    "- Train $D_{\\Theta_{D, (\\tt-1)}}$ on samples\n",
    "    - $\\tilde{\\x} \\in p_\\text{data} \\cup p_{\\text{model}, (\\tt-1)}$\n",
    "        - where $G_{\\Theta_{G, (\\tt-1)}} ( \\z) \\in p_{\\text{model}, (\\tt-1)}$\n",
    "    - Update $\\Theta_{D, (\\tt-1)}$ to $\\Theta_{D, \\tp}$ via gradient $\\frac{\\partial \\loss_D}{\\partial \\Theta_{D,(\\tt-1)}}$\n",
    "        - $D$ is a maximizer of $\\int_{\\x \\in p_\\text{data}} \\log D(\\x) + \\int_{\\z \\in p_\\z} \\log ( \\, 1 - D(G(\\z)) \\, )$\n",
    "- Train $G_{\\Theta_{G, (\\tt-1)}}$ on random samples $\\z$\n",
    "    - Create samples $\\hat{\\x}_\\tp \\in G_{\\Theta_{G, (\\tt-1)}}(\\z)  \\in p_\\text{model}$\n",
    "    - Have Discriminator $D_{\\Theta_{D, \\tp}}$ evaluate $D_{\\Theta_{D,\\tp}} ( \\hat{\\x}_\\tp )$\n",
    "    - Update $\\Theta_{G, (\\tt-1)}$ to $\\Theta_{G, \\tp}$ via gradient $\\frac{\\partial \\loss_G}{\\partial \\Theta_{G,(\\tt-1)}}$\n",
    "        - $G$ is a minimizer of $\\int_{\\z \\in p_\\z} \\log ( \\, 1 - D(G(\\z)) \\, )$\n",
    "            - i.e., want $D(G(\\z))$ to be high\n",
    "    - May update $G$ multiple times per update of $D$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Training code for a simple GAN**\n",
    "\n",
    "[Here](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/dcgan_overriding_train_step.ipynb#scrollTo=AOO8AqLy86jb)\n",
    "       is the code for the training step of a simple GAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Issues\n",
    "\n",
    "Although the description of GAN training as an adversarial game is appealing,\n",
    "actually getting training to find a stable equilibrium is difficult in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Vanishing Gradient\n",
    "\n",
    "Early in training, the Discriminator has the advantage\n",
    "- it has been trained to distinguish real input from noise\n",
    "- the parameters of the Generator are uninitialized\n",
    "    - Generator needs feedback from Discriminator in order to learn direction for improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What happens if the Discriminator is \"too good\" ?\n",
    "- $D(\\hat{\\x}) = 0$ for all $\\hat{\\x} \\in p_\\text{model}$ \n",
    "\n",
    "With absolute certainty that every $\\hat{\\x}$ from the Generator is Fake, the gradient is zero (or near zero)\n",
    "- Generator can't learn (weight updates near zero)\n",
    "\n",
    "So we don't want the Discriminator to be too good, too early in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mode Collapse\n",
    "\n",
    "We condition the Generator on random $\\z$ so that it will produce diverse $\\hat{\\x}$.\n",
    "\n",
    "Sometimes, the Generator is only able to create a single (or small number) $\\hat{\\x}'$ that is good\n",
    "enough to fool the Discriminator.\n",
    "\n",
    "In this case: the Generator may learn to ignore input $\\z$ and *only* produce $\\hat{\\x}'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hard to achieve equilibrium\n",
    "\n",
    "The optimal solution is the Nash equilibrium of the minimax problem\n",
    "$$\n",
    "\\min{G}\\max{D} \\left( { \\mathbb{E}_{\\x \\in p_\\text{data}} \\log D(\\x) + \\mathbb{E}_{\\z \\in p_z} ( 1 - \\log D(G(\\z))} \\right)\n",
    "$$\n",
    "\n",
    "However: the objective of Neural Network training is minimization of a Loss.\n",
    "\n",
    "There is no guarantee that Gradient Descent will always converge to the Nash equilibrium\n",
    "- [See this paper, section 3](https://arxiv.org/pdf/1412.6515.pdf)\n",
    "- [Also, see this paper, section 3](https://arxiv.org/pdf/1606.03498.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The gradients are partials with respect to the denominator, *holding everything else constant*.\n",
    "\n",
    "But everything is *not* constant: the Generator and Discriminator are each modifying their weights.\n",
    "- So the weight update of the Generator may not result in improvement if the simultaneous weight update of the Discriminator moves in the opposite direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An often cited example 2 player game illustrates the point\n",
    "- Player 1 seeks to minimize product $x * y$ by manipulating $x$\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\frac{\\partial x * y}{\\partial x} = y \\\\\n",
    "x \\rightarrow (x -y) & \\text {update x by negative of gradient} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "- Player 2 seeks to minimize product $- x * y$ by manipulating $x$\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\frac{\\partial (- x * y)}{\\partial y} = - x \\\\\n",
    "y \\rightarrow (y + x) & \\text {update y by negative of gradient} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "If $x, y$ have opposite signs, then the update causes them to either both increase or both decreases.\n",
    "- one can show by experiment that each update causes $x, y$ to oscillate in increasing magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Code\n",
    "\n",
    "- [GAN on Colab](https://keras.io/examples/generative/dcgan_overriding_train_step/)\n",
    "- [Wasserstein GAN with Gradient Penalty](https://keras.io/examples/generative/wgan_gp/#create-the-wgangp-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "- [Goodfellow](https://arxiv.org/pdf/1406.2661.pdf)\n",
    "- [Huszar](https://arxiv.org/pdf/1511.05101.pdf)\n",
    "- [Wasserstein GAN paper](https://arxiv.org/pdf/1701.07875.pdf)\n",
    "\n",
    "**Good blog, submitted as paper**\n",
    "- [Weng blog](https://arxiv.org/pdf/1904.08994.pdf)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
