{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hugging Face\n",
    "\n",
    "[Hugging Face](huggingface.co) is a Data Science platform that (among many other functions) hosts\n",
    "- Pre-trained models\n",
    "- Datasets\n",
    "\n",
    "They are particularly (but not exclusively) known from Transformers and NLP tasks.\n",
    "\n",
    "We will use them as our paradigm for Transfer Learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our overview will necessarily be brief.\n",
    "\n",
    "We recommend the [free, on-line course](https://huggingface.co/course)\n",
    "- We will not explain the datasets API\n",
    "    - similar to Tensorflow Dataset API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: more than just a model\n",
    "\n",
    "You will find many models for NLP on HuggingFace.\n",
    "\n",
    "One of the most important aspects of a Model Hub for NLP is\n",
    "- providing a consistent interface to multiple models\n",
    "- with different authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pre and post processing\n",
    "\n",
    "To ensure universality of models obtained from many sources\n",
    "- all NLP models take sequences of *tokens* as inputs\n",
    "- **not** sequences of words (the raw input format)\n",
    "\n",
    "Thus, there needs to be some *pre-processing* performed before using a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarly, there may need to be some *post-processing* performed on the model output.\n",
    "\n",
    "For example: creating a task-specific \"head\"\n",
    "-  e.g., classifier over *our task's* classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In addition to models, HuggingFace\n",
    "- supplies common pre-processors and post-processors\n",
    "- wraps them all up into a simple to use, powerful organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## One model, many flavors\n",
    "\n",
    "As we shall see, HuggingFace models come in many variations\n",
    "- `Keras` or `Pytorch`\n",
    "- with a \"head\" specific to a common task\n",
    "\n",
    "This reduces the need for user customization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "HuggingFace also makes it easy to match the correct pre-processor (Tokenizer) to a model\n",
    "- a model is identified by a \"check-point\" referring to its training dataset\n",
    "\n",
    "        checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "- one instantiates a model from a checkpoint\n",
    "\n",
    "         model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "    - this model has a \"head\" specialized for the task of Sequence Classification (e.g., Sentiment Analysis)\n",
    "        - the `ForSequenceClassification` suffix\n",
    "    - this model is implemented in TensorFlow\n",
    "        - the `TF` prefix\n",
    "- it's easy to choose the \"correct\" Tokenizer by using the same one as was used in training the model\n",
    "      \n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preparing input examples\n",
    "\n",
    "A unique aspect to sequence inputs is that the sequence length varies across examples.\n",
    "\n",
    "Hence, \"padding\" is usually necessary to make example lengths uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is tempting equalize the length of each example to the length of the longest example in the *training dataset*.\n",
    "\n",
    "This is not the best strategy\n",
    "- the model works in much smaller *mini-batches*\n",
    "- only necessary to equalize the length of each example *within* a batch\n",
    "    - not *across* all batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Equalizing sequence length of examples within a batch is just one example of\n",
    "preparing examples in a batch.\n",
    "\n",
    "Another case: masking words (usually randomly) in a example for a Masked Language Modeling task.\n",
    "\n",
    "HuggingFace has an abstract class [`DataCollator`](https://huggingface.co/docs/transformers/main_classes/data_collator#data-collator) to prepare examples within a batch\n",
    "- specializations for common tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Putting it all together: running an NLP task\n",
    "\n",
    "Easy !  So is actually running all the steps needed to generate output\n",
    "\n",
    "    sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "    \n",
    "    tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "    \n",
    "    output = model(**tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And one \"not so easy\" part\n",
    "- Like any classifier: the output is a *vector* of probabilities over the universe of tokens\n",
    "    - Technically: they are \"logits\" rather than probabilities\n",
    "- So the *user* needs to sample from this vector in order to choose a single output token\n",
    "    - e.g., the one with highest probability\n",
    "- This maximizes the flexibility for the user\n",
    "    - to sample rather than choose the token with highest probability\n",
    "    - to add further logic\n",
    "        - sample \"top 5\" tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A model has many \"flavors\"\n",
    "\n",
    "In our discussion of Modern NLP, we highlighted the \"Text to Text Universal API\"\n",
    "- Can translate many NLP tasks into variations of translating Text to Text\n",
    "- A post-processing step can map back from the output text to the actual Task outputs\n",
    "\n",
    "Hugging Face\n",
    "- implements a \"base model\" (universal API)\n",
    "- and the derives specializations with task-specific \"heads\"\n",
    "\n",
    "Moreover, Hugging Face models usually have implementations in both TensorFlow and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The name of a Hugging Face model is a compound string indicating\n",
    "- the base\n",
    "- the specialized head\n",
    "- the implementation language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Illustrating this with the DistilBERT model implemented in TensorFlow ([github](https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L24)), here is the class hierarchy:\n",
    "- `class TFDistilBertPreTrainedModel(TFPreTrainedModel):`\n",
    "- `class TFDistilBertModel(TFDistilBertPreTrainedModel)`\n",
    "- `class TFDistilBertForMaskedLM(TFDistilBertPreTrainedModel, TFMaskedLanguageModelingLoss):`\n",
    "- `class TFDistilBertForSequenceClassification(TFDistilBertPreTrainedModel, TFSequenceClassificationLoss):`\n",
    "- `class TFDistilBertForTokenClassification(TFDistilBertPreTrainedModel, TFTokenClassificationLoss):`\n",
    "- $\\vdots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `TF` prefix indicates that the implementation language is TensorFlow.\n",
    "\n",
    "- There is an abstract class for Pretrained TensorFlow models: `TFPreTrainedModel)`\n",
    "- `TFDistilBertPreTrainedModel` is a pretrained model\n",
    "- There are specialization of `TFDistilBertPreTrainedModel` for each NLP task\n",
    "    - `TFDistilBertForMaskedLM` for the Masked Language Modelin task\n",
    "    - `TFDistilBertForSequenceClassification` for the Sequence Classification task\n",
    "    - `TFDistilBertForTokenClassification` for the Token Classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You get to choose !\n",
    "- If you choose a model instance with the `*For*` pattern in the name\n",
    "    - you get a pre-trained model with a Classification head (post-processing) for a specific task\n",
    "- If you choose a model instances *without* the `*For*` pattern\n",
    "    - you get a model without a head (or post-processing)\n",
    "    - you can add your own\n",
    "    \n",
    "You can also instantiate a model *without* pre-trained weights\n",
    "- just the architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: `TFDistilBertForSequenceClassification`\n",
    "\n",
    "Let's take a high-level look at one specific model: `TFDistilBertForSequenceClassification`\n",
    "- this is a TensorFlow implementation (the `TF` prefix) of the Pytorch `DistilBert` model\n",
    "- with a classification head for `SequenceClassification`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model card\n",
    "\n",
    "Part of a good model hub is a *model card* for each model that describes\n",
    "- The model\n",
    "- The data on which it was trained\n",
    "- Gives reference to the original work (paper, code)\n",
    "- Discusses potential *bias*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There is often also some very practical (technical) information\n",
    "- code sample\n",
    "- how long it took to train \n",
    "    - 90 hours using 8 GPU's\n",
    "    - we will get all the benefit without the cost of training via Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A potential user can also often get a sense of the behavior via a *Hosted Inference API*\n",
    "- try the model without coding !\n",
    " - The `Deploy` button gives code fragments for\n",
    "     - accessing this hosted inference\n",
    "     - creating *your own* hosted inference\n",
    "         - free version: limited host machine capability\n",
    "         - paid versions\n",
    " \n",
    "[Here](https://huggingface.co/distilbert-base-uncased) is the model card for `DistilBert`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model bias\n",
    "\n",
    "Bias of models (really, the data on which they were trained) is important and is being increasingly recognized.\n",
    "\n",
    "Models are often trained from publicly available Web content (e.g., Wikipedia).\n",
    "\n",
    "Although the datasets were not designed to be biased\n",
    "- the frequency of biased concepts that they contain\n",
    "- causes the biases to be transfered to models on which they are trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the Intro course, we discussed how the GloVe word embeddings may reveal bias:\n",
    "\n",
    "$\n",
    "\\begin{array}[llllll]\\\\\n",
    "\\text{doctor - man + woman} &  \\approx_{n',d} & \\text{nurse } \\\\\n",
    "\\text{mechanic  - man + woman} &  \\approx_{n',d} & \\text{teacher } \\\\\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An important part of releasing a model for public use is informing the user of any bias that they inherit.\n",
    "\n",
    "Discovering and documenting potential bias is non-trivial and potentially time-consuming.\n",
    "\n",
    "In this case, for the Masked Language Modeling task, the authors look at what the model chooses\n",
    "- to fill in the mask\n",
    "- conditional on some aspects of the subject of the sentence\n",
    "\n",
    "[Here are the biases](https://huggingface.co/distilbert-base-uncased#limitations-and-bias) discussed on the model card."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Using a pre-trained model for NLP (w/o Transfer Learning)\n",
    "\n",
    "Our first demo will be to show how to use a pre-trained model.\n",
    "- We will not (yet) specialize it to a new Task\n",
    "\n",
    "The task is Text Sequence Classification (sentiment)\n",
    "\n",
    "The model is based on the Transformer architecture.\n",
    "\n",
    "We give a quick review of how Transformers are typically used for NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Pre processing**\n",
    "\n",
    "Text is a sequence of words, but the words must be parsed into *tokens*\n",
    "- a word may turn into multiple tokens: Byte Piece Encoding\n",
    "\n",
    "The tokens must be converted into integer indices in a finite vocabulary\n",
    "- It is the sequence of *token ids* that is consumed by the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Post processsing**\n",
    "\n",
    "Hugging Face Classification models are slightly different in that they return\n",
    "- logits (scores)\n",
    "- **not** probabilities\n",
    "\n",
    "We can use `argmax` on the score to decide the class, but it's nice to see the associated probability too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Making it concrete: let's write some code\n",
    "\n",
    "We make these abstract ideas concrete by writing some code\n",
    "- especially: looking at the shape and type outputs\n",
    "\n",
    "Let's go to the [notebook](HF_quick_intro_to_models.ipynb)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
