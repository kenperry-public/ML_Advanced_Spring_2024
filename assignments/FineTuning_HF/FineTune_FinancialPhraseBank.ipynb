{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"../../Latex_macros.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment\n",
    "\n",
    "In this assignment you will use the Unsupervised Pre-Training + Supervised Fine-Tuning paradigm.\n",
    "\n",
    "The objective is to wind up with a model that can perform Sentiment Analysis on sentences from\n",
    "a Financial domain.\n",
    "\n",
    "Specifically\n",
    "- you will choose a Pre-Trained Language Model from the HuggingFace platform\n",
    "- Fine-Tune the model on the Financial PhraseBank dataset\n",
    "    - a collection of hand-annotated sentences with sentiment classified a Negative, Neutral, Positive\n",
    "\n",
    "The assignment will consist of\n",
    "- a \"basic\" part\n",
    "    - This is the minimum that you must submit (and succeed on)  in order to earn a passing grade\n",
    "- \"extras\"\n",
    "    - more challenging objectives \n",
    "    - earn points beyond the basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Financial PhraseBank dataset\n",
    "\n",
    "The dataset was created as part of a research paper in Finance.\n",
    "\n",
    "There were several humans who labeled the sentiment of sentences.  Naturally humans don't always agree.\n",
    "Thus, the dataset has several \"flavors\" (called \"subsets\") based on the fraction of annotators who\n",
    "agree.\n",
    "\n",
    "As part of the \"basic\" assignment, you should use the \"all agree\" flavor.\n",
    "\n",
    "There are multiple ways to access this dataset.\n",
    "- The preferred way: as a [HuggingFace dataset](https://huggingface.co/datasets/financial_phrasebank)\n",
    "- Discouraged: as a [TFDS](https://www.tensorflow.org/datasets/community_catalog/huggingface/financial_phrasebank)\n",
    "    - this is discouraged *only* because there will be an \"extra\" part asking you to convert the HuggingFace dataset to a TFDS\n",
    "    - you won't get credit for this \"extra\" if you obtain it directly as a TFDS !\n",
    "- Discouraged: From the [authors](https://www.researchgate.net/publication/251231364_FinancialPhraseBank-v10)\n",
    "    - the \"Download the PDF\" link contains an archive file with the data\n",
    "    - this is discouraged only because it involves extra work\n",
    "    - but you might find it handy in creating your own TFDS in the \"extra\" part\n",
    "    \n",
    "You don't need to understand the paper but here is a [reference](https://arxiv.org/pdf/1307.5336.pdf#page=9)\n",
    "to the paper that introduced it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Choose a Pre-Trained Language Model from the HuggingFace platform\n",
    "\n",
    "There are lots of Language Models on HuggingFace.  They vary by, among other things\n",
    "- model size\n",
    "- training objective\n",
    "- language of training data\n",
    "\n",
    "An important consideration is computational\n",
    "- the model fits in the memory of the machine you are using\n",
    "- the execution time is rapid enough for you to experiment\n",
    "    - it will help to have a machine with a GPU (e.g., Google Colab)\n",
    "\n",
    "You are free to choose *subject to limitations*\n",
    "- the model should *not* have already been trained on a Financial dataset\n",
    "    - e.g., you may not use FinBERT\n",
    "\n",
    "I can verify that the choice of `distilbert-base-uncased` is feasible when using the free version of Google Colab\n",
    "\n",
    "One of the options for extra credit is to experiment with different models and report and explain the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Submission requirements\n",
    "\n",
    "You will submit a Jupyter notebook that follows the steps in\n",
    "the [Recipe for Machine Learning](https://github.com/kenperry-public/ML_Fall_2022/blob/master/Recipe_Overview.ipynb)\n",
    "\n",
    "That is, rather than just submitting a final model\n",
    "- you should follow the steps  (as appropriate) in the Recipe\n",
    "- treat the notebook as a movie showing\n",
    "    - the Recipe steps that you took\n",
    "        - Exploratory Data analysis and Error Analysis are steps that are ignored at your own peril   \n",
    "    - including steps that led to failure (e.g., bad choices)\n",
    "        - what you learned from the failure\n",
    "        - how you overcame it or learned to make a different choice\n",
    "        \n",
    "Use Markup in Jupyter rather than code comments to convey your thoughts.\n",
    "\n",
    "Use Section Headings to clearly indicate different parts of the assignment and different experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Your code should be TensorFlow/Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Discussion\n",
    "\n",
    "In each part, or each experiment, I would like to see a Discussion of results\n",
    "- Can you conclude that your choices led to better of worse out of sample performance ?\n",
    "- Do you have a theory as to why the performance changed/did not change ?\n",
    "\n",
    "The objective is to get you to think like a scientist rather than just completing a task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Originality\n",
    "\n",
    "There are no doubt published notebooks that use this dataset for a similar purpose.\n",
    "\n",
    "The work that you submit should be your own.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Getting started\n",
    "\n",
    "The HuggingFace course is a great way to get up to speed on various aspects of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basic parts of assignment\n",
    "\n",
    "## Part 1: Fine-tuning a pre-trained model, conventional approach\n",
    "\n",
    "There are several sub-parts. \n",
    "- Fine-tune *only the Classifier head*\n",
    "    - This will help determine how well your Pre-Trained model performs on financial data\n",
    "- *Keeping the trained Classifier head of the first part*: Fine-tune *all* the weights\n",
    "    - This will help determine how much fine-tuning improved your Pre-Trained model\n",
    "    - *Do not* start this step with an uninitialized Classifier head\n",
    "- Fine-tune all the weights (Pre-Trained + Classifier Head) simultaneously\n",
    "    - start with an uninitialized Classifier head\n",
    "\n",
    "You will present a discussion of how your out of sample performance was affected\n",
    "by any of the sub-parts.\n",
    "\n",
    "The Basic part allows you several simplifications\n",
    "- your model fitting can take features/labels that are Python data structures\n",
    "    - these are most familiar to you\n",
    "    - obtained from the dataset\n",
    "- you can using a HuggingFace model that already comes with a Classifier head\n",
    "\n",
    "I suggest that one section of your notebook clearly demonstrates success on the Basic Part.\n",
    "- Walk before you try running\n",
    "- Extra parts should be distinct suggestions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2: Fine-tuning a pre-trained model using LoRA\n",
    "\n",
    "You will use LoRA to fine-tune a pre-trained model.\n",
    "\n",
    "Our objective is identical to Part 1\n",
    "- compare the performance of a model that has *not* been fine-tuned with Financial data\n",
    "- to a model that *has* been fine-tuned with Financial data\n",
    "\n",
    "The difference:\n",
    "- you will use LoRA to perform the fine-tuning\n",
    "- we will fine-tune only a subset of the model's weights\n",
    "\n",
    "We now describe Part 2 in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In Part 1, you were able to choose your own pre-trained model.\n",
    "\n",
    "In Part 2 (to simplify grading),\n",
    "the model we require you to fine tune is `distilbert-base-uncased`\n",
    "\n",
    "Your first step: repeat the first sub-part of Part 1 using the Part 2 model\n",
    "- Fine-tune *only the Classifier head*\n",
    "- If you also used the Part 2 model in Part 1\n",
    "    - you can save a copy of the model from Part 1 (after training only the Classifier head)\n",
    "    - and re-use it rather than performing this step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Your second step: perform something *similar* to the second sub-part of Part 1 (using the Part 2 model)\n",
    "- *Keeping the trained Classifier head of the first part*: Fine-tune *a specified subset*  of the weights\n",
    "    - In Part 1: you fine-tuned **all** the weights\n",
    "    - In Part 2: you will fine-tune **only the weights** of the FFN blocks in every layer\n",
    "        - this is where we hypothesize \"world knowledge\" acquired during training is stored\n",
    "        - each FFN block has 2 `Dense` layers: `lin1`, `lin2`\n",
    "            - apply LoRA **to each** of the 2 `Dense` layers\n",
    "    \n",
    "The way you will fine-tune a subset of the weights is\n",
    "- using the LoRA technique\n",
    "    - adding LoRA adapters to the blocks to be fine-tuned\n",
    "     - **use rank 8** for LoRA adapters\n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Required questions to answer**\n",
    "\n",
    "- How does the performance of the model fine-tuned with LoRA compare to that of the pre-trained (not fine-tuned) model \n",
    "- How does the performance of the model fine-tuned with LoRA compare to that of the model fine-tuned using the conventional approach\n",
    "- Provide answers to the following but *also* show code to compute the answer (use program statements to compute the answers)\n",
    "    - How many LoRA parameters for each FFN block\n",
    "    - How many LoRA parameters total (number of FFN blocks times the LoRA parameters per block)\n",
    "        - assign this to variable `added_total`\n",
    "    - How many *trainable* parameters in your model\n",
    "        - this will help us confirm that you have fine-tuned *only* the FFN blocks\n",
    "    - How many *total* parameters in your model\n",
    "    - Calculate, based on your knowledge of the LoRA adapters you added\n",
    "        - how many *trainable parameters* you *expect* to be added to the pre-trained model\n",
    "            - assign this to variable `added_parms_calc`\n",
    "        - verify that the number of trainable parameters you added is equal to the number your expect voa tje following statement\n",
    "        \n",
    "        `assert(added_total == added_parms_calc)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Extras for Part 1\n",
    "\n",
    "You can demonstrate greater skill (and earn more points) by completing some extra parts.\n",
    "\n",
    "After each extra, please complete a Discussion section as you did for the Basic part.\n",
    "\n",
    "My suggestions follow, but I'm open to your ideas.\n",
    "\n",
    "I will determine the amount of extra points by my perception of the difficulty of each extra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Create (and fit the model with) a TensorFlow Dataset (TFDS)\n",
    "\n",
    "*Relative difficulty*: low to medium\n",
    "\n",
    "*Starting off with the HuggingFace dataset*, your\n",
    "objective \n",
    "- is to create a TFDS such that you could train a model without knowing \n",
    "anything about how the TFDS was prepared beforehand. \n",
    "- similar to how you would use any other externally supplied dataset\n",
    "\n",
    "We want to *pretend* that our machine's memory is too small to load the entire dataset into memory *for training*, and hence need to use a TFDS.\n",
    "- it is perfectly acceptable to load the entire dataset into memory for the sole purpose of creating the TFDS\n",
    "\n",
    "Given an arbitrary \n",
    "dataset, you might need to perform some operations for training, such as\n",
    "\n",
    "- shuffling\n",
    "- splitting into train and validation and/or test datasets\n",
    "- batching\n",
    "\n",
    "All operations for training on the dataset you create should be performed by TFDS operations \n",
    "*after* you have created the TFDS.  \n",
    "\n",
    "For training, you *may not assume* that any \n",
    "necessary operation has been performed before-hand , for example\n",
    "- you may not shuffle or split the source dataset before turning it into a TFDS\n",
    "\n",
    "You may create the \n",
    "TFDS in any way that you like but, once created, anyone should be able to use it \n",
    "without knowing the steps involved in its creation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create your own Classification head\n",
    "\n",
    "*Relative difficulty*: medium to hard\n",
    "\n",
    "Rather than using a Classification head automatically provided with the HuggingFace model\n",
    "- you will obtain a head-less model from HugginFace\n",
    "- you will add your own Classification head\n",
    "    - with as many layers as you like\n",
    "- you *must* explain\n",
    "    - the logic of how you knew where to graft the head on the headless model\n",
    "    - how you adapted (if necessary) the output of the headless model to the shape requirements of the Classifier head\n",
    "\n",
    "The objective is for you to demonstrate some skills with the Keras Functional Model API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use different \"flavors\" of the dataset\n",
    "\n",
    "*Relative difficulty*: low, but time-consuming\n",
    "\n",
    "The basic part used examples on which all annotators agreed.\n",
    "\n",
    "Try different flavors and discuss the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Address any Imbalanced Data issues\n",
    "\n",
    "*Relative difficulty: low*\n",
    "\n",
    "- If the distribution between labels in the dataset is not uniform, you may want to address the imbalance\n",
    "\n",
    "## Superior Error Analysis\n",
    "\n",
    "*Relative difficulty: medium*\n",
    "\n",
    "Rather than just reporting a single summary statistic for out of sample performance, analyze the results in detail\n",
    "- Is one class harder to correctly classify than others\n",
    "- Is there some systematic pattern of errors\n",
    "    - e.g., characteristics of input sentences that are more difficult to correctly classify\n",
    "    \n",
    "Try to use the results of this analysis to improve the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Experiment with different Pre-Trained models\n",
    "\n",
    "*Relative difficulty*: low, but time-consuming\n",
    "\n",
    "Try several different pre-trained Language Models.\n",
    "\n",
    "Discuss the results.  For example\n",
    "- does a bigger pre-trained model lead to better results\n",
    "    - before and after fine-tuning all the weights\n",
    "- does the type of data on which the model was trained make a difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Experiment with Fine-Tuning\n",
    "\n",
    "*Relative difficulty*: low, but time-consuming\n",
    "\n",
    "Does out of sample performance vary with changing\n",
    "- the number of examples in Fine-Tuning\n",
    "    - what is the smallest number that you think is sufficient\n",
    "- there are many choices of proper subsets of a given size\n",
    "    - does it matter which one you choose ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In-context learning\n",
    "\n",
    "*Relative difficulty: low but fun !*\n",
    "\n",
    "Can you use few-shot learning successfully (i.e., no further training) ?\n",
    "\n",
    "It would be great to do this for Financial PhraseBank but the sentences may be too long\n",
    "- pre-trained models have maximum sequence lengths that may be too small\n",
    "\n",
    "Propose some interesting task related to Finance and try to achieve Few Shot Learning on the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Extras for Part 2\n",
    "\n",
    "## Experiment with the rank used in LoRA\n",
    "\n",
    "In the basic part, we asked that your LoRA adapters using rank $8$\n",
    "- this facilitated our comparison of the number of parameters you added with what we expect\n",
    "\n",
    "Conduct experiments in varying the rank\n",
    "- how does the Performance Metric change with rank ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Some helpful tips\n",
    "\n",
    "## Making sure your model has trained\n",
    "\n",
    "The assignment asks you to train a Classifier over 3 possible classes. \n",
    "\n",
    "If your \n",
    "model performs no better than random chance, you will find that it has an \n",
    "Accuracy of around 33%.\n",
    "\n",
    "So if, after training, your model's accuracy is not *much* higher than this: \n",
    "your model has a problem -- It's very likely that some model weight's have not \n",
    "been trained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Freezing the pre-trained model\n",
    "\n",
    "HuggingFace allows you the option of instantiating a model with a task-specific \n",
    "head.  \n",
    "\n",
    "In class I have always used a simple one-layer head, e.g., a Dense layer \n",
    "to implement a Classifier. \n",
    "\n",
    "But you *should not assume* that the head supplied \n",
    "by someone else's model has a head designed exactly that way.  Always look !\n",
    "\n",
    "Using the `summary` method on the model returned by HuggingFace will help you determine what to freeze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unfreezing\n",
    "\n",
    "Before you can fit a model in Keras, you must execute a `compile` statement.  \n",
    "\n",
    "On \n",
    "the surface, the purpose is to associate a loss function (and optionally: an \n",
    "optimizer and metrics) with the model.  \n",
    "\n",
    "Below the surface, it causes the \n",
    "`trainable` attribute of the weights of a layer or model to become static. \n",
    "\n",
    "The \n",
    "practical implication: if you \"freeze\" the weights of some part of the model for \n",
    "an initial round of training, and then wish to unfreeze those parts of the model \n",
    "so that they will change in subsequent training, you must re-issue a `compile` \n",
    "statement after unfreezing.\n",
    "\n",
    "See the [Keras guide to Transfer Learning](<https://urldefense.proofpoint.com/v2/url?u=https-3A__keras.io_guides_transfer-5Flearning_-23finetuning&d=DwMBaQ&c=slrrB7dE8n7gBJbeO0g-IQ&r=58cSHneR9qnbk2_epkhw3g&m=BemEXtkIoZA-tri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Extras for Part 2\n",
    "\n",
    "## Experiment with the rank of the LoRA adapters\n",
    "\n",
    "We required you to use a specific value for the rank in the basic part\n",
    "- this facilitated our comparing the number of parameters you added with what we would expect\n",
    "\n",
    "Vary the rank and examine the impact on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
